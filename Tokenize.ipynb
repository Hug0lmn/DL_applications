{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff93f3f",
   "metadata": {},
   "source": [
    "## This notebook serve as a way to assess the good embedding of the corpus using fasttext (which uses n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73355329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a10e01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6820622682571411, 'poli'),\n",
       " (0.6406318545341492, 'justice'),\n",
       " (0.590151309967041, 'polie'),\n",
       " (0.5569336414337158, 'complice'),\n",
       " (0.5478050708770752, 'politique'),\n",
       " (0.5321061611175537, \"d'vice\"),\n",
       " (0.500546932220459, 'vice'),\n",
       " (0.4951876997947693, 'vengeance'),\n",
       " (0.492553174495697, 'pouce'),\n",
       " (0.48715028166770935, 'niques')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = fasttext.train_unsupervised(\"corpus_tokenization_Rohff.txt\", model = \"skipgram\", epoch = 20, dim = 200, lr=0.05, ws=8)\n",
    "word2vec.get_nearest_neighbors(\"police\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065e1d0",
   "metadata": {},
   "source": [
    "As you can see, there are semantic similarities such as:\n",
    "{police / justice}, {police / politique}\n",
    "\n",
    "Some of them are also close in terms of rhyme, such as:\n",
    "{police / complice}, {police / vice}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6a4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6820622682571411, 'poli'), (0.6406318545341492, 'justice'), (0.590151309967041, 'polie'), (0.5569336414337158, 'complice'), (0.5478050708770752, 'politique')]\n",
      "\n",
      "[(0.7639144062995911, \"l'humanité\"), (0.7130317687988281, 'fierté'), (0.6860047578811646, 'son-pri'), (0.6439739465713501, 'élu'), (0.6025695204734802, 'bénef')]\n",
      "\n",
      "[(0.8188565969467163, 'distance'), (0.75723797082901, 'résistance'), (0.7381221055984497, 'ambiance'), (0.7338213324546814, 'innocence'), (0.7159032225608826, 'souffrance')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in [\"police\", \"liberté\", \"vengeance\"]:\n",
    "    print(word2vec.get_nearest_neighbors(w)[:5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2de05",
   "metadata": {},
   "source": [
    "The embedding seems to work, as shown by the phonetic and semantic neighbors:\n",
    "\n",
    "Liberté: {l'humanité / fierté / son-pri (slang for jail) / élu}\n",
    "\n",
    "Vengeance: {distance / résistance / ambiance / innocence / souffrance}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a323d4a",
   "metadata": {},
   "source": [
    "In a future version : I will provide a way to visualize the embedding and words by using dimensionnality reduction\n",
    "\n",
    "I will also, after building the first RNN for generations, use the specific fasttext's embedding as an input into a NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
