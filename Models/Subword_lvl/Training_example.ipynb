{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13425138,"sourceType":"datasetVersion","datasetId":8348047}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook will serve as a way to implement character generation LSTM and other implementation","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nimport re\nimport numpy as np\nimport pickle\nfrom transformers import PreTrainedTokenizerFast\nimport math\n\nfrom functools import partial","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:47.980563Z","iopub.execute_input":"2025-10-19T15:13:47.980737Z","iopub.status.idle":"2025-10-19T15:13:53.998525Z","shell.execute_reply.started":"2025-10-19T15:13:47.980716Z","shell.execute_reply":"2025-10-19T15:13:53.997717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"TOKENIZERS_PARALLELISM=True\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file=\"/kaggle/input/subword-rnn-lstm/rap_tokenizer.json\",\n    pad_token = \"<PAD>\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:53.999792Z","iopub.execute_input":"2025-10-19T15:13:54.000151Z","iopub.status.idle":"2025-10-19T15:13:54.033564Z","shell.execute_reply.started":"2025-10-19T15:13:54.000124Z","shell.execute_reply":"2025-10-19T15:13:54.032833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creation of the dataset","metadata":{}},{"cell_type":"markdown","source":"To increase the randomness during the training :\n\nFor each epoch the entire corpus will have a random specific offset value in order that the model during training doesn't see the exact same text during X epochs.","metadata":{}},{"cell_type":"code","source":"dataset_ = np.load(\"/kaggle/input/subword-rnn-lstm/encoded.npy\",\"r\")\n\nresult = []\n\nfor t in dataset_:\n    if t == tokenizer.convert_tokens_to_ids(\"α\") : \n        current = []\n        current.append(t)\n    elif t == tokenizer.convert_tokens_to_ids(\"θ\") :\n        current.append(t)\n        result.append(torch.tensor(current))\n    else :\n        current.append(t)\nif current:  \n    result.append(torch.tensor(current))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:54.034195Z","iopub.execute_input":"2025-10-19T15:13:54.034514Z","iopub.status.idle":"2025-10-19T15:13:56.653306Z","shell.execute_reply.started":"2025-10-19T15:13:54.034488Z","shell.execute_reply":"2025-10-19T15:13:56.652739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SongDataset(Dataset):\n    def __init__(self, texts, length_seq, stride, use_offset=True):\n        self.samples = []\n        self.length_seq = length_seq\n        self.stride = stride\n\n        for text in texts:\n            L = len(text)\n\n            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n\n            # --- Boucle principale ---\n            for start in range(offset, max(1, L - self.length_seq), self.stride):\n                x_start, x_end = start, start + self.length_seq\n                y_start, y_end = start + 1, start + 1 + self.length_seq\n\n                x = text[x_start:x_end]\n                y = text[y_start:y_end]\n\n                self.samples.append((x, y))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:56.654927Z","iopub.execute_input":"2025-10-19T15:13:56.655140Z","iopub.status.idle":"2025-10-19T15:13:56.660676Z","shell.execute_reply.started":"2025-10-19T15:13:56.655122Z","shell.execute_reply":"2025-10-19T15:13:56.660099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len_train = int(len(result) * 0.8)\nlen_test = len(result) - len_train\n\ngenerator = torch.Generator().manual_seed(42)\ntrain, test = random_split(result, [len_train, len_test], generator = generator)\n\ntrain_ds = SongDataset(train, length_seq=256, stride = 16, use_offset = True)\ntest_ds = SongDataset(test, length_seq=256, stride = 16, use_offset = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:56.661367Z","iopub.execute_input":"2025-10-19T15:13:56.661633Z","iopub.status.idle":"2025-10-19T15:13:57.204082Z","shell.execute_reply.started":"2025-10-19T15:13:56.661609Z","shell.execute_reply":"2025-10-19T15:13:57.203520Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creation of Dataloader and co","metadata":{}},{"cell_type":"code","source":"def collate_batch(batch, tokenizer):\n    # batch = list of (X, Y) tuples from your Dataset\n    X, Y = zip(*batch)\n\n    # Pad each side to the longest sequence in the batch\n    X_padded = pad_sequence(X, batch_first=True, padding_value=tokenizer.pad_token_id)\n    Y_padded = pad_sequence(Y, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    # Return the structure expected by your model\n    batch_enc = [ X_padded,Y_padded]\n\n    return batch_enc\n\n# Create a callable version of collate_fn with your tokenizer\ncollate_fn = partial(collate_batch, tokenizer=tokenizer)\n\n#Normally a function requires to specify the options at the initiation but partial allows to specify values for the required option that will\n#be stored and then be used when the function will be called\n# collate_fn(batch) == collate_batch(batch, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:57.204803Z","iopub.execute_input":"2025-10-19T15:13:57.205024Z","iopub.status.idle":"2025-10-19T15:13:57.209795Z","shell.execute_reply.started":"2025-10-19T15:13:57.205007Z","shell.execute_reply":"2025-10-19T15:13:57.209027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 512\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", \n                        num_workers=4, prefetch_factor=4, shuffle=False, drop_last=True, collate_fn = collate_fn) #Shuffle False because we need the RNN to use previous sequences data to predict next one\ntest_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", \n                       num_workers=2, prefetch_factor=2, shuffle=False, drop_last=True, collate_fn = collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:57.210557Z","iopub.execute_input":"2025-10-19T15:13:57.210790Z","iopub.status.idle":"2025-10-19T15:13:57.222769Z","shell.execute_reply.started":"2025-10-19T15:13:57.210774Z","shell.execute_reply":"2025-10-19T15:13:57.222109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"markdown","source":"### Training part","metadata":{}},{"cell_type":"code","source":"class LSTM_3_layers(nn.Module) :\n    def __init__(self, vocab_size, embed_dim, hidden_size, batch_size, proba):\n        super().__init__()\n\n        self.batch_size = batch_size\n        self.hidden_size = hidden_size\n    \n        self.lstm_layers = nn.ModuleList([nn.LSTMCell(embed_dim if i == 0 else hidden_size, hidden_size, bias=True) for i in range(3)])\n\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx = 0)\n        self.drop = nn.Dropout(p=proba)\n        self.ln = nn.LayerNorm(hidden_size)\n\n        self.linear = nn.Linear(hidden_size, vocab_size, bias=False)\n\n    def forward(self, x, hidden) :\n        h,c = hidden \n\n        x_t = self.embed(x)\n        for i, lstm_cell in enumerate(self.lstm_layers) : #No dropout before BN : https://arxiv.org/pdf/1801.05134 \n            h[i], c[i] = lstm_cell(x_t,(h[i],c[i]))\n            x_t = self.drop(self.ln(h[i]))\n\n        logits = self.linear(x_t)\n\n        return logits, (h,c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:57.223404Z","iopub.execute_input":"2025-10-19T15:13:57.223680Z","iopub.status.idle":"2025-10-19T15:13:57.231814Z","shell.execute_reply.started":"2025-10-19T15:13:57.223653Z","shell.execute_reply":"2025-10-19T15:13:57.231144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\")\n\nembedding_dim = 384\nhidden_size = 512\nvocab_size = tokenizer.vocab_size\nnum_epoch = 100\n\nnb_step_train = len(train_dl)\nnb_step_test = len(test_dl)\n\nmodel = LSTM_3_layers(vocab_size, embedding_dim, hidden_size, batch_size, 0.2).to(device)\nmodel = torch.compile(model)\n\nloss_fn = nn.CrossEntropyLoss(ignore_index=0)\nval_loss = nn.CrossEntropyLoss(ignore_index=0)\n\nopti = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\nsched_warm = torch.optim.lr_scheduler.LinearLR(opti, start_factor=0.2, end_factor=1.0, total_iters=nb_step_train*3)\nsched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*10, T_mult=2, eta_min=0.0001) #1 epoch => 2 => 4 => 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:13:57.232513Z","iopub.execute_input":"2025-10-19T15:13:57.232786Z","iopub.status.idle":"2025-10-19T15:14:01.770578Z","shell.execute_reply.started":"2025-10-19T15:13:57.232769Z","shell.execute_reply":"2025-10-19T15:14:01.769814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate_free(model, dl, loss_fn, device):\n    \"\"\"\n    Autoregressive validation (teacher forcing = 0).\n    Steps one token at a time and feeds predictions back in.\n    Returns ppl (computed on next-token NLL).\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for X, Y in dl:\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        bs, sl = X.size(0), X.size(1)\n        hid = model.init_hidden(bs, device1)\n\n        # Start with the first input token\n        inp = X[:, :1]  # (bs, 1)\n        for t in range(sl):\n            with torch.amp.autocast(device_type=\"cuda\"):\n                pred, hid = model(inp, hid)          # (bs, 1, vocab)\n                logits = pred[:, -1, :]              # (bs, vocab)\n                loss = loss_fn(logits, Y[:, t])      # CE over current step\n\n            total_loss += loss.item() * bs\n            total_tokens += bs\n\n            # Greedy next-token to feed back in\n            next_token = logits.argmax(dim=-1).unsqueeze(1)  # (bs, 1)\n            inp = next_token\n\n    ppl = math.exp(total_loss / max(1, total_tokens))\n    return ppl\n\ndef sample_with_temp(logits, temp=1.0):\n    probs = (logits / temp).softmax(dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n    return next_token\n\ndef distinct_n_chars(text, n=3):\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    return len(set(ngrams)) / max(1, len(ngrams))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:14:01.772823Z","iopub.execute_input":"2025-10-19T15:14:01.773127Z","iopub.status.idle":"2025-10-19T15:14:01.781111Z","shell.execute_reply.started":"2025-10-19T15:14:01.773109Z","shell.execute_reply":"2025-10-19T15:14:01.780203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def training(model, train_dl, loss_fn, vocab_size, opti, scaler, device) :\n    \n    model.train()\n    train_loss = 0.0\n\n    for X,Y in iter(train_dl) :\n        \n        X,Y = X.to(device), Y.to(device, dtype=torch.long)\n        h = [torch.zeros(batch_size, hidden_size, device=device) for _ in range(3)]\n        c = [torch.zeros(batch_size, hidden_size, device=device) for _ in range(3)]\n        \n        opti.zero_grad(set_to_none=True)\n        batch_loss = 0\n\n        for t in range(X.size(1)) : #On each timestep of the seq\n            with torch.amp.autocast(device_type=\"cuda\"):\n                pred, (h,c) = model(X[:,t], (h,c))\n                batch_loss += loss_fn(pred.view(-1, vocab_size), Y[:,t])\n\n        batch_loss /= X.size(1)\n\n        scaler.scale(batch_loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n        scaler.step(opti)\n        scaler.update()\n\n        train_loss += batch_loss.detach().item() \n\n    return train_loss, scaler\n\ndef evaluate(model, eval_dl, loss_fn, vocab_size, device) :\n    \n    model.eval()\n    eval_loss = 0.0\n\n    for X,Y in iter(eval_dl) :\n        \n        X,Y = X.to(device), Y.to(device, dtype=torch.long)\n        h = [torch.zeros(batch_size, hidden_size, device=device) for _ in range(3)]\n        c = [torch.zeros(batch_size, hidden_size, device=device) for _ in range(3)]\n    \n        batch_loss = 0\n\n        for t in range(X.size(1)) : #On each timestep of the seq\n            pred, (h,c) = model(X[:,t], (h,c))\n            batch_loss += loss_fn(pred.view(-1, vocab_size), Y[:,t])\n\n        batch_loss /= X.size(1)\n        eval_loss += batch_loss.detach().item() \n\n    return eval_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:14:01.782730Z","iopub.execute_input":"2025-10-19T15:14:01.782983Z","iopub.status.idle":"2025-10-19T15:14:01.802733Z","shell.execute_reply.started":"2025-10-19T15:14:01.782961Z","shell.execute_reply":"2025-10-19T15:14:01.802013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_offset = []\nl_tot = []\nteacher_forcing_ratio = 1\nbest_val = float(\"inf\")\n\nscaler = torch.amp.GradScaler()\n\nfor epoch in range(num_epoch):\n    \n    # -------------- TRAIN LOOP --------------\n    train_loss, scaler = training(model, train_dl, loss_fn, vocab_size, opti, scaler, device)\n    train_ppl = math.exp(train_loss / nb_step_train)\n\n    # ---- Scheduler part ---- #\n\n    if sched_post.T_cur == 0 and epoch > 4:  #After warm restart decrease the max learning rate\n        sched_post.base_lrs[0] = sched_post.base_lrs[0] * 0.9\n    #    sched_post.eta_min = sched_post.eta_min * 0.85\n        print(f\"Decrease {sched_post.base_lrs[0]}, {sched_post.eta_min}\")\n\n    if epoch < 3:\n        sched_warm.step()\n    else:\n        sched_post.step()\n    \n    # -------------- VALIDATION --------------\n    with torch.no_grad() :      \n        val_loss = evaluate(model, test_dl, loss_fn, vocab_size, device)\n        val_ppl = math.exp(val_loss / nb_step_test)\n\n    print(\n        f\"Epoch {epoch} | \"\n        f\"Train PPL: {train_ppl:.3f} | \"\n        f\"Val PPL : {val_ppl:.3f}\"\n    )\n\n    # --------- Sample generation + diversity metrics ---------\n    if epoch % 8 == 0 and epoch != 0 :\n        \n        model.eval()\n        with torch.no_grad():\n        # Context priming\n            X, _ = next(iter(test_dl))\n            \n            start = X[0:1, :20].to(device)\n            h = [torch.zeros(1, hidden_size, device=device) for _ in range(3)]\n            c = [torch.zeros(1, hidden_size, device=device) for _ in range(3)]\n\n            for j in range(start.size(1)) : \n                _, (h,c) = model(start[:,j] ,(h,c))\n\n        #Generating\n            inp = start[:,j]\n            gen_chars = []\n\n            for t in range(200):  \n                logits, (h,c) = model(inp, (h,c))\n                next_char = sample_with_temp(logits, temp=0.6)\n                gen_chars.append(tokenizer.decode(next_char.item()))\n                inp = next_char.view(1) # feed back\n\n            gen_text = \"\".join(gen_chars)\n\n        d2 = distinct_n_chars(gen_text, n=2)\n        d3 = distinct_n_chars(gen_text, n=3)\n\n        print(\"\\n=== Initial text ===\")\n        print(\"\".join(tokenizer.decode(X[0:1,:].squeeze(0))))\n        print(\"\\n=== Sample Generation ===\")\n        print(gen_text[:200]) \n        print(f\"Distinct-2: {d2:.3f} | Distinct-3: {d3:.3f}\", end=\"\\n\")\n\n    # Record accuracy\n    l_tot.append(val_ppl)\n    if val_ppl < best_val :\n        best_val = val_ppl\n        torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": opti.state_dict(),\n                    \"scheduler_state_dict\": sched_post.state_dict(),\n                    \"val_ppl\": val_ppl,\n                },\n                \"model\",\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T15:14:01.803553Z","iopub.execute_input":"2025-10-19T15:14:01.803805Z","execution_failed":"2025-10-19T15:49:35.622Z"}},"outputs":[],"execution_count":null}]}