{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13591902,"sourceType":"datasetVersion","datasetId":8299981},{"sourceId":646847,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":487848,"modelId":503271}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook will serve as a way to implement character generation LSTM and other implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import get_cosine_schedule_with_warmup\n\n\nimport fasttext\nimport math\nimport numpy as np\nimport pickle\nimport random\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:06:47.294806Z","iopub.execute_input":"2025-11-15T23:06:47.295039Z","iopub.status.idle":"2025-11-15T23:06:52.857638Z","shell.execute_reply.started":"2025-11-15T23:06:47.295013Z","shell.execute_reply":"2025-11-15T23:06:52.857074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate_tf1(model, dl, loss_fn, device, vocab_size):\n    \"\"\"Validation with teacher forcing = 1 (parallel, fast). Returns (ppl, acc).\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    correct = 0\n    total = 0\n    nb_step = len(dl)\n\n    for X, local_context, Y in dl:\n        local_context = local_context.to(device)\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        h = model.init_hidden(1024).to(device)\n        \n        with torch.amp.autocast(device_type=\"cuda\"):\n            pred, h = model(X, local_context, h)  # (bs, sl, vocab)\n            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n        total_loss += loss.item() \n\n    ppl = math.exp(total_loss / nb_step)\n    return ppl\n\n\n@torch.no_grad()\ndef evaluate_free(model, dl, loss_fn, device):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for X, local_context, Y in dl:\n        local_context = local_context.to(device)\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        bs, sl = X.size(0), X.size(1)\n        hid = model.init_hidden(bs).to(device)\n\n        # Start with the first input token\n        inp = X[:, :1]  # (bs, 1)\n        for t in range(sl):\n            inp_context = local_context[:, 0:t]\n            with torch.amp.autocast(device_type=\"cuda\"):\n                pred, hid = model(inp, local_context, hid)          # (bs, 1, vocab)\n                logits = pred[:, -1, :]              # (bs, vocab)\n                loss = loss_fn(logits, Y[:, t])      # CE over current step\n\n            total_loss += loss.item() \n\n            # Greedy next-token to feed back in\n            next_token = logits.argmax(dim=-1).unsqueeze(1)  # (bs, 1)\n            inp = next_token\n\n    ppl = math.exp(total_loss / bs)\n    return ppl\n\ndef sample_with_temp(logits, temp=1.0):\n    probs = (logits / temp).softmax(dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n    return next_token\n\ndef distinct_n_chars(text, n=3):\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    return len(set(ngrams)) / max(1, len(ngrams))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:06:52.859328Z","iopub.execute_input":"2025-11-15T23:06:52.859623Z","iopub.status.idle":"2025-11-15T23:06:52.869414Z","shell.execute_reply.started":"2025-11-15T23:06:52.859605Z","shell.execute_reply":"2025-11-15T23:06:52.868740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/rnn-input/encoding_map.pkl\", \"rb\") as f:\n    mapping = pickle.load(f)\n\nmapping[\"PAD\"] = len(mapping)\n\n# Decode\nint2char = {i: ch for ch, i in mapping.items()}\nprint(int2char)\n\nnb_char = len(int2char)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:06:52.870223Z","iopub.execute_input":"2025-11-15T23:06:52.870446Z","iopub.status.idle":"2025-11-15T23:06:52.889408Z","shell.execute_reply.started":"2025-11-15T23:06:52.870429Z","shell.execute_reply":"2025-11-15T23:06:52.888857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creation of the dataset","metadata":{}},{"cell_type":"code","source":"ends = np.load(\"/kaggle/input/rnn-input/ends.npy\",\"r\")\nstarts = np.load(\"/kaggle/input/rnn-input/starts.npy\",\"r\")\nword_matrix = np.load(\"/kaggle/input/rnn-input/word_matrix.npy\",\"r\")\n\nfast_emb = fasttext.load_model(\"/kaggle/input/rnn-input/fasttext_corpus\")\ndataset_ = np.load(\"/kaggle/input/rnn-input/corpora_encoded.npy\",\"r\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:06:52.890227Z","iopub.execute_input":"2025-11-15T23:06:52.890448Z","iopub.status.idle":"2025-11-15T23:06:58.097016Z","shell.execute_reply.started":"2025-11-15T23:06:52.890421Z","shell.execute_reply":"2025-11-15T23:06:58.096432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_context_windows(word_matrix, starts, ends, numerot_seq) :\n    \"\"\"\n    Extract from a specific text the specific context_window for the entire of the sequence by using pre specified indication about word length\n    For each character in the sequence, gave back a context windows of 3,100 values that represent the words in a space of 100 dim\n    \"\"\"\n\n    x_begin = numerot_seq[0]\n    x_end = numerot_seq[-1]\n    mask = (starts <= x_end) & (ends >= x_begin)\n    indices = np.where(mask)[0]\n\n    beginning = starts[indices[0]]\n    if beginning < x_begin :\n        beginning = x_begin\n\n    ending = ends[indices[-1]]\n    if ending > x_end :\n        ending = x_end\n\n    whole_matrix = []\n    first_idx = indices[0]\n    for k,idx in enumerate(indices) :\n\n        is_last = (idx == indices[-1])\n    \n        if k < 3 :\n            context = np.vstack([np.zeros((3-k,100),dtype = word_matrix.dtype),word_matrix[first_idx:idx]])\n        else : \n            context = word_matrix[idx-3:idx]\n    \n        if k == 0 :\n            start_i = beginning\n            end_i = ends[idx]\n        elif is_last :\n            start_i = ends[idx-1]\n            end_i = ending\n        else :\n            start_i = ends[idx-1]\n            end_i = ends[idx]\n        \n        for i in range(start_i,end_i) :\n            whole_matrix.append(context)\n\n    return np.array(whole_matrix, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:06:58.097725Z","iopub.execute_input":"2025-11-15T23:06:58.097982Z","iopub.status.idle":"2025-11-15T23:06:58.105394Z","shell.execute_reply.started":"2025-11-15T23:06:58.097958Z","shell.execute_reply":"2025-11-15T23:06:58.104594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SongDataset(Dataset):\n    def __init__(self, texts, length_seq, stride, starts, ends, word_matrix, \n                 pad_id=mapping[\"PAD\"], use_offset=True):\n        self.samples = []\n        self.length_seq = length_seq\n        self.stride = stride\n        self.pad_id = pad_id\n        self.word_matrix = word_matrix\n        self.starts = starts\n        self.ends = ends\n\n        for elem in texts:\n\n            text, numerot = elem \n            \n            L = len(text)\n            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n\n            # --- Boucle principale ---\n            for start in range(offset, max(1, L - self.length_seq - 1), self.stride):\n                x_start, x_end = start, start + self.length_seq\n                y_start, y_end = start + 1, start + 1 + self.length_seq\n\n                numerot_seq = numerot[x_start:x_end] \n                x = text[x_start:x_end]\n                y = text[y_start:y_end]\n\n                local_context = get_context_windows(self.word_matrix, self.starts, self.ends, numerot_seq)\n                local_context = torch.from_numpy(local_context)\n\n                # --- Padding uniforme ---\n                def pad_to_len(seq, pad_id, target_len):\n                    pad_len = target_len - len(seq)\n                    if pad_len > 0:\n                        seq = torch.cat([seq, torch.full((pad_len,), pad_id, dtype=seq.dtype)])\n                    return seq\n\n                x = pad_to_len(x, self.pad_id, self.length_seq)\n                y = pad_to_len(y, self.pad_id, self.length_seq)\n\n                if len(local_context) < self.length_seq:\n                    pad_len = self.length_seq - len(local_context)\n                    pad = torch.zeros((pad_len, local_context.shape[1], local_context.shape[2]))\n                    local_context = torch.cat([local_context, pad], dim=0)\n\n                self.samples.append((x, local_context, y))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:06:58.107678Z","iopub.execute_input":"2025-11-15T23:06:58.107885Z","iopub.status.idle":"2025-11-15T23:06:58.120090Z","shell.execute_reply.started":"2025-11-15T23:06:58.107867Z","shell.execute_reply":"2025-11-15T23:06:58.119568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_ = np.load(\"/kaggle/input/rnn-input/corpora_encoded.npy\",\"r\")\n\nresult = []\nfor i,t in enumerate(dataset_):\n    if t == 63 : \n        current = []\n        num = []\n        current.append(t)\n        num.append(i)\n    elif t == 69:\n        current.append(t)\n        num.append(i)\n        result.append([torch.tensor(current), num])\n    else :\n        current.append(t)\n        num.append(i)\nif current:  \n    result.append([torch.tensor(current), num])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:06:58.120731Z","iopub.execute_input":"2025-11-15T23:06:58.120918Z","iopub.status.idle":"2025-11-15T23:07:00.571862Z","shell.execute_reply.started":"2025-11-15T23:06:58.120902Z","shell.execute_reply":"2025-11-15T23:07:00.571276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len_train = int(len(result) * 0.8)\nlen_test = len(result) - len_train\n\ngenerator = torch.Generator().manual_seed(42)\ntrain, test = random_split(result, [len_train, len_test], generator = generator)\n\ntrain_ds = SongDataset(train,length_seq= 256, stride = 48, starts = starts, ends = ends, word_matrix = word_matrix, use_offset = False)\ntest_ds = SongDataset(test,length_seq= 256, stride = 48, starts = starts, ends = ends, word_matrix = word_matrix, use_offset = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:07:00.572776Z","iopub.execute_input":"2025-11-15T23:07:00.572966Z","iopub.status.idle":"2025-11-15T23:08:18.294082Z","shell.execute_reply.started":"2025-11-15T23:07:00.572949Z","shell.execute_reply":"2025-11-15T23:08:18.293244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creation of Dataloader and co","metadata":{}},{"cell_type":"code","source":"batch_size = 1024\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True) #Shuffle False because we need the RNN to use previous sequences data to predict next one\ntest_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:08:18.294996Z","iopub.execute_input":"2025-11-15T23:08:18.295243Z","iopub.status.idle":"2025-11-15T23:08:18.300224Z","shell.execute_reply.started":"2025-11-15T23:08:18.295224Z","shell.execute_reply":"2025-11-15T23:08:18.299449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"markdown","source":"### Training part","metadata":{}},{"cell_type":"code","source":"class Char_GRU(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout, num_layers):\n        super(Char_GRU, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.drop = nn.Dropout(p=dropout)\n        self.ln = nn.LayerNorm(hidden_size)\n\n        self.embed_drop = nn.Dropout(p=0.1)    \n        self.rnn_drop = nn.Dropout(p=0.2)      \n        self.attn_drop = nn.Dropout(p=0.1)     \n        self.proj_drop = nn.Dropout(p=0.1)  \n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=71)\n        self.hidden_to_emb = nn.Linear(hidden_size, embedding_dim)\n\n        self.fc = nn.Linear(embedding_dim, vocab_size, bias=False)\n        self.fc.weight = self.embedding.weight\n        \n        # Context dim proj\n        self.context_proj = nn.Linear(3 * 100, embedding_dim)  # 3 prev words Ã— 100-dim vectors\n\n        self.gru = nn.GRU(\n            input_size=embedding_dim * 2,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=0.1\n        )\n\n        # Attention\n        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size) \n        self.Wa = nn.Linear(hidden_size, hidden_size, bias=False) #Learnable attention matrix\n\n    def forward(self, x, context, h):\n        \n        x_embed = self.embed_drop(self.embedding(x))\n\n        # --- Context projection ---\n        context_flat = context.view(context.size(0), context.size(1), -1)  \n        context_proj = self.context_proj(context_flat)                     \n\n        x_input = torch.cat([x_embed, context_proj], dim=-1)               \n\n        out, h = self.gru(x_input, h)\n        out = self.rnn_drop(self.ln(out))\n\n        #Attention part Q/K/V\n        #query = hidden[-1].unsqueeze(1)    \n        keys = self.Wa(out)\n        values = out\n        \n        attn_scores = torch.bmm(out, keys.transpose(1, 2))/ math.sqrt(out.size(-1))\n\n        #Mask attention\n        L = out.size(1)\n        mask = torch.tril(torch.ones(L, L, device=out.device)).unsqueeze(0)  \n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_weights = F.softmax(attn_scores, dim=-1)                    \n        context_vec = torch.bmm(attn_weights, values)\n\n        combined = torch.cat((out, context_vec), dim=-1)                 \n        combined = torch.tanh(self.attn_combine(combined)) #Non linearity\n\n        emb_space = self.hidden_to_emb(combined)\n\n        out = self.fc(emb_space)\n        return out, h\n\n    def init_hidden(self, batch_size):\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n        return h0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:08:18.301063Z","iopub.execute_input":"2025-11-15T23:08:18.301353Z","iopub.status.idle":"2025-11-15T23:08:18.312814Z","shell.execute_reply.started":"2025-11-15T23:08:18.301327Z","shell.execute_reply":"2025-11-15T23:08:18.312071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device1 = torch.device(\"cuda:0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:08:18.313451Z","iopub.execute_input":"2025-11-15T23:08:18.313702Z","iopub.status.idle":"2025-11-15T23:08:18.325652Z","shell.execute_reply.started":"2025-11-15T23:08:18.313687Z","shell.execute_reply":"2025-11-15T23:08:18.325020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 128\nvocab_size = len(int2char)\nhidden_size = 512\nnum_epoch = 200\n\nnb_step_train = len(train_dl)\nnb_step_test = len(test_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:08:18.326602Z","iopub.execute_input":"2025-11-15T23:08:18.326919Z","iopub.status.idle":"2025-11-15T23:08:18.336429Z","shell.execute_reply.started":"2025-11-15T23:08:18.326895Z","shell.execute_reply":"2025-11-15T23:08:18.335738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Char_GRU(vocab_size, embedding_dim, hidden_size, dropout = 0.2, num_layers=3).to(device1)\nmodel = torch.compile(model)\n\nloss_fn = nn.CrossEntropyLoss(ignore_index=71)\n\nopti = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n\nscheduler = get_cosine_schedule_with_warmup(\n    opti,             \n    num_warmup_steps=nb_step_train*4,  \n    num_training_steps=nb_step_train*num_epoch\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:08:18.337056Z","iopub.execute_input":"2025-11-15T23:08:18.337239Z","iopub.status.idle":"2025-11-15T23:08:23.150317Z","shell.execute_reply.started":"2025-11-15T23:08:18.337215Z","shell.execute_reply":"2025-11-15T23:08:23.149582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"mod = torch.load(\"/kaggle/input/gru/pytorch/default/1/Model_GRU.pt\")\nmodel.load_state_dict(mod[\"model_state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2025-11-15T22:58:54.935111Z","iopub.execute_input":"2025-11-15T22:58:54.935545Z","iopub.status.idle":"2025-11-15T22:58:55.436964Z","shell.execute_reply.started":"2025-11-15T22:58:54.935518Z","shell.execute_reply":"2025-11-15T22:58:55.436288Z"}}},{"cell_type":"code","source":"opti = torch.optim.AdamW(model.parameters(), lr=0.00025, weight_decay=1e-4)\n\nscheduler = get_cosine_schedule_with_warmup(\n    opti,             \n    num_warmup_steps=nb_step_train*4,  \n    num_training_steps=nb_step_train*num_epoch\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:49:41.119397Z","iopub.execute_input":"2025-11-15T23:49:41.119677Z","iopub.status.idle":"2025-11-15T23:49:41.123890Z","shell.execute_reply.started":"2025-11-15T23:49:41.119657Z","shell.execute_reply":"2025-11-15T23:49:41.123285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_offset = []\nl_tot = []\nteacher_forcing_ratio = 1\nbest_val = float(\"inf\")\n\nscaler = torch.amp.GradScaler()\n\nfor epoch in range(num_epoch):\n\n    model.train()\n\n    # -------------- TRAIN LOOP --------------\n    train_loss = 0.0\n    \n    for X, local_context, Y in iter(train_dl):\n        h = model.init_hidden(batch_size).to(device1)\n        local_context = local_context.to(device1)\n        X = X.to(device1)\n        Y = Y.to(device1, dtype=torch.long)\n        opti.zero_grad(set_to_none=True)\n\n        with torch.amp.autocast(device_type=\"cuda\"):\n            pred, h = model(X, local_context, h)\n            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n        scaler.scale(loss).backward()\n#        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n        scaler.step(opti)\n        scaler.update()\n\n        # book-keeping\n        train_loss += loss.detach().item()\n        scheduler.step()\n        \n    train_ppl = math.exp(train_loss/nb_step_train)\n\n    # -------------- VALIDATION --------------\n    val_ppl_tf1 = evaluate_tf1(model, test_dl, loss_fn, device1, vocab_size)\n#    val_ppl_free = evaluate_free(model, test_dl, val_fn, device1)\n\n    print(\n        f\"Epoch {epoch} | \"\n        f\"Train PPL: {train_ppl:.3f} | \"\n        f\"Val PPL (TF=1): {val_ppl_tf1:.3f} | \"\n#        f\"Val PPL (free): {val_ppl_free:.3f} | \"\n    )\n\n    # --------- Sample generation + diversity metrics ---------\n\n    # Record accuracy\n    l_tot.append(val_ppl_tf1)\n    if val_ppl_tf1 < best_val :\n        best_val = val_ppl_tf1\n        torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": opti.state_dict(),\n                    \"scheduler_state_dict\": scheduler.state_dict(),\n                    \"val_ppl\": val_ppl_tf1,\n                },\n                \"model\",\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T23:49:42.070068Z","iopub.execute_input":"2025-11-15T23:49:42.070459Z","execution_failed":"2025-11-16T00:25:04.231Z"}},"outputs":[],"execution_count":null}]}