{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13591902,"sourceType":"datasetVersion","datasetId":8299981},{"sourceId":633403,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":477565,"modelId":493514}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook will serve as a way to implement character generation LSTM and other implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import get_cosine_schedule_with_warmup\n\n\nimport fasttext\nimport math\nimport numpy as np\nimport pickle\nimport random\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:02.842718Z","iopub.execute_input":"2025-11-08T20:28:02.842964Z","iopub.status.idle":"2025-11-08T20:28:08.304239Z","shell.execute_reply.started":"2025-11-08T20:28:02.842947Z","shell.execute_reply":"2025-11-08T20:28:08.303483Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate_tf1(model, dl, loss_fn, device, vocab_size):\n    \"\"\"Validation with teacher forcing = 1 (parallel, fast). Returns (ppl, acc).\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    correct = 0\n    total = 0\n    nb_step = len(dl)\n\n    for X, local_context, Y in dl:\n        local_context = local_context.to(device)\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        h = model.init_hidden(1024, device)\n        \n        with torch.amp.autocast(device_type=\"cuda\"):\n            pred, h = model(X, local_context, h)  # (bs, sl, vocab)\n            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n        total_loss += loss.item() \n\n    ppl = math.exp(total_loss / nb_step)\n    return ppl\n\n\n@torch.no_grad()\ndef evaluate_free(model, dl, loss_fn, device):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for X, local_context, Y in dl:\n        local_context = local_context.to(device)\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        bs, sl = X.size(0), X.size(1)\n        hid = model.init_hidden(bs).to(device)\n\n        # Start with the first input token\n        inp = X[:, :1]  # (bs, 1)\n        for t in range(sl):\n            inp_context = local_context[:, 0:t]\n            with torch.amp.autocast(device_type=\"cuda\"):\n                pred, hid = model(inp, local_context, hid)          # (bs, 1, vocab)\n                logits = pred[:, -1, :]              # (bs, vocab)\n                loss = loss_fn(logits, Y[:, t])      # CE over current step\n\n            total_loss += loss.item() \n\n            # Greedy next-token to feed back in\n            next_token = logits.argmax(dim=-1).unsqueeze(1)  # (bs, 1)\n            inp = next_token\n\n    ppl = math.exp(total_loss / bs)\n    return ppl\n\ndef sample_with_temp(logits, temp=1.0):\n    probs = (logits / temp).softmax(dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n    return next_token\n\ndef distinct_n_chars(text, n=3):\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    return len(set(ngrams)) / max(1, len(ngrams))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:08.305429Z","iopub.execute_input":"2025-11-08T20:28:08.305722Z","iopub.status.idle":"2025-11-08T20:28:08.317514Z","shell.execute_reply.started":"2025-11-08T20:28:08.305705Z","shell.execute_reply":"2025-11-08T20:28:08.316761Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/rnn-input/encoding_map.pkl\", \"rb\") as f:\n    mapping = pickle.load(f)\n\nmapping[\"PAD\"] = len(mapping)\n\n# Decode\nint2char = {i: ch for ch, i in mapping.items()}\nprint(int2char)\n\nnb_char = len(int2char)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:08.318268Z","iopub.execute_input":"2025-11-08T20:28:08.318550Z","iopub.status.idle":"2025-11-08T20:28:08.343532Z","shell.execute_reply.started":"2025-11-08T20:28:08.318529Z","shell.execute_reply":"2025-11-08T20:28:08.342873Z"}},"outputs":[{"name":"stdout","text":"{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '%', 5: \"'\", 6: '+', 7: ',', 8: '-', 9: '.', 10: '/', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '?', 24: 'a', 25: 'b', 26: 'c', 27: 'd', 28: 'e', 29: 'f', 30: 'g', 31: 'h', 32: 'i', 33: 'j', 34: 'k', 35: 'l', 36: 'm', 37: 'n', 38: 'o', 39: 'p', 40: 'q', 41: 'r', 42: 's', 43: 't', 44: 'u', 45: 'v', 46: 'w', 47: 'x', 48: 'y', 49: 'z', 50: 'à', 51: 'â', 52: 'ç', 53: 'è', 54: 'é', 55: 'ê', 56: 'ë', 57: 'ì', 58: 'î', 59: 'ï', 60: 'ô', 61: 'ù', 62: 'û', 63: 'α', 64: 'β', 65: 'γ', 66: 'ε', 67: 'ζ', 68: 'η', 69: 'θ', 70: '€', 71: 'PAD'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Creation of the dataset","metadata":{}},{"cell_type":"code","source":"ends = np.load(\"/kaggle/input/rnn-input/ends.npy\",\"r\")\nstarts = np.load(\"/kaggle/input/rnn-input/starts.npy\",\"r\")\nword_matrix = np.load(\"/kaggle/input/rnn-input/word_matrix.npy\",\"r\")\n\nfast_emb = fasttext.load_model(\"/kaggle/input/rnn-input/fasttext_corpus\")\ndataset_ = np.load(\"/kaggle/input/rnn-input/corpora_encoded.npy\",\"r\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:08.345267Z","iopub.execute_input":"2025-11-08T20:28:08.345468Z","iopub.status.idle":"2025-11-08T20:28:12.901137Z","shell.execute_reply.started":"2025-11-08T20:28:08.345451Z","shell.execute_reply":"2025-11-08T20:28:12.900536Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_context_windows(word_matrix, starts, ends, x_begin, x_end) :\n    \"\"\"\n    Extract from a specific text the specific context_window for the entire of the sequence by using pre specified indication about word length\n    For each character in the sequence, gave back a context windows of 3,100 values that represent the words in a space of 100 dim\n    \"\"\"\n\n    mask = (starts <= x_end) & (ends >= x_begin)\n    indices = np.where(mask)[0]\n\n    beginning = starts[indices[0]]\n    if beginning < x_begin :\n        beginning = x_begin\n\n    ending = ends[indices[-1]]\n    if ending > x_end :\n        ending = x_end\n\n    whole_matrix = []\n    first_idx = indices[0]\n    for k,idx in enumerate(indices) :\n\n        is_last = (idx == indices[-1])\n    \n        if k < 3 :\n            context = np.vstack([np.zeros((3-k,100),dtype = word_matrix.dtype),word_matrix[first_idx:idx]])\n        else : \n            context = word_matrix[idx-3:idx]\n\n    \n        if k == 0 :\n            start_i = beginning\n            end_i = ends[idx]\n        elif is_last :\n            start_i = ends[idx-1]\n            end_i = ending\n        else :\n            start_i = ends[idx-1]\n            end_i = ends[idx]\n        \n        for i in range(start_i,end_i) :\n            whole_matrix.append(context)\n\n    return np.array(whole_matrix, dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:12.901772Z","iopub.execute_input":"2025-11-08T20:28:12.901963Z","iopub.status.idle":"2025-11-08T20:28:12.908347Z","shell.execute_reply.started":"2025-11-08T20:28:12.901947Z","shell.execute_reply":"2025-11-08T20:28:12.907696Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class SongDataset(Dataset):\n    def __init__(self, texts, length_seq, stride, starts, ends, word_matrix, \n                 pad_id=mapping[\"PAD\"], use_offset=True):\n        self.samples = []\n        self.length_seq = length_seq\n        self.stride = stride\n        self.pad_id = pad_id\n        self.word_matrix = word_matrix\n        self.starts = starts\n        self.ends = ends\n\n        for text in texts:\n            \n            L = len(text)\n            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n\n            # --- Boucle principale ---\n            for start in range(offset, max(1, L - self.length_seq - 1), self.stride):\n                x_start, x_end = start, start + self.length_seq\n                y_start, y_end = start + 1, start + 1 + self.length_seq\n\n                x = text[x_start:x_end]\n                y = text[y_start:y_end]\n\n                local_context = get_context_windows(self.word_matrix, self.starts, self.ends, x_start, x_end)\n                local_context = torch.from_numpy(local_context)\n\n                # --- Padding uniforme ---\n                def pad_to_len(seq, pad_id, target_len):\n                    pad_len = target_len - len(seq)\n                    if pad_len > 0:\n                        seq = torch.cat([seq, torch.full((pad_len,), pad_id, dtype=seq.dtype)])\n                    return seq\n\n                x = pad_to_len(x, self.pad_id, self.length_seq)\n                y = pad_to_len(y, self.pad_id, self.length_seq)\n\n                if len(local_context) < self.length_seq:\n                    pad_len = self.length_seq - len(local_context)\n                    pad = torch.zeros((pad_len, local_context.shape[1], local_context.shape[2]))\n                    local_context = torch.cat([local_context, pad], dim=0)\n\n                self.samples.append((x, local_context, y))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:12.908974Z","iopub.execute_input":"2025-11-08T20:28:12.909320Z","iopub.status.idle":"2025-11-08T20:28:12.933032Z","shell.execute_reply.started":"2025-11-08T20:28:12.909294Z","shell.execute_reply":"2025-11-08T20:28:12.932464Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset_ = np.load(\"/kaggle/input/rnn-input/corpora_encoded.npy\",\"r\")\n\nresult = []\nfor t in dataset_:\n    if t == 63 : \n        current = []\n        current.append(t)\n    elif t == 69:\n        current.append(t)\n        result.append(torch.tensor(current))\n    else :\n        current.append(t)\nif current:  \n    result.append(torch.tensor(current))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:12.933836Z","iopub.execute_input":"2025-11-08T20:28:12.934055Z","iopub.status.idle":"2025-11-08T20:28:15.075285Z","shell.execute_reply.started":"2025-11-08T20:28:12.934021Z","shell.execute_reply":"2025-11-08T20:28:15.074747Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"len_train = int(len(result) * 0.8)\nlen_test = len(result) - len_train\n\ngenerator = torch.Generator().manual_seed(42)\ntrain, test = random_split(result, [len_train, len_test], generator = generator)\n\ntrain_ds = SongDataset(train,length_seq= 256, stride = 48, starts = starts, ends = ends, word_matrix = word_matrix, use_offset = True)\ntest_ds = SongDataset(test,length_seq= 256, stride = 48, starts = starts, ends = ends, word_matrix = word_matrix, use_offset = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:28:15.075993Z","iopub.execute_input":"2025-11-08T20:28:15.076276Z","iopub.status.idle":"2025-11-08T20:29:21.638924Z","shell.execute_reply.started":"2025-11-08T20:28:15.076252Z","shell.execute_reply":"2025-11-08T20:29:21.638090Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Creation of Dataloader and co","metadata":{}},{"cell_type":"code","source":"batch_size = 1024\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True) #Shuffle False because we need the RNN to use previous sequences data to predict next one\ntest_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:29:21.639859Z","iopub.execute_input":"2025-11-08T20:29:21.640140Z","iopub.status.idle":"2025-11-08T20:29:21.644459Z","shell.execute_reply.started":"2025-11-08T20:29:21.640116Z","shell.execute_reply":"2025-11-08T20:29:21.643819Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"markdown","source":"### Training part","metadata":{}},{"cell_type":"code","source":"class CharLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.2, num_layers=1):\n        super(CharLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.drop = nn.Dropout(p=dropout)\n        self.ln = nn.LayerNorm(hidden_size)\n        self.embed_drop = nn.Dropout(p=0.1)    # on embeddings\n        self.rnn_drop = nn.Dropout(p=0.2)      # recurrent dropout (between GRU layers)\n        self.attn_drop = nn.Dropout(p=0.1)     # on attention output\n        self.proj_drop = nn.Dropout(p=0.1)  \n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=71)\n        # Project hidden → embedding space\n        self.hidden_to_emb = nn.Linear(hidden_size, embedding_dim)\n\n        # Output projection (tied to embedding)\n        self.fc = nn.Linear(embedding_dim, vocab_size, bias=False)\n        self.fc.weight = self.embedding.weight\n        \n        # Context dim proj\n        self.context_proj = nn.Linear(3 * 100, embedding_dim)  # 3 prev words × 100-dim vectors\n\n        # RNN now expects concatenated embeddings → 2 * emb_size\n        self.gru = nn.GRU(\n            input_size=embedding_dim * 2,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=0.1\n        )\n\n        # Attention\n        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size) #After concat, need to stream back the params to hidden_size\n        self.Wa = nn.Linear(hidden_size, hidden_size, bias=False) #Learnable attention matrix\n\n    def forward(self, x, context, h):\n        \n        x_embed = self.embed_drop(self.embedding(x))\n\n        # --- Context projection ---\n        context_flat = context.view(context.size(0), context.size(1), -1)  \n        context_proj = self.context_proj(context_flat)                     \n\n        x_input = torch.cat([x_embed, context_proj], dim=-1)               \n\n        out, h = self.gru(x_input, h)\n        out = self.rnn_drop(self.ln(out))\n\n        #Attention part Q/K/V\n        #query = hidden[-1].unsqueeze(1)    \n        keys = self.Wa(out)\n        values = out\n        \n        attn_scores = torch.bmm(out, keys.transpose(1, 2))/ math.sqrt(out.size(-1))\n\n        #Mask attention\n        L = out.size(1)\n        mask = torch.tril(torch.ones(L, L, device=out.device)).unsqueeze(0)  \n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_weights = F.softmax(attn_scores, dim=-1)                    \n        context_vec = torch.bmm(attn_weights, values)\n\n        combined = torch.cat((out, context_vec), dim=-1)                 \n        combined = torch.tanh(self.attn_combine(combined)) #Non linearity\n\n        emb_space = self.hidden_to_emb(combined)\n\n        out = self.fc(emb_space)\n        return out, h\n\n    def init_hidden(self, batch_size, device):\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device)\n        return h0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:29:21.646634Z","iopub.execute_input":"2025-11-08T20:29:21.646844Z","iopub.status.idle":"2025-11-08T20:29:21.659970Z","shell.execute_reply.started":"2025-11-08T20:29:21.646829Z","shell.execute_reply":"2025-11-08T20:29:21.659461Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device1 = torch.device(\"cuda:0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:29:21.660641Z","iopub.execute_input":"2025-11-08T20:29:21.660797Z","iopub.status.idle":"2025-11-08T20:29:21.674380Z","shell.execute_reply.started":"2025-11-08T20:29:21.660784Z","shell.execute_reply":"2025-11-08T20:29:21.673786Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"embedding_dim = 128\nvocab_size = len(int2char)\nhidden_size = 512\nnum_epoch = 200\n\nnb_step_train = len(train_dl)\nnb_step_test = len(test_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:29:21.675005Z","iopub.execute_input":"2025-11-08T20:29:21.675219Z","iopub.status.idle":"2025-11-08T20:29:21.685691Z","shell.execute_reply.started":"2025-11-08T20:29:21.675204Z","shell.execute_reply":"2025-11-08T20:29:21.685062Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model = CharLSTM(vocab_size, embedding_dim, hidden_size, num_layers=3).to(device1)\nmodel = torch.compile(model)\n\nloss_fn = nn.CrossEntropyLoss(ignore_index=71)\n\nopti = torch.optim.AdamW(model.parameters(), lr=0.00025, weight_decay=1e-4)\n\nscheduler = get_cosine_schedule_with_warmup(\n    opti,             \n    num_warmup_steps=nb_step_train*4,  \n    num_training_steps=nb_step_train*num_epoch\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:29:21.686285Z","iopub.execute_input":"2025-11-08T20:29:21.686475Z","iopub.status.idle":"2025-11-08T20:29:26.353497Z","shell.execute_reply.started":"2025-11-08T20:29:21.686461Z","shell.execute_reply":"2025-11-08T20:29:26.352747Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"mod = torch.load(\"/kaggle/input/3-lay/pytorch/default/1/gru_tie_embed_3lay\")\nmodel.load_state_dict(mod[\"model_state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2025-11-07T23:40:02.637537Z","iopub.execute_input":"2025-11-07T23:40:02.638115Z","iopub.status.idle":"2025-11-07T23:40:02.722274Z","shell.execute_reply.started":"2025-11-07T23:40:02.638087Z","shell.execute_reply":"2025-11-07T23:40:02.721512Z"}}},{"cell_type":"code","source":"list_offset = []\nl_tot = []\nteacher_forcing_ratio = 1\nbest_val = float(\"inf\")\n\nscaler = torch.amp.GradScaler()\n\nfor epoch in range(num_epoch):\n\n    model.train()\n\n    # -------------- TRAIN LOOP --------------\n    train_loss = 0.0\n    \n    for X, local_context, Y in iter(train_dl):\n        h = model.init_hidden(batch_size, device1)\n        local_context = local_context.to(device1)\n        X = X.to(device1)\n        Y = Y.to(device1, dtype=torch.long)\n        opti.zero_grad(set_to_none=True)\n\n        with torch.amp.autocast(device_type=\"cuda\"):\n            pred, h = model(X, local_context, h)\n            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n        scaler.scale(loss).backward()\n#        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n        scaler.step(opti)\n        scaler.update()\n\n        # book-keeping\n        train_loss += loss.detach().item()\n        scheduler.step()\n        \n    train_ppl = math.exp(train_loss/nb_step_train)\n\n    # -------------- VALIDATION --------------\n    val_ppl_tf1 = evaluate_tf1(model, test_dl, loss_fn, device1, vocab_size)\n#    val_ppl_free = evaluate_free(model, test_dl, val_fn, device1)\n\n    print(\n        f\"Epoch {epoch} | \"\n        f\"Train PPL: {train_ppl:.3f} | \"\n        f\"Val PPL (TF=1): {val_ppl_tf1:.3f} | \"\n#        f\"Val PPL (free): {val_ppl_free:.3f} | \"\n    )\n\n    # --------- Sample generation + diversity metrics ---------\n\n    # Record accuracy\n    l_tot.append(val_ppl_tf1)\n    if val_ppl_tf1 < best_val :\n        best_val = val_ppl_tf1\n        torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": opti.state_dict(),\n                    \"scheduler_state_dict\": scheduler.state_dict(),\n                    \"val_ppl\": val_ppl_tf1,\n                },\n                \"model\",\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:29:26.354307Z","iopub.execute_input":"2025-11-08T20:29:26.354609Z","execution_failed":"2025-11-08T20:47:51.799Z"}},"outputs":[{"name":"stderr","text":"W1108 20:29:35.485000 38 torch/_inductor/utils.py:1137] [0/0_1] Not enough SMs to use max_autotune_gemm mode\n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train PPL: 204.002 | Val PPL (TF=1): 29.028 | \nEpoch 1 | Train PPL: 29.472 | Val PPL (TF=1): 17.826 | \nEpoch 2 | Train PPL: 21.168 | Val PPL (TF=1): 19.329 | \nEpoch 3 | Train PPL: 17.035 | Val PPL (TF=1): 11.866 | \nEpoch 4 | Train PPL: 12.227 | Val PPL (TF=1): 10.227 | \nEpoch 5 | Train PPL: 10.875 | Val PPL (TF=1): 9.417 | \nEpoch 6 | Train PPL: 10.071 | Val PPL (TF=1): 8.791 | \nEpoch 7 | Train PPL: 9.438 | Val PPL (TF=1): 8.286 | \nEpoch 8 | Train PPL: 8.901 | Val PPL (TF=1): 7.857 | \nEpoch 9 | Train PPL: 8.449 | Val PPL (TF=1): 7.498 | \nEpoch 10 | Train PPL: 8.072 | Val PPL (TF=1): 7.205 | \nEpoch 11 | Train PPL: 7.751 | Val PPL (TF=1): 6.959 | \nEpoch 12 | Train PPL: 7.478 | Val PPL (TF=1): 6.743 | \nEpoch 13 | Train PPL: 7.235 | Val PPL (TF=1): 6.543 | \nEpoch 14 | Train PPL: 7.014 | Val PPL (TF=1): 6.359 | \nEpoch 15 | Train PPL: 6.812 | Val PPL (TF=1): 6.182 | \nEpoch 16 | Train PPL: 6.627 | Val PPL (TF=1): 6.015 | \nEpoch 17 | Train PPL: 6.453 | Val PPL (TF=1): 5.858 | \nEpoch 18 | Train PPL: 6.294 | Val PPL (TF=1): 5.713 | \nEpoch 19 | Train PPL: 6.144 | Val PPL (TF=1): 5.568 | \nEpoch 20 | Train PPL: 6.004 | Val PPL (TF=1): 5.436 | \n","output_type":"stream"}],"execution_count":null}]}