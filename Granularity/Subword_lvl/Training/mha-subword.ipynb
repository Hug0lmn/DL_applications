{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will serve as a way to train subword generation MultiHead attention transformer + RoPe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, get_cosine_schedule_with_warmup\n",
    "from torchtune.modules import RotaryPositionalEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:36.905563Z",
     "iopub.status.busy": "2025-10-20T17:50:36.905169Z",
     "iopub.status.idle": "2025-10-20T17:50:36.940545Z",
     "shell.execute_reply": "2025-10-20T17:50:36.939731Z",
     "shell.execute_reply.started": "2025-10-20T17:50:36.905544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TOKENIZERS_PARALLELISM=True\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/kaggle/input/subword-rnn-lstm/rap_tokenizer.json\",\n",
    "    pad_token = \"<PAD>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:36.941430Z",
     "iopub.status.busy": "2025-10-20T17:50:36.941238Z",
     "iopub.status.idle": "2025-10-20T17:50:39.459021Z",
     "shell.execute_reply": "2025-10-20T17:50:39.458223Z",
     "shell.execute_reply.started": "2025-10-20T17:50:36.941407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_ = np.load(\"/kaggle/input/subword-rnn-lstm/encoded.npy\",\"r\")\n",
    "\n",
    "result = []\n",
    "\n",
    "for t in dataset_:\n",
    "    if t == tokenizer.convert_tokens_to_ids(\"α\") : \n",
    "        current = []\n",
    "        current.append(t)\n",
    "    elif t == tokenizer.convert_tokens_to_ids(\"θ\") :\n",
    "        current.append(t)\n",
    "        result.append(torch.tensor(current))\n",
    "    else :\n",
    "        current.append(t)\n",
    "if current:  \n",
    "    result.append(torch.tensor(current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:39.460239Z",
     "iopub.status.busy": "2025-10-20T17:50:39.459849Z",
     "iopub.status.idle": "2025-10-20T17:50:39.466072Z",
     "shell.execute_reply": "2025-10-20T17:50:39.465357Z",
     "shell.execute_reply.started": "2025-10-20T17:50:39.460215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SongDataset(Dataset):\n",
    "    def __init__(self, texts, length_seq, stride, use_offset=True):\n",
    "        self.samples = []\n",
    "        self.length_seq = length_seq\n",
    "        self.stride = stride\n",
    "\n",
    "        for text in texts:\n",
    "            L = len(text)\n",
    "\n",
    "            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n",
    "\n",
    "            # --- Boucle principale ---\n",
    "            for start in range(offset, max(1, L - self.length_seq), self.stride):\n",
    "                x_start, x_end = start, start + self.length_seq\n",
    "                y_start, y_end = start + 1, start + 1 + self.length_seq\n",
    "\n",
    "                x = text[x_start:x_end]\n",
    "                y = text[y_start:y_end]\n",
    "\n",
    "                self.samples.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:39.468058Z",
     "iopub.status.busy": "2025-10-20T17:50:39.467732Z",
     "iopub.status.idle": "2025-10-20T17:50:39.489370Z",
     "shell.execute_reply": "2025-10-20T17:50:39.488695Z",
     "shell.execute_reply.started": "2025-10-20T17:50:39.468025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len_train = int(len(result) * 0.8)\n",
    "len_test = len(result) - len_train\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train, test = random_split(result, [len_train, len_test], generator = generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Dataloader and co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:39.490131Z",
     "iopub.status.busy": "2025-10-20T17:50:39.489915Z",
     "iopub.status.idle": "2025-10-20T17:50:39.494538Z",
     "shell.execute_reply": "2025-10-20T17:50:39.493711Z",
     "shell.execute_reply.started": "2025-10-20T17:50:39.490115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch, tokenizer):\n",
    "    X, Y = zip(*batch)\n",
    "\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    Y_padded = pad_sequence(Y, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    batch_enc = [ X_padded,Y_padded]\n",
    "\n",
    "    return batch_enc\n",
    "\n",
    "# Create a callable version of collate_fn with your tokenizer\n",
    "collate_fn = partial(collate_batch, tokenizer=tokenizer)\n",
    "\n",
    "#Normally a function requires to specify the options at the initiation but partial allows to specify values for the required option that will\n",
    "#be stored and then be used when the function will be called\n",
    "# collate_fn(batch) == collate_batch(batch, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T19:40:37.074268Z",
     "iopub.status.busy": "2025-10-20T19:40:37.073480Z",
     "iopub.status.idle": "2025-10-20T19:40:39.357251Z",
     "shell.execute_reply": "2025-10-20T19:40:39.356612Z",
     "shell.execute_reply.started": "2025-10-20T19:40:37.074235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_ds = SongDataset(train, length_seq=256, stride = 4, use_offset = True) #The offset is smaller than for RNN's because the Transformer doesn't have the same assumption about data so it needs to see more to learn the same amount of data\n",
    "test_ds = SongDataset(test, length_seq=256, stride = 4, use_offset = False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", \n",
    "                        num_workers=4, prefetch_factor=4, shuffle=True, drop_last=True, collate_fn = collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", \n",
    "                       num_workers=2, prefetch_factor=2, shuffle=False, drop_last=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:46.504515Z",
     "iopub.status.busy": "2025-10-20T17:50:46.504156Z",
     "iopub.status.idle": "2025-10-20T17:50:46.511642Z",
     "shell.execute_reply": "2025-10-20T17:50:46.510827Z",
     "shell.execute_reply.started": "2025-10-20T17:50:46.504497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nheads, dropout, max_seq, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nheads = nheads\n",
    "        assert embed_dim % nheads == 0, \"Embedding dim is not divisible by nheads\"\n",
    "        self.head_dim = embed_dim // nheads\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.prob_drop = dropout\n",
    "        \n",
    "        self.proj_qkv = nn.Linear(embed_dim, embed_dim * 3, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        \n",
    "        self.rope = RotaryPositionalEmbeddings(dim=self.head_dim, max_seq_len = max_seq)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask=None, is_causal=True) -> torch.Tensor:\n",
    "        \n",
    "        # Step 1\n",
    "        B,L,_ = x.shape\n",
    "        result = self.proj_qkv(x)\n",
    "        q, k, v = torch.chunk(result, 3, dim=-1)\n",
    "\n",
    "        # Step 2\n",
    "        # (N, L_t, head_dim) -> (N, L_t, nheads, head_dim)\n",
    "        q,k,v = [t.reshape(B, L, self.nheads, self.head_dim) for t in (q,k,v)]\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        # (N, nheads, L_t, head_dim)\n",
    "        q,k,v = [t.transpose(1,2) for t in (q,k,v)]\n",
    "\n",
    "        # Step 3\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            q, k, v, dropout_p=self.prob_drop, attn_mask = attn_mask, is_causal=is_causal)\n",
    "        attn_output = attn_output.transpose(1, 2).flatten(-2)\n",
    "\n",
    "        return self.drop(self.out_proj(attn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:46.512763Z",
     "iopub.status.busy": "2025-10-20T17:50:46.512541Z",
     "iopub.status.idle": "2025-10-20T17:50:46.531517Z",
     "shell.execute_reply": "2025-10-20T17:50:46.530760Z",
     "shell.execute_reply.started": "2025-10-20T17:50:46.512748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.d_ff = d_model * 4\n",
    "        self.norm = nn.RMSNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout, max_seq=256)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, self.d_ff),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(self.d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # masked multi-head self-attention\n",
    "        x = x + self.attn(self.norm(x), is_causal=True) #Is causal = for token t, mask on every tokens after (cannot see what's coming after)\n",
    "        # feed-forward\n",
    "        x = x + self.ff(self.norm(x))\n",
    "        return x\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=4, max_len=256):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(max_len, d_model)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads) for _ in range(n_layers)])\n",
    "        \n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.norm = nn.RMSNorm(d_model)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size, bias = False)\n",
    "        self.fc.weight = self.emb.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.emb(x))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:46.532519Z",
     "iopub.status.busy": "2025-10-20T17:50:46.532300Z",
     "iopub.status.idle": "2025-10-20T17:50:46.544906Z",
     "shell.execute_reply": "2025-10-20T17:50:46.544304Z",
     "shell.execute_reply.started": "2025-10-20T17:50:46.532502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:46.545976Z",
     "iopub.status.busy": "2025-10-20T17:50:46.545684Z",
     "iopub.status.idle": "2025-10-20T17:50:46.910684Z",
     "shell.execute_reply": "2025-10-20T17:50:46.909976Z",
     "shell.execute_reply.started": "2025-10-20T17:50:46.545914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 4000\n",
    "num_epoch = 100\n",
    "\n",
    "nb_step_train = len(train_dl)\n",
    "nb_step_test = len(test_dl)\n",
    "\n",
    "total_steps = nb_step_train * num_epoch\n",
    "\n",
    "model = DecoderOnlyTransformer(vocab_size, d_model=320, n_heads=5, n_layers=6).to(device) \n",
    "model = torch.compile(model)\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.05)\n",
    "val_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "opti = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=2e-3)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    opti,\n",
    "    num_warmup_steps=int(0.05 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:46.911691Z",
     "iopub.status.busy": "2025-10-20T17:50:46.911450Z",
     "iopub.status.idle": "2025-10-20T17:50:46.915908Z",
     "shell.execute_reply": "2025-10-20T17:50:46.915151Z",
     "shell.execute_reply.started": "2025-10-20T17:50:46.911670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#ckpt = torch.load(\"/kaggle/input/warmed-up-sub-mh1/pytorch/default/3/MHA_model_warmed_up.pt\", map_location=device)\n",
    "#model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "#last_epoch = ckpt[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:46.918723Z",
     "iopub.status.busy": "2025-10-20T17:50:46.918468Z",
     "iopub.status.idle": "2025-10-20T17:50:46.927438Z",
     "shell.execute_reply": "2025-10-20T17:50:46.926809Z",
     "shell.execute_reply.started": "2025-10-20T17:50:46.918699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_with_temp(logits, temp=1.0):\n",
    "    probs = (logits / temp).softmax(dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token\n",
    "\n",
    "def distinct_n_chars(text, n=3):\n",
    "    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    return len(set(ngrams)) / max(1, len(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:50:46.928886Z",
     "iopub.status.busy": "2025-10-20T17:50:46.928404Z",
     "iopub.status.idle": "2025-10-20T17:50:46.941658Z",
     "shell.execute_reply": "2025-10-20T17:50:46.940906Z",
     "shell.execute_reply.started": "2025-10-20T17:50:46.928862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training(model, train_dl, loss_fn, vocab_size, opti, scaler, scheduler, device) :\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X,Y in iter(train_dl) :\n",
    "        \n",
    "        X,Y = X.to(device), Y.to(device, dtype=torch.long)\n",
    "        opti.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        scaler.step(opti)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.detach().item() \n",
    "\n",
    "    return train_loss, scaler\n",
    "\n",
    "def evaluate(model, eval_dl, loss_fn, vocab_size, device) :\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "\n",
    "    for X,Y in iter(eval_dl) :\n",
    "        \n",
    "        X,Y = X.to(device), Y.to(device, dtype=torch.long)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "\n",
    "        eval_loss += loss.detach().item() \n",
    "\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T19:40:55.957043Z",
     "iopub.status.busy": "2025-10-20T19:40:55.956236Z",
     "iopub.status.idle": "2025-10-20T19:53:04.738144Z",
     "shell.execute_reply": "2025-10-20T19:53:04.736552Z",
     "shell.execute_reply.started": "2025-10-20T19:40:55.957013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "list_offset = []\n",
    "l_tot = []\n",
    "teacher_forcing_ratio = 1\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # -------------- TRAIN LOOP --------------\n",
    "    train_loss, scaler = training(model, train_dl, loss_fn, vocab_size, opti, scaler, scheduler, device)\n",
    "    train_ppl = math.exp(train_loss / nb_step_train)\n",
    "    \n",
    "    # -------------- VALIDATION --------------\n",
    "    with torch.no_grad() :      \n",
    "        val_loss = evaluate(model, test_dl, val_fn, vocab_size, device)\n",
    "        val_ppl = math.exp(val_loss / nb_step_test)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"Train PPL: {train_ppl:.3f} | \"\n",
    "        f\"Val PPL : {val_ppl:.3f}\"\n",
    "    )\n",
    "\n",
    "    # --------- Sample generation + diversity metrics ---------\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "        # Context priming\n",
    "            X, _ = next(iter(test_dl))\n",
    "            start = X[0:1, :20].to(device)\n",
    "\n",
    "        #Generating\n",
    "            context = start\n",
    "            for _ in range(200):  \n",
    "                logits = model(context)\n",
    "                next_token = sample_with_temp(logits[:, -1, :], temp=0.6)\n",
    "                context = torch.cat([context, next_token], dim=1)\n",
    "\n",
    "            gen_text = tokenizer.decode(context[0, 20:].tolist())\n",
    "            \n",
    "        d2 = distinct_n_chars(gen_text, n=2)\n",
    "        d3 = distinct_n_chars(gen_text, n=3)\n",
    "\n",
    "        print(\"\\n=== Initial text ===\")\n",
    "        print(\"\".join(tokenizer.decode(X[0:1,:20].squeeze(0))))\n",
    "        print(\"\\n=== Sample Generation ===\")\n",
    "        print(gen_text[:200]) \n",
    "        print(f\"Distinct-2: {d2:.3f} | Distinct-3: {d3:.3f}\", end=\"\\n\")\n",
    "\n",
    "    # Record accuracy\n",
    "    l_tot.append(val_ppl)\n",
    "    if val_ppl < best_val :\n",
    "        best_val = val_ppl\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": opti.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"val_ppl\": val_ppl,\n",
    "                },\n",
    "                \"model\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8348047,
     "sourceId": 13425138,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 476868,
     "modelInstanceId": 461101,
     "sourceId": 614147,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
