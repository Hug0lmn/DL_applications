{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T13:20:43.767688Z",
     "iopub.status.busy": "2025-08-26T13:20:43.767446Z",
     "iopub.status.idle": "2025-08-26T13:20:48.176921Z",
     "shell.execute_reply": "2025-08-26T13:20:48.176138Z",
     "shell.execute_reply.started": "2025-08-26T13:20:43.767664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..')))\n",
    "from Functions_generation import generate_a_song_structure, sample_with_temp_topk, Subword_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T13:20:48.179209Z",
     "iopub.status.busy": "2025-08-26T13:20:48.178868Z",
     "iopub.status.idle": "2025-08-26T13:20:48.204248Z",
     "shell.execute_reply": "2025-08-26T13:20:48.203458Z",
     "shell.execute_reply.started": "2025-08-26T13:20:48.179190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../../../Corpus/Encoding_RNN_LSTM/Subword/rap_tokenizer.json\",\n",
    "    pad_token = \"<PAD>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_csv(\"../../../Markov/transition_matrix.csv\")\n",
    "\n",
    "states = np.array(matrix.iloc[6])\n",
    "prob_transi = np.array(matrix.iloc[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "model = Subword_LSTM(vocab_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"../Models/LSTM_model.pt\", map_location=\"cpu\")\n",
    "\n",
    "new_state_dict = {}\n",
    "for key, value in ckpt[\"model_state_dict\"].items():\n",
    "    new_key = key.replace(\"_orig_mod.\", \"\")           \n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "ckpt[\"model_state_dict\"] = new_state_dict\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = generate_a_song_structure(prob_transi.astype(float),states)\n",
    "\n",
    "#struct = ['<BEGINNING>', '<COUPLET>', '<REFRAIN>', '<COUPLET>', '<REFRAIN>', '<END>']\n",
    "struct = ['α', 'γ', 'ε', 'γ', 'ε', 'θ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forbidden to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_gen = tokenizer.encode(\"\".join(['α', 'β', 'γ', 'ε', 'ζ', 'η', 'θ']))\n",
    "\n",
    "vocab = tokenizer.get_vocab()  \n",
    "tokens_with_slash = [tokenizer.encode(tok)[0] for tok in vocab.keys() if \"/\" in tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decod_structure = {\"β\" : \"<INTRO>\",\n",
    "                   \"γ\" : \"<COUPLET>\",\n",
    "                   \"ε\" : \"<REFRAIN>\",\n",
    "                   \"ζ\" : \"<PONT>\",\n",
    "                   \"η\" : \"<OUTRO>\",\n",
    "                   \"θ\" : \"<END>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_gen = \"aujourd'hui nous faisons\"\n",
    "context_encode = tokenizer.encode(context_gen)\n",
    "context_size = len(context_encode)\n",
    "\n",
    "hid = model.init_hidden(batch_size=1, device=\"cpu\")\n",
    "\n",
    "for i in range(len(struct) - 2):\n",
    "\n",
    "    print()\n",
    "    print(decod_structure[struct[i+1]])\n",
    "\n",
    "    if i == 0 : \n",
    "        context_encode.insert(0,tokenizer.encode(struct[i+1])[0])\n",
    "        context_encode.insert(1,tokenizer.encode(\"\\n\")[0])\n",
    "        encoded = torch.tensor(context_encode, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        print(\"\".join([tokenizer.decode(context_encode[1:])]), end=\"\")\n",
    "\n",
    "        out, hid = model(encoded, hid)\n",
    "        next_input = encoded[:, -1].unsqueeze(0)  # shape (1, 1)\n",
    "\n",
    "    else : \n",
    "        next_input = torch.tensor(tokenizer.encode(struct[i+1])[0],dtype=torch.long).view(1,1)\n",
    "\n",
    "    last_input = 0\n",
    "    first_input = True\n",
    "\n",
    "    while last_input not in tokens_with_slash:\n",
    "        with torch.no_grad():\n",
    "            out, hid = model(next_input, hid)   # out shape: (1, 1, vocab_size)\n",
    "\n",
    "            out[0, -1][forbidden_gen] = float(\"-inf\")\n",
    "\n",
    "        # 4. On échantillonne un token\n",
    "            next_token = sample_with_temp_topk(out[0, -1], temperature=0.5, top_k=20)\n",
    "            last_input = next_token.item()\n",
    "\n",
    "        # 5. Conditions d’impression\n",
    "            if first_input and last_input in tokens_with_slash:  # Si début et le modèle sort directement <END>\n",
    "                last_input = 0\n",
    "                continue\n",
    "\n",
    "            elif next_token.item() != tokens_with_slash:\n",
    "                first_input = False\n",
    "                print(tokenizer.decode(next_token.item()), end=\"\")\n",
    "\n",
    "        # 6. Préparer la prochaine entrée\n",
    "            next_input = next_token.view(1, 1)\n",
    "\n",
    "print(\"\\n\", decod_structure[struct[-1]])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8138327,
     "sourceId": 12873076,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
