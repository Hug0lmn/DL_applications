{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3085aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlm/Documents/Stanford Course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..')))\n",
    "from Functions_generation import generate_a_song_structure, sample_with_temp_topk, load_and_clean, Subword_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d325f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance_metric (ppl) :  50.80905611509434\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model = Subword_Models(model_type='LSTM')\n",
    "ckpt = load_and_clean(\"../Models/LSTM_model.pt\")\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../../../Corpus/Encoding_RNN_LSTM/Subword/rap_tokenizer.json\",\n",
    "    pad_token = \"<PAD>\")\n",
    "\n",
    "#Load states transition matrix\n",
    "matrix = pd.read_csv(\"../../../Markov/transition_matrix.csv\")\n",
    "\n",
    "states = np.array(matrix.iloc[6])\n",
    "prob_transi = np.array(matrix.iloc[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93902e",
   "metadata": {},
   "source": [
    "I will give the same structure for RNN and LSTM to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b0ebb",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "struct = generate_a_song_structure(prob_transi.astype(float),states)\n",
    "\n",
    "struct = ['<BEGINNING>', '<COUPLET>', '<REFRAIN>', '<COUPLET>', '<REFRAIN>', '<END>']\n",
    "struct = ['α', 'γ', 'ε', 'γ', 'ε', 'θ']\n",
    "\n",
    "encoded = [tokenizer.encode(c)[0] for c in struct]\n",
    "\n",
    "decod_structure = {\"β\" : \"<INTRO>\",\n",
    "                   \"γ\" : \"<COUPLET>\",\n",
    "                   \"ε\" : \"<REFRAIN>\",\n",
    "                   \"ζ\" : \"<PONT>\",\n",
    "                   \"η\" : \"<OUTRO>\",\n",
    "                   \"θ\" : \"<END>\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Me voici devant toi\\nTu es mort dans le livre\\n\"\n",
    "context_ids = tokenizer.encode(context, add_special_tokens=False)\n",
    "id_not_penalized = [211, 26, 43, 1474, 24, 567]\n",
    "forbidden_ids = [i for i in range(2,8)]\n",
    "end_of_gen = [i for i in range(8,13)]+[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, context : str, max_token : int = 200, \n",
    "                  temperature : float = 0.8, rep_penalty : float = 1.2, rep_penalty_less : float = 1.05,\n",
    "                  top_k = 40, seed = 42, forbidden_ids = forbidden_ids, id_not_penalized = id_not_penalized): \n",
    "\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    forbidden_ids = set(forbidden_ids or [])\n",
    "\n",
    "    # encode context\n",
    "    context_ids = tokenizer.encode(context.lower(), add_special_tokens=False)\n",
    "    # init hidden\n",
    "    hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_input = None\n",
    "        if model.model._get_name() == \"Subword_LSTM\" : \n",
    "            for tid in context_ids:\n",
    "                inp = torch.tensor([tid], dtype=torch.long)\n",
    "                out, hid = model(inp, hid)\n",
    "                #set last_input to last token of context\n",
    "                last_input = inp\n",
    "        elif model.model._get_name() == \"Subword_RNN\" :\n",
    "            last_input = None\n",
    "            inp = torch.tensor([context_ids], dtype=torch.long)\n",
    "            out, hid = model(inp, hid)\n",
    "            # set last_input to last token of context\n",
    "            last_input = inp.squeeze(0)[-1].unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    generated = []\n",
    "    token_counts = {}\n",
    "\n",
    "    for step in range(max_token):\n",
    "        with torch.no_grad():\n",
    "            out, hid = model(last_input, hid)\n",
    "            if out.ndim == 3: #RNN out\n",
    "                logits = out[0, -1].clone()\n",
    "            else: #LSTM out\n",
    "                logits = out.squeeze().clone()\n",
    "\n",
    "            if forbidden_ids:\n",
    "                logits[list(forbidden_ids)] = float(\"-inf\") #prob_forbidden = 0\n",
    "\n",
    "            # scale logits of previously generated tokens\n",
    "            if rep_penalty > 1.0 and token_counts:\n",
    "                for tid, count in token_counts.items():\n",
    "                    # reduce logits progressively\n",
    "                    if tid not in id_not_penalized:\n",
    "                        logits[tid] = logits[tid] / (rep_penalty ** count)\n",
    "                    else :\n",
    "                        logits[tid] = logits[tid] / (rep_penalty_less ** count)\n",
    "\n",
    "            # apply temperature\n",
    "            logits = logits / max(temperature, 1e-8)\n",
    "\n",
    "            # top-k filtering\n",
    "            if top_k > 0:\n",
    "                values, indices = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                kth_value = values[-1]\n",
    "                mask = logits < kth_value\n",
    "                logits[mask] = float(\"-inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            generated.append(next_id)\n",
    "            token_counts[next_id] = token_counts.get(next_id, 0) + 1\n",
    "\n",
    "            # prepare next input\n",
    "            if model.model._get_name() == \"Subword_LSTM\" :\n",
    "                last_input = torch.tensor([next_id], dtype=torch.long)\n",
    "\n",
    "            elif model.model._get_name() == \"Subword_RNN\" :\n",
    "                last_input = torch.tensor([[next_id]], dtype=torch.long)\n",
    "\n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return context+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f8eaf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me voici devant toi\n",
      "tu es mort dans le livre\n",
      "j'ai le sang froid et l'soir, j'te dirais pas tout c'monde, yeah\n",
      "pas de blem', t'as plus peur du ciel\n",
      "si ça m'demande: il fait notre beau mais est toujours pour-co\n",
      "hun, je vais pas les plombs\n",
      "je voudrais qu'tu perceptes la tête avec un bon spliff\n",
      "quand tu fais mon rêve sans faire un grand frère qui dit quoi?\n",
      "qu'est-ce tu veux que ça soit bizarre, c'est une clope à la page, ouvre des gros sacrimes secondes\n",
      "les enfants d'reviendrai jamais seul dans ma tête vide\n",
      "pour moi ce matin j'veux juste m'dois penser en eux, nananana comme tony sur mes chillers\n",
      "on s'reste après le bip,  on a aucun troisième pétard et tous mes potes au quartier de serrer ta mère mais ils sont content\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,tokenizer,context, rep_penalty = 1.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
