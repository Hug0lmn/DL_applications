{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1a3085aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..')))\n",
    "from Functions_generation import generate_a_song_structure, sample_with_temp_topk, load_and_clean, Subword_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "82d325f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance_metric (ppl) :  54.306537798841084\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model = Subword_Models(model_type='RNN')\n",
    "ckpt = load_and_clean(\"../Models/RNN_model.pt\")\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../../../Corpus/Encoding_RNN_LSTM/Subword/rap_tokenizer.json\",\n",
    "    pad_token = \"<PAD>\")\n",
    "\n",
    "#Load states transition matrix\n",
    "matrix = pd.read_csv(\"../../../Markov/transition_matrix.csv\")\n",
    "\n",
    "states = np.array(matrix.iloc[6])\n",
    "prob_transi = np.array(matrix.iloc[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93902e",
   "metadata": {},
   "source": [
    "I will give the same structure for RNN and LSTM to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b0ebb",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "struct = generate_a_song_structure(prob_transi.astype(float),states)\n",
    "\n",
    "struct = ['<BEGINNING>', '<COUPLET>', '<REFRAIN>', '<COUPLET>', '<REFRAIN>', '<END>']\n",
    "struct = ['α', 'γ', 'ε', 'γ', 'ε', 'θ']\n",
    "\n",
    "encoded = [tokenizer.encode(c)[0] for c in struct]\n",
    "\n",
    "decod_structure = {\"β\" : \"<INTRO>\",\n",
    "                   \"γ\" : \"<COUPLET>\",\n",
    "                   \"ε\" : \"<REFRAIN>\",\n",
    "                   \"ζ\" : \"<PONT>\",\n",
    "                   \"η\" : \"<OUTRO>\",\n",
    "                   \"θ\" : \"<END>\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8f17b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"me voici devant toi\\ntu es mort dans le livre\\n\"\n",
    "context_ids = tokenizer.encode(context, add_special_tokens=False)\n",
    "id_not_penalized = [211, 26, 43, 1474, 24, 567]\n",
    "forbidden_ids = [i for i in range(2,8)]\n",
    "end_of_gen = [i for i in range(8,13)]+[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0eda404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, context : str, max_token : int = 200, \n",
    "                  temperature : float = 0.8, rep_penalty : float = 1.2, rep_penalty_less : float = 1.05,\n",
    "                  top_k = 40, seed = 42, forbidden_ids = forbidden_ids, id_not_penalized = id_not_penalized): \n",
    "\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    forbidden_ids = set(forbidden_ids or [])\n",
    "\n",
    "    # encode context\n",
    "    context_ids = tokenizer.encode(context.lower(), add_special_tokens=False)\n",
    "    # init hidden\n",
    "    hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_input = None\n",
    "        if model.model._get_name() == \"Subword_LSTM\" : \n",
    "            for tid in context_ids:\n",
    "                inp = torch.tensor([tid], dtype=torch.long)\n",
    "                out, hid = model(inp, hid)\n",
    "                #set last_input to last token of context\n",
    "                last_input = inp\n",
    "        elif model.model._get_name() == \"Subword_RNN\" :\n",
    "            last_input = None\n",
    "            inp = torch.tensor([context_ids], dtype=torch.long)\n",
    "            out, hid = model(inp, hid)\n",
    "            # set last_input to last token of context\n",
    "            last_input = inp.squeeze(0)[-1].unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    generated = []\n",
    "    token_counts = {}\n",
    "\n",
    "    for step in range(max_token):\n",
    "        with torch.no_grad():\n",
    "            out, hid = model(last_input, hid)\n",
    "            if out.ndim == 3: #RNN out\n",
    "                logits = out[0, -1].clone()\n",
    "            else: #LSTM out\n",
    "                logits = out.squeeze().clone()\n",
    "\n",
    "            if forbidden_ids:\n",
    "                logits[list(forbidden_ids)] = float(\"-inf\") #prob_forbidden = 0\n",
    "\n",
    "            # scale logits of previously generated tokens\n",
    "            if rep_penalty > 1.0 and token_counts:\n",
    "                for tid, count in token_counts.items():\n",
    "                    # reduce logits progressively\n",
    "                    if tid not in id_not_penalized:\n",
    "                        logits[tid] = logits[tid] / (rep_penalty ** count)\n",
    "                    else :\n",
    "                        logits[tid] = logits[tid] / (rep_penalty_less ** count)\n",
    "\n",
    "            # apply temperature\n",
    "            logits = logits / max(temperature, 1e-8)\n",
    "\n",
    "            # top-k filtering\n",
    "            if top_k > 0:\n",
    "                values, indices = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                kth_value = values[-1]\n",
    "                mask = logits < kth_value\n",
    "                logits[mask] = float(\"-inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            generated.append(next_id)\n",
    "            token_counts[next_id] = token_counts.get(next_id, 0) + 1\n",
    "\n",
    "            # prepare next input\n",
    "            if model.model._get_name() == \"Subword_LSTM\" :\n",
    "                last_input = torch.tensor([next_id], dtype=torch.long)\n",
    "\n",
    "            elif model.model._get_name() == \"Subword_RNN\" :\n",
    "                last_input = torch.tensor([[next_id]], dtype=torch.long)\n",
    "\n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return context+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2f8eaf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me voici devant toi\n",
      "tu es mort dans le livre\n",
      "j'ai le quartier, et j'suis tout à la fin saint-o\n",
      "j'me réveille loin comme un oseille\n",
      "et nous tu connais ma ville m'attrape avec les hommes de merde\n",
      "les yeux fermés et des larmes de kaïn\n",
      "\n",
      " re\n",
      "cousin s.i.p oligy l'eserx en cuir dabedar, dans la rame danichie\n",
      "la plupart du désespoir, y a pas trop d'temps quand on côtoie le meilleur chemin que j'déprime, c'tait bien\n",
      "\n",
      "t\n",
      "la vie c'est pas très mal-haut?\n",
      "il disait: ça change rien\n",
      "rien n'sert peut être rempli jusquainau week\n",
      "sur qui tu fais quoi?? tpa pour quasi amateur, on a su l'regardre et tous mes potes au gourou!\n",
      "deg-ju\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,tokenizer,context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_repetition_penalty(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context: str = \"\",\n",
    "    max_tokens: int = 200,\n",
    "    temperature: float = 0.8,\n",
    "    top_k: Optional[int] = 40,\n",
    "    rep_penalty: float = 1.2,\n",
    "    ngram_block: Optional[int] = 3,\n",
    "    forbidden_ids: Optional[List[int]] = None,\n",
    "    device: Optional[str] = None,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tokens step-by-step from a Subword RNN/LSTM model while penalizing repetitions.\n",
    "\n",
    "    - rep_penalty > 1 reduces probability of tokens already generated (frequency-based).\n",
    "    - ngram_block = n will block generation of any token that would recreate a previously seen n-gram.\n",
    "    - forbidden_ids: list of token ids to always ban (set logit = -inf).\n",
    "    \"\"\"\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "    # prepare model + tokenizer\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    forbidden_ids = set(forbidden_ids or [])\n",
    "\n",
    "    # encode context\n",
    "    context_ids = tokenizer.encode(context, add_special_tokens=False)\n",
    "    # init hidden\n",
    "    hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "    # prime model with context tokens (token-by-token)\n",
    "    with torch.no_grad():\n",
    "        last_input = None\n",
    "        for tid in context_ids:\n",
    "            inp = torch.tensor([[tid]], dtype=torch.long, device=device)\n",
    "            out, hid = model(inp, hid)\n",
    "            # set last_input to last token of context\n",
    "            last_input = inp\n",
    "\n",
    "        # if no context, pick a non-pad start token (id != 0)\n",
    "        if last_input is None:\n",
    "            start_id = 1 if tokenizer.pad_token_id == 0 else tokenizer.pad_token_id + 1\n",
    "            last_input = torch.tensor([[start_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    generated = []\n",
    "    token_counts = {}\n",
    "    seen_ngrams = set()  # store tuples of length ngram_block\n",
    "    if ngram_block and len(context_ids) >= ngram_block:\n",
    "        # initialize seen_ngrams from context\n",
    "        for i in range(len(context_ids) - ngram_block + 1):\n",
    "            seen_ngrams.add(tuple(context_ids[i:i+ngram_block]))\n",
    "\n",
    "    for step in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            out, hid = model(last_input, hid)\n",
    "            # normalize logits retrieval (handle (1,1,V) or (1,V) shapes)\n",
    "            if out.ndim == 3:\n",
    "                logits = out[0, -1].clone()\n",
    "            elif out.ndim == 2:\n",
    "                logits = out[0].clone()\n",
    "            else:\n",
    "                logits = out.squeeze().clone()\n",
    "\n",
    "            vocab_size = logits.shape[-1]\n",
    "\n",
    "            # ban forbidden ids (pad / special)\n",
    "            if forbidden_ids:\n",
    "                logits[list(forbidden_ids)] = float(\"-inf\")\n",
    "\n",
    "            # apply n-gram blocking: for current prefix (last n-1 tokens), ban candidates that recreate seen n-gram\n",
    "            if ngram_block and ngram_block > 1 and len(generated) >= ngram_block - 1:\n",
    "                prefix = tuple(generated[-(ngram_block-1):]) if ngram_block > 1 else ()\n",
    "                # for efficiency: build mask over vocab where prefix + token in seen_ngrams\n",
    "                if prefix:\n",
    "                    # iterate tokens that would form a seen ngram\n",
    "                    for tok_id in range(vocab_size):\n",
    "                        if tuple(prefix + (tok_id,)) in seen_ngrams:\n",
    "                            logits[tok_id] = float(\"-inf\")\n",
    "\n",
    "            # frequency-based repetition penalty: scale logits of previously generated tokens\n",
    "            if rep_penalty is not None and rep_penalty > 1.0 and token_counts:\n",
    "                for tid, count in token_counts.items():\n",
    "                    # reduce logits progressively; dividing by rep_penalty**count lowers its prob\n",
    "                    logits[tid] = logits[tid] / (rep_penalty ** count)\n",
    "\n",
    "            # apply temperature\n",
    "            logits = logits / max(temperature, 1e-8)\n",
    "\n",
    "            # top-k filtering\n",
    "            if top_k is not None and top_k > 0:\n",
    "                values, indices = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                kth_value = values[-1]\n",
    "                mask = logits < kth_value\n",
    "                logits[mask] = float(\"-inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            # if all -inf -> break\n",
    "            if torch.isnan(probs).any() and torch.sum(~torch.isfinite(logits)) == logits.numel():\n",
    "                break\n",
    "\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # update bookkeeping\n",
    "        generated.append(next_id)\n",
    "        token_counts[next_id] = token_counts.get(next_id, 0) + 1\n",
    "        # update seen ngrams\n",
    "        if ngram_block and ngram_block > 1 and len(generated) >= ngram_block:\n",
    "            seen_ngrams.add(tuple(generated[-ngram_block:]))\n",
    "\n",
    "        # prepare next input\n",
    "        last_input = torch.tensor([[next_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return text, generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
