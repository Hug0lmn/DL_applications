{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3085aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "import pandas as pd\n",
    "#from Functions_generation import generate_a_song_structure, sample_with_temp_topk, Subword_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..')))\n",
    "from Functions_generation import generate_a_song_structure, sample_with_temp_topk, Subword_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854856be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../../../Corpus/Encoding_RNN_LSTM/Subword/rap_tokenizer.json\",\n",
    "    pad_token = \"<PAD>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b090ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_csv(\"../../../Markov/transition_matrix.csv\")\n",
    "\n",
    "states = np.array(matrix.iloc[6])\n",
    "prob_transi = np.array(matrix.iloc[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "model = Subword_RNN(vocab_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"../Models/RNN_model.pt\", map_location=\"cpu\")\n",
    "\n",
    "new_state_dict = {}\n",
    "for key, value in ckpt[\"model_state_dict\"].items():\n",
    "    new_key = key.replace(\"_orig_mod.\", \"\")           \n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "ckpt[\"model_state_dict\"] = new_state_dict\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93902e",
   "metadata": {},
   "source": [
    "I will give the same structure for RNN and LSTM to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = generate_a_song_structure(prob_transi.astype(float),states)\n",
    "\n",
    "#struct = ['<BEGINNING>', '<COUPLET>', '<REFRAIN>', '<COUPLET>', '<REFRAIN>', '<END>']\n",
    "struct = ['α', 'γ', 'ε', 'γ', 'ε', 'θ']\n",
    "\n",
    "encoded = torch.tensor([mapping[c] for c in struct[1]], dtype=torch.long).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54be15",
   "metadata": {},
   "source": [
    "Forbidden to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f182654",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_generation = [i for i in range(64,72)]\n",
    "[int2char[i] for i in forbidden_generation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf163652",
   "metadata": {},
   "outputs": [],
   "source": [
    "decod_structure = {\"β\" : \"<INTRO>\",\n",
    "                   \"γ\" : \"<COUPLET>\",\n",
    "                   \"ε\" : \"<REFRAIN>\",\n",
    "                   \"ζ\" : \"<PONT>\",\n",
    "                   \"η\" : \"<OUTRO>\",\n",
    "                   \"θ\" : \"<END>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_gen = \"voila pourquoi\"\n",
    "context_encode = [mapping[i] for i in context_gen]\n",
    "context_size = len(context_encode)\n",
    "\n",
    "hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "for i in range(len(struct) - 2):\n",
    "\n",
    "    print()\n",
    "    print(decod_structure[struct[i+1]])\n",
    "\n",
    "    if i == 0 : \n",
    "        context_encode.insert(0,mapping[struct[i+1]])\n",
    "        context_encode.insert(1,mapping[\"\\n\"])\n",
    "        encoded = torch.tensor(context_encode, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        print(\"\".join([int2char[i] for i in context_encode[1:]]), end=\"\")\n",
    "\n",
    "        out, hid = model(encoded, hid)\n",
    "        next_input = encoded[:, -1].unsqueeze(0)  # shape (1, 1)\n",
    "\n",
    "    else : \n",
    "        #next_input = torch.tensor(mapping[struct[i+1]],dtype=torch.long).unsqueeze(0)\n",
    "        next_input = torch.tensor(mapping[struct[i+1]],dtype=torch.long).view(1,1)\n",
    "\n",
    "    last_input = 0\n",
    "    first_input = True\n",
    "\n",
    "    while last_input != 12:\n",
    "        with torch.no_grad():\n",
    "            out, hid = model(next_input, hid)   # out shape: (1, 1, vocab_size)\n",
    "\n",
    "            out[0, -1][forbidden_generation] = float(\"-inf\")\n",
    "\n",
    "        # 4. On échantillonne un token\n",
    "            next_token = sample_with_temp_topk(out[0, -1], temperature=0.5, top_k=20)\n",
    "            last_input = next_token.item()\n",
    "\n",
    "        # 5. Conditions d’impression\n",
    "            if first_input and last_input == 12:  # Si début et le modèle sort directement <END>\n",
    "                last_input = 0\n",
    "                continue\n",
    "\n",
    "            elif first_input and last_input == 0:  # Si début et sortie = \\n\n",
    "                first_input = False\n",
    "                forbidden_generation.extend([0])\n",
    "                print(int2char[next_token.item()], end=\"\")\n",
    "\n",
    "            elif next_token.item() != 12:\n",
    "                first_input = False\n",
    "                print(int2char[next_token.item()], end=\"\")\n",
    "\n",
    "                if 0 in forbidden_generation:\n",
    "                    forbidden_generation.pop(-1)\n",
    "\n",
    "        # 6. Préparer la prochaine entrée\n",
    "            next_input = next_token.view(1, 1)\n",
    "\n",
    "print(\"\\n\", decod_structure[struct[-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce69ad8",
   "metadata": {},
   "source": [
    "# If no restriction on the generation part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c844fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_gen = \"malice\"\n",
    "context_encode = [mapping[i] for i in context_gen]\n",
    "\n",
    "hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "encoded = torch.tensor(context_encode, dtype=torch.long).unsqueeze(0)\n",
    "print(\"\".join([int2char[i] for i in context_encode]), end=\"\")\n",
    "\n",
    "out, hid = model(encoded, hid)\n",
    "next_input = encoded[:, -1].unsqueeze(0) \n",
    "\n",
    "last_input = 0\n",
    "first_input = True\n",
    "\n",
    "while last_input != 70:\n",
    "    with torch.no_grad():\n",
    "        out, hid = model(next_input, hid) \n",
    "\n",
    "        next_token = sample_with_temp_topk(out[0, -1], temperature=0.5, top_k=20)\n",
    "\n",
    "        if (last_input == 0) and (next_token != list(decod_structure.keys())):\n",
    "            print(int2char[next_token.item()].upper(), end=\"\")\n",
    "        elif last_input != 70 : \n",
    "            print(int2char[next_token.item()], end=\"\")\n",
    "\n",
    "        last_input = next_token.item()\n",
    "        next_input = next_token.view(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aaa335",
   "metadata": {},
   "source": [
    "# Teacher forcing < 1 toward the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"..\\Models\\RNN_model_not_full_forced.pt\", map_location=\"cpu\")\n",
    "new_state_dict = {}\n",
    "for k, v in ckpt[\"model_state_dict\"].items():\n",
    "    new_key = k.replace(\"_orig_mod.\", \"\")  # supprime le préfixe \"origin.\"\n",
    "    new_state_dict[new_key] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt = torch.load(\"..\\Models\\LSTM_model_not_full_forced.pt\", map_location=\"cpu\")\n",
    "#torch.compile(model)\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d25db6",
   "metadata": {},
   "source": [
    "Forbidden to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0025751",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_gen = \"voila pourquoi\"\n",
    "context_encode = [mapping[i] for i in context_gen]\n",
    "context_size = len(context_encode)\n",
    "\n",
    "hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "for i in range(len(struct) - 2):\n",
    "\n",
    "    print()\n",
    "    print(decod_structure[struct[i+1]])\n",
    "\n",
    "    if i == 0 : \n",
    "        context_encode.insert(0,mapping[struct[i+1]])\n",
    "        context_encode.insert(1,mapping[\"\\n\"])\n",
    "        encoded = torch.tensor(context_encode, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        print(\"\".join([int2char[i] for i in context_encode[1:]]), end=\"\")\n",
    "\n",
    "        out, hid = model(encoded, hid)\n",
    "        next_input = encoded[:, -1].unsqueeze(0)  # shape (1, 1)\n",
    "\n",
    "    else : \n",
    "        #next_input = torch.tensor(mapping[struct[i+1]],dtype=torch.long).unsqueeze(0)\n",
    "        next_input = torch.tensor(mapping[struct[i+1]],dtype=torch.long).view(1,1)\n",
    "\n",
    "    last_input = 0\n",
    "    first_input = True\n",
    "\n",
    "    while last_input != 12:\n",
    "        with torch.no_grad():\n",
    "            out, hid = model(next_input, hid)   # out shape: (1, 1, vocab_size)\n",
    "\n",
    "            out[0, -1][forbidden_generation] = float(\"-inf\")\n",
    "\n",
    "        # 4. On échantillonne un token\n",
    "            next_token = sample_with_temp_topk(out[0, -1], temperature=0.5, top_k=20)\n",
    "            last_input = next_token.item()\n",
    "\n",
    "        # 5. Conditions d’impression\n",
    "            if first_input and last_input == 12:  # Si début et le modèle sort directement <END>\n",
    "                last_input = 0\n",
    "                continue\n",
    "\n",
    "            elif first_input and last_input == 0:  # Si début et sortie = \\n\n",
    "                first_input = False\n",
    "                forbidden_generation.extend([0])\n",
    "                print(int2char[next_token.item()], end=\"\")\n",
    "\n",
    "            elif next_token.item() != 12:\n",
    "                first_input = False\n",
    "                print(int2char[next_token.item()], end=\"\")\n",
    "\n",
    "                if 0 in forbidden_generation:\n",
    "                    forbidden_generation.pop(-1)\n",
    "\n",
    "        # 6. Préparer la prochaine entrée\n",
    "            next_input = next_token.view(1, 1)\n",
    "\n",
    "print(\"\\n\", decod_structure[struct[-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733f83f",
   "metadata": {},
   "source": [
    "# If no restriction on the generation part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_gen = \"malice\"\n",
    "context_encode = [mapping[i] for i in context_gen]\n",
    "\n",
    "hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "encoded = torch.tensor(context_encode, dtype=torch.long).unsqueeze(0)\n",
    "print(\"\".join([int2char[i] for i in context_encode]), end=\"\")\n",
    "\n",
    "out, hid = model(encoded, hid)\n",
    "next_input = encoded[:, -1].unsqueeze(0) \n",
    "\n",
    "last_input = 0\n",
    "first_input = True\n",
    "\n",
    "while last_input != 70:\n",
    "    with torch.no_grad():\n",
    "        out, hid = model(next_input, hid) \n",
    "\n",
    "        next_token = sample_with_temp_topk(out[0, -1], temperature=0.5, top_k=20)\n",
    "\n",
    "        if (last_input == 0) and (next_token != list(decod_structure.keys())):\n",
    "            print(int2char[next_token.item()].upper(), end=\"\")\n",
    "        elif last_input != 70 : \n",
    "            print(int2char[next_token.item()], end=\"\")\n",
    "\n",
    "        last_input = next_token.item()\n",
    "        next_input = next_token.view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_csv(r\"..\\Markov\\transition_matrix.csv\")\n",
    "\n",
    "states = np.array(matrix.iloc[6])\n",
    "prob_transi = np.array(matrix.iloc[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8ae7b",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(int2char)\n",
    "hidden_size = 250\n",
    "\n",
    "model = CharRNN(vocab_size, hidden_size, num_layers=3)   \n",
    "\n",
    "ckpt = torch.load(\"..\\Models\\RNN_model.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a273d6a",
   "metadata": {},
   "source": [
    "The char token representing the end of a section is at place 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = generate_a_song_structure(prob_transi.astype(float),states)\n",
    "\n",
    "encoded = torch.tensor([mapping[c] for c in struct[1]], dtype=torch.long).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid = model.init_hidden(batch_size=1)\n",
    "\n",
    "for i in range(len(struct)-2) :\n",
    "\n",
    "    print()\n",
    "    print(struct[i+1])\n",
    "    encoded = torch.tensor([mapping[c] for c in struct[i+1]], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # prime the model with your encoded context\n",
    "    out, hid = model(encoded, hid)\n",
    "\n",
    "    # take the last token of the context\n",
    "    next_input = encoded[:, -1].unsqueeze(0)  # shape (1, 1)\n",
    "\n",
    "    last_input = 0\n",
    "    first_input = True\n",
    "\n",
    "    while last_input != 79 :\n",
    "        with torch.no_grad():\n",
    "            out, hid = model(next_input, hid)   # out shape: (1, 1, vocab_size)\n",
    "\n",
    "    # sample from logits\n",
    "        next_token = sample_with_temp_topk(out[0, -1], temperature=0.5, top_k=20)\n",
    "        last_input = next_token.item()\n",
    "\n",
    "    # print char\n",
    "        if first_input and last_input == 79 :\n",
    "            last_input = 0\n",
    "            continue\n",
    "\n",
    "        if next_token.item() != 79 :\n",
    "            first_input = False\n",
    "            print(int2char[next_token.item()], end=\"\")\n",
    "\n",
    "\n",
    "    # prepare next input (still (1, 1))\n",
    "        next_input = next_token.view(1, 1)\n",
    "\n",
    "print(\"\\n\",struct[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
