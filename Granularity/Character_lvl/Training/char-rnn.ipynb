{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:33.523105Z",
     "iopub.status.busy": "2025-10-05T15:20:33.522874Z",
     "iopub.status.idle": "2025-10-05T15:20:39.634880Z",
     "shell.execute_reply": "2025-10-05T15:20:39.634098Z",
     "shell.execute_reply.started": "2025-10-05T15:20:33.523081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:39.636812Z",
     "iopub.status.busy": "2025-10-05T15:20:39.636525Z",
     "iopub.status.idle": "2025-10-05T15:20:39.645925Z",
     "shell.execute_reply": "2025-10-05T15:20:39.645268Z",
     "shell.execute_reply.started": "2025-10-05T15:20:39.636794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"../../../Corpus/Encoding_RNN_LSTM/Char_level/encoding_map.pkl\", \"rb\") as f:\n",
    "    mapping = pickle.load(f)\n",
    "\n",
    "mapping[\"PAD\"] = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:39.646950Z",
     "iopub.status.busy": "2025-10-05T15:20:39.646691Z",
     "iopub.status.idle": "2025-10-05T15:20:39.661229Z",
     "shell.execute_reply": "2025-10-05T15:20:39.660546Z",
     "shell.execute_reply.started": "2025-10-05T15:20:39.646927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '%', 5: '&', 6: \"'\", 7: ')', 8: '+', 9: ',', 10: '-', 11: '.', 12: '/', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '?', 26: 'a', 27: 'b', 28: 'c', 29: 'd', 30: 'e', 31: 'f', 32: 'g', 33: 'h', 34: 'i', 35: 'j', 36: 'k', 37: 'l', 38: 'm', 39: 'n', 40: 'o', 41: 'p', 42: 'q', 43: 'r', 44: 's', 45: 't', 46: 'u', 47: 'v', 48: 'w', 49: 'x', 50: 'y', 51: 'z', 52: 'à', 53: 'â', 54: 'ç', 55: 'è', 56: 'é', 57: 'ê', 58: 'ë', 59: 'î', 60: 'ï', 61: 'ô', 62: 'ù', 63: 'û', 64: 'α', 65: 'β', 66: 'γ', 67: 'ε', 68: 'ζ', 69: 'η', 70: 'θ', 71: '€', 72: 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "int2char = {i: ch for ch, i in mapping.items()}\n",
    "print(int2char)\n",
    "nb_char = len(int2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is a huge chunk of text but we need to split it in songs because we don't want that the model learn that there is a chance of having a text after the end of a song. Thiw will prevent learning non existing relationship in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = np.load(\"../../../Corpus/Encoding_RNN_LSTM/Char_level/corpora_encoded.npy\",\"r\")\n",
    "\n",
    "result = []\n",
    "for t in dataset_:\n",
    "    if t == 64 : # α or beginning of a song\n",
    "        current = []\n",
    "        current.append(t)\n",
    "    elif t == 70: # θ or end of a song\n",
    "        current.append(t)\n",
    "        result.append(torch.tensor(current))\n",
    "    else :\n",
    "        current.append(t)\n",
    "if current:  \n",
    "    result.append(torch.tensor(current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:39.662411Z",
     "iopub.status.busy": "2025-10-05T15:20:39.662025Z",
     "iopub.status.idle": "2025-10-05T15:20:42.974249Z",
     "shell.execute_reply": "2025-10-05T15:20:42.973657Z",
     "shell.execute_reply.started": "2025-10-05T15:20:39.662382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SongDataset(Dataset):\n",
    "    def __init__(self, texts, length_seq, stride, pad_id=mapping[\"PAD\"], use_offset=True):\n",
    "        self.samples = []\n",
    "        self.length_seq = length_seq\n",
    "        self.stride = stride\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        for text in texts:\n",
    "\n",
    "            L = len(text)\n",
    "            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n",
    "\n",
    "            # --- Boucle principale ---\n",
    "            for start in range(offset, max(1, L - self.length_seq - 1), self.stride):\n",
    "                x_start, x_end = start, start + self.length_seq\n",
    "                y_start, y_end = start + 1, start + 1 + self.length_seq\n",
    "\n",
    "                x = text[x_start:x_end]\n",
    "                y = text[y_start:y_end]\n",
    "\n",
    "                # --- Padding uniforme ---\n",
    "                def pad_to_len(seq, pad_id, target_len):\n",
    "                    pad_len = target_len - len(seq)\n",
    "                    if pad_len > 0:\n",
    "                        seq = torch.cat([seq, torch.full((pad_len,), pad_id, dtype=seq.dtype)])\n",
    "                    return seq\n",
    "\n",
    "                x = pad_to_len(x, self.pad_id, self.length_seq)\n",
    "                y = pad_to_len(y, self.pad_id, self.length_seq)\n",
    "\n",
    "                self.samples.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:42.975108Z",
     "iopub.status.busy": "2025-10-05T15:20:42.974893Z",
     "iopub.status.idle": "2025-10-05T15:20:42.995973Z",
     "shell.execute_reply": "2025-10-05T15:20:42.995317Z",
     "shell.execute_reply.started": "2025-10-05T15:20:42.975091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "seq_length = 256\n",
    "stride = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:42.996835Z",
     "iopub.status.busy": "2025-10-05T15:20:42.996655Z",
     "iopub.status.idle": "2025-10-05T15:20:43.004237Z",
     "shell.execute_reply": "2025-10-05T15:20:43.003638Z",
     "shell.execute_reply.started": "2025-10-05T15:20:42.996819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Then we can split between songs and not part of a song\n",
    "len_train = int(len(result) * 0.85)\n",
    "len_test = len(result) - len_train\n",
    "\n",
    "train, test = random_split(result, [len_train, len_test])\n",
    "\n",
    "#Use of stride 16 to limit the overlap between each sequence, that reduce training time and reduce overfit \n",
    "# and also because one character doesn't contain a lot of information in itself\n",
    "train_ds = SongDataset(train,length_seq=seq_length, stride = stride, use_offset = True)\n",
    "test_ds = SongDataset(test,length_seq=seq_length, stride = stride, use_offset = False)\n",
    "\n",
    "#Shuffle False because a song is like a time series and cannot be shuffle randomly inside it. The position of each part has a meaning.\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:45.366521Z",
     "iopub.status.busy": "2025-10-05T15:20:45.366370Z",
     "iopub.status.idle": "2025-10-05T15:20:45.382356Z",
     "shell.execute_reply": "2025-10-05T15:20:45.381822Z",
     "shell.execute_reply.started": "2025-10-05T15:20:45.366509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers=1, dropout = 0):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=72)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True, dropout = dropout, nonlinearity =\"relu\")\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.drop(self.embedding(x))\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc(out)                  \n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:45.383168Z",
     "iopub.status.busy": "2025-10-05T15:20:45.382912Z",
     "iopub.status.idle": "2025-10-05T15:20:45.399300Z",
     "shell.execute_reply": "2025-10-05T15:20:45.398771Z",
     "shell.execute_reply.started": "2025-10-05T15:20:45.383144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device1 = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:45.419218Z",
     "iopub.status.busy": "2025-10-05T15:20:45.418997Z",
     "iopub.status.idle": "2025-10-05T15:20:50.612397Z",
     "shell.execute_reply": "2025-10-05T15:20:50.611533Z",
     "shell.execute_reply.started": "2025-10-05T15:20:45.419203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "vocab_size = len(int2char)\n",
    "hidden_size = 512\n",
    "num_epoch = 50\n",
    "\n",
    "nb_step_train = len(train_dl)\n",
    "nb_step_test = len(test_dl)\n",
    "\n",
    "model = CharRNN(vocab_size, embedding_dim, hidden_size, num_layers=2, dropout = 0.2).to(device1)\n",
    "model = torch.compile(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=72)\n",
    "\n",
    "opti = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)\n",
    "sched_warm = torch.optim.lr_scheduler.LinearLR(opti,start_factor=0.2,end_factor=1.0,total_iters=nb_step_train * 3)\n",
    "sched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*10, T_mult=2, eta_min=0.0002) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T14:20:36.279047Z",
     "iopub.status.busy": "2025-10-05T14:20:36.278855Z",
     "iopub.status.idle": "2025-10-05T14:20:36.440381Z",
     "shell.execute_reply": "2025-10-05T14:20:36.439192Z",
     "shell.execute_reply.started": "2025-10-05T14:20:36.279031Z"
    }
   },
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:20:50.613469Z",
     "iopub.status.busy": "2025-10-05T15:20:50.613119Z",
     "iopub.status.idle": "2025-10-05T15:20:50.624961Z",
     "shell.execute_reply": "2025-10-05T15:20:50.624375Z",
     "shell.execute_reply.started": "2025-10-05T15:20:50.613446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_tf1(model, dl, loss_fn, device, vocab_size, bs = batch_size, sl = seq_length):\n",
    "    \"\"\"Validation with teacher forcing = 1 (parallel, fast). Returns (ppl, acc).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, Y in dl:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device, dtype=torch.long)\n",
    "        hid = model.init_hidden(bs, device1)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            pred, _ = model(X, hid) \n",
    "            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "\n",
    "        total_loss += loss.item() * bs * sl\n",
    "        total_tokens += bs * sl\n",
    "\n",
    "        pred_ids = pred.argmax(dim=-1)\n",
    "        correct += (pred_ids == Y).sum().item()\n",
    "        total += bs * sl\n",
    "\n",
    "    ppl = math.exp(total_loss / max(1, total_tokens))\n",
    "    acc = correct / max(1, total)\n",
    "    return ppl, acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_free(model, dl, loss_fn, device, bs=batch_size, sl=seq_length):\n",
    "    \"\"\"\n",
    "    Autoregressive validation (teacher forcing = 0).\n",
    "    Steps one token at a time and feeds predictions back in.\n",
    "    Returns ppl.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for X, Y in dl:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device, dtype=torch.long)\n",
    "        hid = model.init_hidden(bs, device1)\n",
    "\n",
    "        # Start with the first input token\n",
    "        inp = X[:, :1]  # (bs, 1)\n",
    "        for t in range(sl):\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred, hid = model(inp, hid)          # (bs, 1, vocab)\n",
    "                logits = pred[:, -1, :]              # (bs, vocab)\n",
    "                loss = loss_fn(logits, Y[:, t])      # CE over current step\n",
    "\n",
    "            total_loss += loss.item() * bs\n",
    "            total_tokens += bs\n",
    "\n",
    "            # Greedy next-token to feed back in\n",
    "            next_token = logits.argmax(dim=-1).unsqueeze(1)  # (bs, 1)\n",
    "            inp = next_token\n",
    "\n",
    "    ppl = math.exp(total_loss / max(1, total_tokens))\n",
    "    return ppl\n",
    "\n",
    "def sample_with_temp(logits, temp=1.0):\n",
    "    probs = (logits / temp).softmax(dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token\n",
    "\n",
    "def distinct_n_chars(text, n=3):\n",
    "    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    return len(set(ngrams)) / max(1, len(ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, first I put :\n",
    "- Every 2/3 epoch : teacher_forcing_ratio - 0.02\n",
    "\n",
    "Then when teacher_forcing_ratio < 0.5 :\n",
    "- Every epoch or 2 epoch : teacher_forcing_ratio - 0.01\n",
    "\n",
    "I stop the teacher_forcing_ratio around 0.25/0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T16:36:11.779613Z",
     "iopub.status.busy": "2025-10-05T16:36:11.779361Z",
     "iopub.status.idle": "2025-10-05T16:43:30.134285Z",
     "shell.execute_reply": "2025-10-05T16:43:30.133340Z",
     "shell.execute_reply.started": "2025-10-05T16:36:11.779594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 | Train PPL: 7.216 | Val PPL (TF=1): 5.303 | Val PPL (free): 35.853 | Val Acc: 0.492\n",
      "\n",
      "=== Initial text ===\n",
      ", mama désolé\n",
      "l'été, la vie est si belle, augmente les décibels\n",
      "on fly-ye-ye\n",
      "l'été la vie est si belle, augmente les décibels\n",
      "faut qu'on s'taille-aille-aille\n",
      "/ε\n",
      "γ\n",
      "après que le corps d'cette se frotte sur ma  aouh\n",
      "j'sors d'ma douche, j'aime voir le sun qui \n",
      "\n",
      "=== Sample Generation ===\n",
      " le  porhhes se coouveent   temps\n",
      "on s'aiit qu'j'vous fais  d'aassooiree, j'ai d'la horritt\n",
      "    mes miss  maa cherché, je coois qu'aa  fait les pétages  qu'écrire le  vediss, j'ai l'aille \n",
      "je   temps \n",
      "Distinct-2: 0.497 | Distinct-3: 0.768\n",
      "Epoch 101 | Train PPL: 7.221 | Val PPL (TF=1): 5.310 | Val PPL (free): 31.878 | Val Acc: 0.492\n",
      "\n",
      "Epoch 102 | Teacher forcing ratio = 0.25\n",
      "Epoch 102 | Train PPL: 7.220 | Val PPL (TF=1): 5.344 | Val PPL (free): 31.376 | Val Acc: 0.491\n",
      "Epoch 103 | Train PPL: 7.221 | Val PPL (TF=1): 5.335 | Val PPL (free): 32.391 | Val Acc: 0.491\n",
      "Epoch 104 | Train PPL: 7.220 | Val PPL (TF=1): 5.362 | Val PPL (free): 33.659 | Val Acc: 0.490\n",
      "\n",
      "=== Initial text ===\n",
      ", mama désolé\n",
      "l'été, la vie est si belle, augmente les décibels\n",
      "on fly-ye-ye\n",
      "l'été la vie est si belle, augmente les décibels\n",
      "faut qu'on s'taille-aille-aille\n",
      "/ε\n",
      "γ\n",
      "après que le corps d'cette se frotte sur ma  aouh\n",
      "j'sors d'ma douche, j'aime voir le sun qui \n",
      "\n",
      "=== Sample Generation ===\n",
      " le  poitts   ll'argent de  pooches\n",
      "les mons  qui parlent les comptes\n",
      "  nuus     continuer\n",
      "j'uuis ees dde  souvenirs   lla complle  t'endrrmii  pas\n",
      "j''étais le  toouues   oaarreer c'que laa l'été, j'e\n",
      "Distinct-2: 0.503 | Distinct-3: 0.803\n",
      "\n",
      "Epoch 105 | Teacher forcing ratio = 0.25\n",
      "Epoch 105 | Train PPL: 7.221 | Val PPL (TF=1): 5.370 | Val PPL (free): 33.515 | Val Acc: 0.490\n",
      "Epoch 106 | Train PPL: 7.217 | Val PPL (TF=1): 5.369 | Val PPL (free): 39.931 | Val Acc: 0.490\n",
      "Early Stopping (no acc improvement)\n",
      "Epoch 107 | Train PPL: 7.209 | Val PPL (TF=1): 5.374 | Val PPL (free): 35.581 | Val Acc: 0.490\n",
      "\n",
      "Epoch 108 | Teacher forcing ratio = 0.25\n",
      "Epoch 108 | Train PPL: 7.207 | Val PPL (TF=1): 5.394 | Val PPL (free): 34.906 | Val Acc: 0.489\n",
      "\n",
      "=== Initial text ===\n",
      ", mama désolé\n",
      "l'été, la vie est si belle, augmente les décibels\n",
      "on fly-ye-ye\n",
      "l'été la vie est si belle, augmente les décibels\n",
      "faut qu'on s'taille-aille-aille\n",
      "/ε\n",
      "γ\n",
      "après que le corps d'cette se frotte sur ma  aouh\n",
      "j'sors d'ma douche, j'aime voir le sun qui \n",
      "\n",
      "=== Sample Generation ===\n",
      " eaaa baoonne  mais tii  t'aaaiss jamais aaisserr   quoi?\n",
      "ok  j'ai pas paassé  des polinns\n",
      "pass, t'as las choséé  moi je me vee que jelle  ldes plus les nouveaux proches sur la plus iie\n",
      "\n",
      "même le  raam\n",
      "Distinct-2: 0.523 | Distinct-3: 0.818\n",
      "Epoch 109 | Train PPL: 7.202 | Val PPL (TF=1): 5.406 | Val PPL (free): 31.457 | Val Acc: 0.489\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1297951196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_tot = []\n",
    "bs = batch_size\n",
    "sl = seq_length\n",
    "teacher_forcing_ratio = 1\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    # --- Teacher Forcing ratio decay ---\n",
    "    if epoch % 3 == 0 :\n",
    "        teacher_forcing_ratio = max(0.0, min(1.0, teacher_forcing_ratio - 0.02))\n",
    "        print(f\"\\nEpoch {epoch} | Teacher forcing ratio = {teacher_forcing_ratio:.2f}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # -------------- TRAIN LOOP --------------\n",
    "    train_loss_sum = 0.0\n",
    "    train_tokens = 0\n",
    "    nb_step_train = 0\n",
    "\n",
    "    for X, Y in iter(train_dl):\n",
    "        hid = model.init_hidden(batch_size).to(device1)\n",
    "        X = X.to(device1)\n",
    "        Y = Y.to(device1, dtype=torch.long)\n",
    "        opti.zero_grad(set_to_none=True)\n",
    "\n",
    "        if teacher_forcing_ratio == 1.0:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred, hid = model(X, hid)\n",
    "                loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "        else:\n",
    "            # ---- Pass 1: forward with TF=1  ----\n",
    "            with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred_tf, _ = model(X, hid)   \n",
    "            pred_tokens = pred_tf.argmax(dim=-1)  \n",
    "\n",
    "            # ---- Random mask for TF < 1 ----\n",
    "            mask = (torch.rand_like(X.float()) < teacher_forcing_ratio)\n",
    "            X_mixed = torch.where(mask, X, pred_tokens)\n",
    "\n",
    "            # ---- Pass 2: forward with partial TF ----\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred, hid = model(X_mixed, hid)\n",
    "                loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        scaler.step(opti)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss_sum += loss.detach().item() * bs * sl\n",
    "        train_tokens += bs * sl\n",
    "        nb_step_train += 1\n",
    "\n",
    "        if epoch < 3:\n",
    "            sched_warm.step()\n",
    "        else:\n",
    "            sched_post.step()\n",
    "\n",
    "    train_ppl = math.exp(train_loss_sum / max(1, train_tokens))\n",
    "\n",
    "    # -------------- VALIDATION --------------\n",
    "    val_ppl_tf1, val_acc = evaluate_tf1(model, test_dl, loss_fn, device1, vocab_size)\n",
    "    val_ppl_free = evaluate_free(model, test_dl, loss_fn, device1)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"Train PPL: {train_ppl:.3f} | \"\n",
    "        f\"Val PPL (TF=1): {val_ppl_tf1:.3f} | \"\n",
    "        f\"Val PPL (free): {val_ppl_free:.3f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # --------- Sample generation + diversity metrics ---------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Warm up for generation\n",
    "        start = X[0:1, :20]  \n",
    "        hid_gen = model.init_hidden(1).to(device1)\n",
    "        inp = start\n",
    "\n",
    "        gen_chars = []\n",
    "        for t in range(200):  \n",
    "            pred, hid_gen = model(inp, hid_gen)\n",
    "            logits = pred[:, -1, :]  \n",
    "            next_char = sample_with_temp(logits, temp=0.6)\n",
    "            gen_chars.append(int2char[next_char.item()])\n",
    "            inp = next_char \n",
    "\n",
    "        gen_text = \"\".join(gen_chars)\n",
    "\n",
    "    d2 = distinct_n_chars(gen_text, n=2)\n",
    "    d3 = distinct_n_chars(gen_text, n=3)\n",
    "\n",
    "    \n",
    "    if epoch % 4 == 0 :\n",
    "        print(\"\\n=== Initial text ===\")\n",
    "        print(\"\".join([int2char[i] for i in X[0:1,:].squeeze(0).tolist()]))\n",
    "        print(\"\\n=== Sample Generation ===\")\n",
    "        print(gen_text[:200])  \n",
    "        print(f\"Distinct-2: {d2:.3f} | Distinct-3: {d3:.3f}\", end=\"\\n\")\n",
    "\n",
    "    l_tot.append(val_acc)\n",
    "    torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": opti.state_dict(),\n",
    "                \"scheduler_state_dict\": sched_post.state_dict(),\n",
    "                \"val_acc\": val_acc,\n",
    "            },\n",
    "            \"model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 15 | Teacher forcing ratio = 0.88\n",
    "Epoch 15 | Train PPL: 5.163 | Val PPL (TF=1): 3.891 | Val PPL (free): 570.861 | Val Acc: 0.579\n",
    "\n",
    "=== Sample Generation ===\n",
    "omis j'ai pas d'amour est parti de la maison d'un peu d'couplet d'la maison d'un peu d'couplet d'la maison d'un peu d'couplet d'la maison d'un peu d'couplet d'la maison d'un peu d'couplet d'la maison \n",
    "Distinct-1: 0.085 | Distinct-2: 0.241 | Distinct-3: 0.313\n",
    "\n",
    "Epoch 16 | Teacher forcing ratio = 0.86\n",
    "Epoch 16 | Train PPL: 5.367 | Val PPL (TF=1): 3.916 | Val PPL (free): 546.126 | Val Acc: 0.577\n",
    "\n",
    "=== Sample Generation ===\n",
    " s'en fout d'me faire des coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups d'coups \n",
    "Distinct-1: 0.080 | Distinct-2: 0.136 | Distinct-3: 0.167"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8299981,
     "sourceId": 13220961,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 462509,
     "modelInstanceId": 446043,
     "sourceId": 595734,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
