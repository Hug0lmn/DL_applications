{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13220961,"sourceType":"datasetVersion","datasetId":8299981}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook will serve as a way to implement character generation MHA","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport re\nimport numpy as np\nimport pickle\nimport random\nfrom torch.utils.data import random_split\nimport torch.nn.functional as F\nfrom torchtune.modules import RotaryPositionalEmbeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:45:55.009055Z","iopub.execute_input":"2025-10-18T13:45:55.009288Z","iopub.status.idle":"2025-10-18T13:46:07.493389Z","shell.execute_reply.started":"2025-10-18T13:45:55.009264Z","shell.execute_reply":"2025-10-18T13:46:07.492615Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"with open(\"/kaggle/input/rnn-input/encoding_map.pkl\", \"rb\") as f:\n    mapping = pickle.load(f)\n\nmapping[\"PAD\"] = len(mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:07.494160Z","iopub.execute_input":"2025-10-18T13:46:07.494618Z","iopub.status.idle":"2025-10-18T13:46:07.504095Z","shell.execute_reply.started":"2025-10-18T13:46:07.494598Z","shell.execute_reply":"2025-10-18T13:46:07.503163Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"# Decode\nint2char = {i: ch for ch, i in mapping.items()}\nprint(int2char)\n\nnb_char = len(int2char)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:07.505954Z","iopub.execute_input":"2025-10-18T13:46:07.506258Z","iopub.status.idle":"2025-10-18T13:46:07.617833Z","shell.execute_reply.started":"2025-10-18T13:46:07.506239Z","shell.execute_reply":"2025-10-18T13:46:07.617062Z"}},"outputs":[{"name":"stdout","text":"{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '%', 5: '&', 6: \"'\", 7: ')', 8: '+', 9: ',', 10: '-', 11: '.', 12: '/', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '?', 26: 'a', 27: 'b', 28: 'c', 29: 'd', 30: 'e', 31: 'f', 32: 'g', 33: 'h', 34: 'i', 35: 'j', 36: 'k', 37: 'l', 38: 'm', 39: 'n', 40: 'o', 41: 'p', 42: 'q', 43: 'r', 44: 's', 45: 't', 46: 'u', 47: 'v', 48: 'w', 49: 'x', 50: 'y', 51: 'z', 52: 'à', 53: 'â', 54: 'ç', 55: 'è', 56: 'é', 57: 'ê', 58: 'ë', 59: 'î', 60: 'ï', 61: 'ô', 62: 'ù', 63: 'û', 64: 'α', 65: 'β', 66: 'γ', 67: 'ε', 68: 'ζ', 69: 'η', 70: 'θ', 71: '€', 72: 'PAD'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Creation of the dataset","metadata":{}},{"cell_type":"markdown","source":"To increase the randomness during the training :\n\nFor each epoch the entire corpus will have a random specific offset value in order that the model during training doesn't see the exact same text during X epochs.","metadata":{}},{"cell_type":"code","source":"class SongDataset(Dataset):\n    def __init__(self, texts, length_seq, stride, pad_id=mapping[\"PAD\"], use_offset=True):\n        self.samples = []\n        self.length_seq = length_seq\n        self.stride = stride\n        self.pad_id = pad_id\n\n        for text in texts:\n            # --- Sécurité : s'assurer d'un tensor 1D long CPU ---\n            if not isinstance(text, torch.Tensor):\n                text = torch.tensor(text, dtype=torch.long)\n            else:\n                text = text.clone().detach().to(dtype=torch.long, device=\"cpu\").contiguous()\n\n            L = len(text)\n            if L < 2:\n                continue\n\n            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n\n            # --- Boucle principale ---\n            for start in range(offset, max(1, L - self.length_seq - 1), self.stride):\n                x_start, x_end = start, start + self.length_seq\n                y_start, y_end = start + 1, start + 1 + self.length_seq\n\n                x = text[x_start:x_end]\n                y = text[y_start:y_end]\n\n                # --- Padding uniforme ---\n                def pad_to_len(seq, pad_id, target_len):\n                    pad_len = target_len - len(seq)\n                    if pad_len > 0:\n                        seq = torch.cat([seq, torch.full((pad_len,), pad_id, dtype=seq.dtype)])\n                    return seq\n\n                x = pad_to_len(x, self.pad_id, self.length_seq)\n                y = pad_to_len(y, self.pad_id, self.length_seq)\n\n                self.samples.append((x, y))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:07.618558Z","iopub.execute_input":"2025-10-18T13:46:07.618793Z","iopub.status.idle":"2025-10-18T13:46:07.634730Z","shell.execute_reply.started":"2025-10-18T13:46:07.618774Z","shell.execute_reply":"2025-10-18T13:46:07.633837Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset_ = np.load(\"/kaggle/input/rnn-input/corpora_encoded.npy\",\"r\")\n\nresult = []\nfor t in dataset_:\n    if t == 64 : \n        current = []\n        current.append(t)\n    elif t == 70:\n        current.append(t)\n        result.append(torch.tensor(current))\n    else :\n        current.append(t)\nif current:  \n    result.append(torch.tensor(current))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:07.635579Z","iopub.execute_input":"2025-10-18T13:46:07.635960Z","iopub.status.idle":"2025-10-18T13:46:11.282376Z","shell.execute_reply.started":"2025-10-18T13:46:07.635937Z","shell.execute_reply":"2025-10-18T13:46:11.281816Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"len_train = int(len(result) * 0.8)\nlen_test = len(result) - len_train\n\ngenerator = torch.Generator().manual_seed(42)\ntrain, test = random_split(result, [len_train, len_test], generator = generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:11.283180Z","iopub.execute_input":"2025-10-18T13:46:11.283449Z","iopub.status.idle":"2025-10-18T13:46:11.288683Z","shell.execute_reply.started":"2025-10-18T13:46:11.283426Z","shell.execute_reply":"2025-10-18T13:46:11.288000Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_ds = SongDataset(train,length_seq=256, stride = 64, use_offset = True)\ntest_ds = SongDataset(test,length_seq=256, stride = 64, use_offset = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:11.289598Z","iopub.execute_input":"2025-10-18T13:46:11.289930Z","iopub.status.idle":"2025-10-18T13:46:12.078636Z","shell.execute_reply.started":"2025-10-18T13:46:11.289911Z","shell.execute_reply":"2025-10-18T13:46:12.077790Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"\".join([int2char[i] for i in train_ds[0][0][:256].numpy()]))\nprint()\nprint(\"\".join([int2char[i] for i in train_ds[1][0][:256].numpy()]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.079486Z","iopub.execute_input":"2025-10-18T13:46:12.079746Z","iopub.status.idle":"2025-10-18T13:46:12.085478Z","shell.execute_reply.started":"2025-10-18T13:46:12.079720Z","shell.execute_reply":"2025-10-18T13:46:12.084659Z"}},"outputs":[{"name":"stdout","text":"ans tes yeux je peux le voir\ntu penses sûrement qu'j'suis plus heureux que toi\nrien ne dure tout est éphémère frérot\nj'ai rien à cacher comme les femens\navant j'avais qu'des p'tites pièces comme le passeur d'âme\net là je brunch avec madame à amsterdam\nfair\n\neureux que toi\nrien ne dure tout est éphémère frérot\nj'ai rien à cacher comme les femens\navant j'avais qu'des p'tites pièces comme le passeur d'âme\net là je brunch avec madame à amsterdam\nfaire plaisir à ceux qu'on aime ça n'a pas d'prix\npour tout le reste\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Creation of Dataloader and co","metadata":{}},{"cell_type":"code","source":"batch_size = 516\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True) #Shuffle False because we need the RNN to use previous sequences data to predict next one\ntest_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.088157Z","iopub.execute_input":"2025-10-18T13:46:12.088407Z","iopub.status.idle":"2025-10-18T13:46:12.098631Z","shell.execute_reply.started":"2025-10-18T13:46:12.088391Z","shell.execute_reply":"2025-10-18T13:46:12.097938Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"len(dataset_),len(train_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.099448Z","iopub.execute_input":"2025-10-18T13:46:12.099698Z","iopub.status.idle":"2025-10-18T13:46:12.117136Z","shell.execute_reply.started":"2025-10-18T13:46:12.099651Z","shell.execute_reply":"2025-10-18T13:46:12.116476Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(3705039, 80)"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"markdown","source":"### Training part","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, nheads, dropout, max_seq, bias=True):\n        super().__init__()\n\n        self.nheads = nheads\n        assert embed_dim % nheads == 0, \"Embedding dim is not divisible by nheads\"\n        self.head_dim = embed_dim // nheads\n        self.dropout = dropout\n        \n        self.proj_qkv = nn.Linear(embed_dim, embed_dim * 3, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.rope = RotaryPositionalEmbeddings(dim=self.head_dim, max_seq_len = max_seq)\n\n    def forward(self, x: torch.Tensor, attn_mask=None, is_causal=False) -> torch.Tensor:\n        \n        # Step 1\n        B,L,_ = x.shape\n        result = self.proj_qkv(x)\n        q, k, v = torch.chunk(result, 3, dim=-1)\n\n        # Step 2\n        # (N, L_t, head_dim) -> (N, L_t, nheads, head_dim) -> (N, nheads, L_t, head_dim)\n        q = q.view(B, L, self.nheads, self.head_dim)\n        k = k.view(B, L, self.nheads, self.head_dim)\n        v = v.view(B, L, self.nheads, self.head_dim)\n\n#        q = self.rope(q) #Doesn't need Rope / overkill because char didn't have the same relation\n#        k = self.rope(k)\n\n        #Adapt dim \n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        #Local attention window = 64\n        mask = (torch.tril(torch.ones([L,L]), diagonal=-64)+torch.tril(torch.ones([L,L])))==1\n        attn_mask = ~mask.to(device)\n\n        # Step 3\n        # (N, nheads, L_t, E_head)\n        attn_output = F.scaled_dot_product_attention(\n            q, k, v, dropout_p=self.dropout, attn_mask = attn_mask, is_causal=is_causal)\n        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n        attn_output = attn_output.transpose(1, 2).flatten(-2)\n\n        return self.out_proj(attn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.117915Z","iopub.execute_input":"2025-10-18T13:46:12.118185Z","iopub.status.idle":"2025-10-18T13:46:12.132481Z","shell.execute_reply.started":"2025-10-18T13:46:12.118167Z","shell.execute_reply":"2025-10-18T13:46:12.131731Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, max_seq, d_ff=256*4, dropout=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, n_heads, dropout, max_seq)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.Dropout(dropout),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout),\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # masked multi-head self-attention\n        x = x + self.attn(self.norm1(x), is_causal=False) #Is causal = for token t, mask on every tokens after (cannot see what's coming after)\n        # feed-forward\n        x = x + self.ff(self.norm2(x))\n        return x\n\nclass DecoderOnlyTransformer(nn.Module):\n    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=4, max_len=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads, max_seq=max_len) for _ in range(n_layers)])\n        self.norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        B, T = x.shape\n        x = self.emb(x) \n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.133263Z","iopub.execute_input":"2025-10-18T13:46:12.133516Z","iopub.status.idle":"2025-10-18T13:46:12.153274Z","shell.execute_reply.started":"2025-10-18T13:46:12.133497Z","shell.execute_reply":"2025-10-18T13:46:12.152451Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = torch.device(\"cuda:0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.154274Z","iopub.execute_input":"2025-10-18T13:46:12.154592Z","iopub.status.idle":"2025-10-18T13:46:12.172265Z","shell.execute_reply.started":"2025-10-18T13:46:12.154564Z","shell.execute_reply":"2025-10-18T13:46:12.171507Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"vocab_size = len(mapping)\nnum_epoch = 100\n\nnb_step_train = len(train_dl)\nnb_step_test = len(test_dl)\n\nmodel = DecoderOnlyTransformer(vocab_size, d_model=256, n_heads=4, n_layers=8).to(device) \nmodel = torch.compile(model)\n \nloss_fn = nn.CrossEntropyLoss(ignore_index=72)\nval_fn = nn.CrossEntropyLoss(ignore_index=72)\n\nopti = torch.optim.AdamW(model.parameters(), lr=0.0025, weight_decay=1e-3)\nsched_warm = torch.optim.lr_scheduler.LinearLR(opti, start_factor=0.2, end_factor=1, total_iters=nb_step_train*5)\n#sched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*10, T_mult=2, eta_min=0.0005) #1 epoch => 2 => 4 => 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.173032Z","iopub.execute_input":"2025-10-18T13:46:12.173312Z","iopub.status.idle":"2025-10-18T13:46:12.522844Z","shell.execute_reply.started":"2025-10-18T13:46:12.173264Z","shell.execute_reply":"2025-10-18T13:46:12.522231Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import math\n\n@torch.no_grad()\ndef evaluate_tf1(model, dl, loss_fn, device, vocab_size):\n    \"\"\"Validation with teacher forcing = 1 (parallel, fast). Returns (ppl, acc).\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    correct = 0\n    total = 0\n\n    for X, Y in dl:\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        bs, sl = X.size(0), X.size(1)\n\n        with torch.amp.autocast(device_type=\"cuda\"):\n            pred = model(X)  # (bs, sl, vocab)\n            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n        total_loss += loss.item() * bs * sl\n        total_tokens += bs * sl\n\n        pred_ids = pred.argmax(dim=-1)\n        correct += (pred_ids == Y).sum().item()\n        total += bs * sl\n\n    ppl = math.exp(total_loss / max(1, total_tokens))\n    acc = correct / max(1, total)\n    return ppl, acc\n\n\n@torch.no_grad()\ndef evaluate_free(model, dl, loss_fn, device):\n    \"\"\"\n    Autoregressive validation (teacher forcing = 0).\n    Steps one token at a time and feeds predictions back in.\n    Returns ppl (computed on next-token NLL).\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for X, Y in dl:\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        bs, sl = X.size(0), X.size(1)\n\n        # Start with the first input token\n        inp = X[:, :1]  # (bs, 1)\n        for t in range(sl):\n            with torch.amp.autocast(device_type=\"cuda\"):\n                pred = model(inp)          # (bs, 1, vocab)\n                logits = pred[:, -1, :]              # (bs, vocab)\n                loss = loss_fn(logits, Y[:, t])      # CE over current step\n\n            total_loss += loss.item() * bs\n            total_tokens += bs\n\n            # Greedy next-token to feed back in\n            next_token = logits.argmax(dim=-1).unsqueeze(1)  # (bs, 1)\n            inp = next_token\n\n    ppl = math.exp(total_loss / max(1, total_tokens))\n    return ppl\n\ndef sample_with_temp(logits, temp=1.0):\n    probs = (logits / temp).softmax(dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n    return next_token\n\ndef distinct_n_chars(text, n=3):\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    return len(set(ngrams)) / max(1, len(ngrams))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.523551Z","iopub.execute_input":"2025-10-18T13:46:12.523785Z","iopub.status.idle":"2025-10-18T13:46:12.534909Z","shell.execute_reply.started":"2025-10-18T13:46:12.523767Z","shell.execute_reply":"2025-10-18T13:46:12.533892Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#opti = torch.optim.AdamW(model.parameters(), lr=0.003, weight_decay=1e-3)\n#sched_warm = torch.optim.lr_scheduler.LinearLR(opti, start_factor=0.2, end_factor=1, total_iters=nb_step_train*5)\n#sched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*10, T_mult=2, eta_min=0.0001) #1 epoch => 2 => 4 => 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.535730Z","iopub.execute_input":"2025-10-18T13:46:12.535948Z","iopub.status.idle":"2025-10-18T13:46:12.554119Z","shell.execute_reply.started":"2025-10-18T13:46:12.535932Z","shell.execute_reply":"2025-10-18T13:46:12.553468Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#opti = torch.optim.AdamW(model.parameters(), lr=0.0015, weight_decay=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.554816Z","iopub.execute_input":"2025-10-18T13:46:12.555023Z","iopub.status.idle":"2025-10-18T13:46:12.568698Z","shell.execute_reply.started":"2025-10-18T13:46:12.555006Z","shell.execute_reply":"2025-10-18T13:46:12.567982Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from time import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.569519Z","iopub.execute_input":"2025-10-18T13:46:12.569776Z","iopub.status.idle":"2025-10-18T13:46:12.583461Z","shell.execute_reply.started":"2025-10-18T13:46:12.569750Z","shell.execute_reply":"2025-10-18T13:46:12.582706Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"l_tot = []\nbest_val = float(\"inf\")\n\nprint(time())\n\nscaler = torch.amp.GradScaler()\nimport math\nfor epoch in range(num_epoch) :\n    \n    #Offset the datas\n#    train_ds.set_offset(offset); test_ds.set_offset(offset)\n\n    model.train();\n\n    #Create loss per epoch\n    l_train = 0.0\n    l_test = 0.0\n    \n    for X,Y in iter(train_dl) :\n        X = X.to(device); Y= Y.to(device, dtype=torch.long)\n        opti.zero_grad(set_to_none=True)\n        \n        #Computation of model        \n        with torch.amp.autocast(device_type=\"cuda:0\"):\n            pred = model(X)\n            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        scaler.step(opti); scaler.update()\n        l_train += loss.detach().item()\n\n        #Scheduler part\n        #Warm start\n        \n#        if sched_post.T_cur == 0 and epoch > 6:  #After warm restart decrease the max learning rate\n#            sched_post.base_lrs[0] = sched_post.base_lrs[0] * 0.8\n#            sched_post.eta_min = sched_post.eta_min * 1.5\n#            print(f\"Decrease {sched_post.base_lrs[0]}, {sched_post.eta_min}\")\n\n#        step_scheduler = sched_warm if epoch < 5 else sched_post\n        if epoch < 5 :\n            sched_warm.step()\n\n    # -------------- VALIDATION --------------\n    val_ppl_tf1, val_acc = evaluate_tf1(model, test_dl, val_fn, device, vocab_size)\n#    val_ppl_free = evaluate_free(model, test_dl, val_fn, device)\n\n    train_ppl = math.exp(l_train/nb_step_train)\n    print(\n        f\"Epoch {epoch} | \"\n        f\"Train PPL: {train_ppl:.3f} | \"\n        f\"Val PPL (TF=1): {val_ppl_tf1:.3f} | \"\n#        f\"Val PPL (free): {val_ppl_free:.3f} | \"\n        f\"Val Acc: {val_acc:.3f}\"\n    )\n\n    # --------- Sample generation + diversity metrics ---------\n    if epoch % 4 == 0 :\n        model.eval()\n        with torch.no_grad():\n        # prends les 5 premiers chars du batch courant comme seed\n            start = X[0:1, :20]  \n            inp = start\n\n            gen_chars = []\n            for t in range(200):  # génère 200 caractères\n                pred = model(inp)\n                logits = pred[:, -1, :]  # dernier pas\n                next_char = sample_with_temp(logits, temp=0.4)\n                gen_chars.append(int2char[next_char.item()])\n                inp = next_char # feed back\n\n            gen_text = \"\".join(gen_chars)\n\n        d2 = distinct_n_chars(gen_text, n=2)\n        d3 = distinct_n_chars(gen_text, n=3)\n    \n        print(\"\\n=== Initial text ===\")\n        print(\"\".join([int2char[i] for i in X[0:1,:].squeeze(0).tolist()]))\n        print(\"\\n=== Sample Generation ===\")\n        print(gen_text[:200])  # affiche les 200 premiers chars\n        print(f\"Distinct-2: {d2:.3f} | Distinct-3: {d3:.3f}\", end=\"\\n\")\n\n    # Record accuracy\n    if val_ppl_tf1 < best_val :\n        best_val = val_ppl_tf1\n        l_tot.append(val_acc)\n        torch.save(\n                {\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": opti.state_dict(),\n#                \"scheduler_state_dict\": sched_post.state_dict(),\n                \"val_ppl\": val_ppl_tf1,\n            },\n            \"model_forced\",\n        )\nprint(time())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:46:12.584270Z","iopub.execute_input":"2025-10-18T13:46:12.584469Z","execution_failed":"2025-10-18T14:03:48.242Z"}},"outputs":[{"name":"stdout","text":"1760795172.5968385\n","output_type":"stream"},{"name":"stderr","text":"W1018 13:46:24.569000 36 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train PPL: 13.501 | Val PPL (TF=1): 11.074 | Val Acc: 0.282\n\n=== Initial text ===\nn'fait pas l'man\nfaut parfois s'debrouiller seul\nya pas d'famille au parloir\npour s'installer au sommet\ncar l'paradis n'est pas louable\non connait les mauvaises passes\nles frappes un deux trois sommeil\non essaye d'pas faire de mal au gens\nd'pas mettre la s\n\n=== Sample Generation ===\ns d'aile de l's l'a mes de des pont j'd's r les le j'a j'pais s pon de ples l'ons l'eur t d'le d'e qurais de din quns faillan vis pas llentoure condis c'vinde prge d'a de mman de li la les de j'es lai\nDistinct-2: 0.347 | Distinct-3: 0.662\nEpoch 1 | Train PPL: 10.898 | Val PPL (TF=1): 10.685 | Val Acc: 0.292\nEpoch 2 | Train PPL: 10.623 | Val PPL (TF=1): 10.498 | Val Acc: 0.294\nEpoch 3 | Train PPL: 10.435 | Val PPL (TF=1): 10.364 | Val Acc: 0.296\nEpoch 4 | Train PPL: 10.269 | Val PPL (TF=1): 10.220 | Val Acc: 0.296\n\n=== Initial text ===\nn'fait pas l'man\nfaut parfois s'debrouiller seul\nya pas d'famille au parloir\npour s'installer au sommet\ncar l'paradis n'est pas louable\non connait les mauvaises passes\nles frappes un deux trois sommeil\non essaye d'pas faire de mal au gens\nd'pas mettre la s\n\n=== Sample Generation ===\ns à déjà de plà lex d'an déppouindest d'ffailà dégu'mêt d'ant à des dreueréges d'e à s là le duve cont là ffffffffffffffffffllà s à là d'a ond'fffffffffres jà déners mêvondélà lllà à lllexxplest plà l\nDistinct-2: 0.367 | Distinct-3: 0.601\nEpoch 5 | Train PPL: 10.093 | Val PPL (TF=1): 10.050 | Val Acc: 0.299\nEpoch 6 | Train PPL: 9.912 | Val PPL (TF=1): 9.918 | Val Acc: 0.301\nEpoch 7 | Train PPL: 9.769 | Val PPL (TF=1): 9.776 | Val Acc: 0.304\nEpoch 10 | Train PPL: 9.455 | Val PPL (TF=1): 9.510 | Val Acc: 0.308\nEpoch 11 | Train PPL: 9.357 | Val PPL (TF=1): 9.423 | Val Acc: 0.311\nEpoch 12 | Train PPL: 9.083 | Val PPL (TF=1): 8.312 | Val Acc: 0.347\n\n=== Initial text ===\nn'fait pas l'man\nfaut parfois s'debrouiller seul\nya pas d'famille au parloir\npour s'installer au sommet\ncar l'paradis n'est pas louable\non connait les mauvaises passes\nles frappes un deux trois sommeil\non essaye d'pas faire de mal au gens\nd'pas mettre la s\n\n=== Sample Generation ===\nestren pitr dest'landen dest vast d'vantrest qu dévan dest dest de à de de dest'touest'destuimeston quntr destr de plestt'est trestr dest l'an dest lest'ast'en st'plest dévrest destestoust'ale dir à f\nDistinct-2: 0.302 | Distinct-3: 0.510\nEpoch 13 | Train PPL: 5.078 | Val PPL (TF=1): 2.778 | Val Acc: 0.696\nEpoch 14 | Train PPL: 1.583 | Val PPL (TF=1): 1.195 | Val Acc: 0.949\nEpoch 15 | Train PPL: 1.087 | Val PPL (TF=1): 1.033 | Val Acc: 0.992\nEpoch 16 | Train PPL: 1.032 | Val PPL (TF=1): 1.023 | Val Acc: 0.994\n\n=== Initial text ===\nn'fait pas l'man\nfaut parfois s'debrouiller seul\nya pas d'famille au parloir\npour s'installer au sommet\ncar l'paradis n'est pas louable\non connait les mauvaises passes\nles frappes un deux trois sommeil\non essaye d'pas faire de mal au gens\nd'pas mettre la s\n\n=== Sample Generation ===\nmêvovovovéghez'sûrêvovovégoz-vovégovovez'owsûrméghmâmômêvégrôtômêvégâmôtômèmêvégoskumèmêvégozégovow'mêvownôtôtôtôtômêvovowz-voïez'grôtômêvégo$govovéghômâmégâmêvovégâmêvow'grômèmêvégrômègâgovovoz-mêvow\nDistinct-2: 0.276 | Distinct-3: 0.439\nEpoch 17 | Train PPL: 1.023 | Val PPL (TF=1): 1.019 | Val Acc: 0.995\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"3.655","metadata":{}}]}