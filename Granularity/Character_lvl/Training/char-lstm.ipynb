{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13220961,"sourceType":"datasetVersion","datasetId":8299981}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook will serve as a way to implement character generation LSTM ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport re\nimport numpy as np\nimport pickle\nimport random\nfrom torch.utils.data import random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:15.563637Z","iopub.execute_input":"2025-10-17T19:56:15.563865Z","iopub.status.idle":"2025-10-17T19:56:19.090981Z","shell.execute_reply.started":"2025-10-17T19:56:15.563843Z","shell.execute_reply":"2025-10-17T19:56:19.090178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/input/rnn-input/encoding_map.pkl\", \"rb\") as f:\n    mapping = pickle.load(f)\n\nmapping[\"PAD\"] = len(mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:19.091798Z","iopub.execute_input":"2025-10-17T19:56:19.092165Z","iopub.status.idle":"2025-10-17T19:56:19.106009Z","shell.execute_reply.started":"2025-10-17T19:56:19.092137Z","shell.execute_reply":"2025-10-17T19:56:19.105233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"# Decode\nint2char = {i: ch for ch, i in mapping.items()}\nprint(int2char)\n\nnb_char = len(int2char)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:19.107714Z","iopub.execute_input":"2025-10-17T19:56:19.107933Z","iopub.status.idle":"2025-10-17T19:56:19.117799Z","shell.execute_reply.started":"2025-10-17T19:56:19.107915Z","shell.execute_reply":"2025-10-17T19:56:19.117108Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creation of the dataset","metadata":{}},{"cell_type":"code","source":"class SongDataset(Dataset):\n    def __init__(self, texts, length_seq, stride, pad_id=mapping[\"PAD\"], use_offset=True):\n        self.samples = []\n        self.length_seq = length_seq\n        self.stride = stride\n        self.pad_id = pad_id\n\n        for text in texts:\n            # --- Sécurité : s'assurer d'un tensor 1D long CPU ---\n            if not isinstance(text, torch.Tensor):\n                text = torch.tensor(text, dtype=torch.long)\n            else:\n                text = text.clone().detach().to(dtype=torch.long, device=\"cpu\").contiguous()\n\n            L = len(text)\n            if L < 2:\n                continue\n\n            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n\n            # --- Boucle principale ---\n            for start in range(offset, max(1, L - self.length_seq - 1), self.stride):\n                x_start, x_end = start, start + self.length_seq\n                y_start, y_end = start + 1, start + 1 + self.length_seq\n\n                x = text[x_start:x_end]\n                y = text[y_start:y_end]\n\n                # --- Padding uniforme ---\n                def pad_to_len(seq, pad_id, target_len):\n                    pad_len = target_len - len(seq)\n                    if pad_len > 0:\n                        seq = torch.cat([seq, torch.full((pad_len,), pad_id, dtype=seq.dtype)])\n                    return seq\n\n                x = pad_to_len(x, self.pad_id, self.length_seq)\n                y = pad_to_len(y, self.pad_id, self.length_seq)\n\n                self.samples.append((x, y))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:19.118663Z","iopub.execute_input":"2025-10-17T19:56:19.118883Z","iopub.status.idle":"2025-10-17T19:56:19.135434Z","shell.execute_reply.started":"2025-10-17T19:56:19.118868Z","shell.execute_reply":"2025-10-17T19:56:19.134693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_ = np.load(\"/kaggle/input/rnn-input/corpora_encoded.npy\",\"r\")\n\nresult = []\nfor t in dataset_:\n    if t == 64 : \n        current = []\n        current.append(t)\n    elif t == 70:\n        current.append(t)\n        result.append(torch.tensor(current))\n    else :\n        current.append(t)\nif current:  \n    result.append(torch.tensor(current))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:19.136243Z","iopub.execute_input":"2025-10-17T19:56:19.136666Z","iopub.status.idle":"2025-10-17T19:56:22.408864Z","shell.execute_reply.started":"2025-10-17T19:56:19.136642Z","shell.execute_reply":"2025-10-17T19:56:22.408277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len_train = int(len(result) * 0.8)\nlen_test = len(result) - len_train\n\ngenerator = torch.Generator().manual_seed(42)\ntrain, test = random_split(result, [len_train, len_test], generator = generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:22.409737Z","iopub.execute_input":"2025-10-17T19:56:22.409996Z","iopub.status.idle":"2025-10-17T19:56:22.421167Z","shell.execute_reply.started":"2025-10-17T19:56:22.409976Z","shell.execute_reply":"2025-10-17T19:56:22.420645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = SongDataset(train,length_seq=256, stride = 64, use_offset = True)\ntest_ds = SongDataset(test,length_seq=256, stride = 64, use_offset = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:22.421783Z","iopub.execute_input":"2025-10-17T19:56:22.422364Z","iopub.status.idle":"2025-10-17T19:56:22.966486Z","shell.execute_reply.started":"2025-10-17T19:56:22.422346Z","shell.execute_reply":"2025-10-17T19:56:22.965675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\".join([int2char[i] for i in train_ds[0][0][:256].numpy()]))\nprint()\nprint(\"\".join([int2char[i] for i in train_ds[1][0][:256].numpy()]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:22.967301Z","iopub.execute_input":"2025-10-17T19:56:22.967541Z","iopub.status.idle":"2025-10-17T19:56:22.972892Z","shell.execute_reply.started":"2025-10-17T19:56:22.967515Z","shell.execute_reply":"2025-10-17T19:56:22.972201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creation of Dataloader and co","metadata":{}},{"cell_type":"code","source":"batch_size = 1024\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True) #Shuffle False because we need the RNN to use previous sequences data to predict next one\ntest_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:22.974929Z","iopub.execute_input":"2025-10-17T19:56:22.975427Z","iopub.status.idle":"2025-10-17T19:56:22.987305Z","shell.execute_reply.started":"2025-10-17T19:56:22.975409Z","shell.execute_reply":"2025-10-17T19:56:22.986581Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"markdown","source":"### Training part","metadata":{}},{"cell_type":"code","source":"class CharLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n        super(CharLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(input_size = embedding_dim, \n                            hidden_size = hidden_size, \n                            num_layers = num_layers, \n                            batch_first=True, dropout = 0.15)\n        self.drop = nn.Dropout(p=0.15)\n        self.ln = nn.LayerNorm(hidden_size)\n\n#        self.proj = nn.Linear(hidden_size, embedding_dim, bias=False)\n        self.fc = nn.Linear(hidden_size, vocab_size, bias=False)\n\n    def forward(self, x, hidden):\n        x = self.drop(self.embedding(x))              # (batch, seq, hidden_size)\n        out, hidden = self.lstm(x, hidden)\n        out = self.drop(self.ln(out))\n        logits = self.fc(out)                  \n        return logits, hidden\n\n    def init_hidden(self, batch_size, device):\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device)\n        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device)\n        return (h0,c0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:22.988046Z","iopub.execute_input":"2025-10-17T19:56:22.988309Z","iopub.status.idle":"2025-10-17T19:56:23.001000Z","shell.execute_reply.started":"2025-10-17T19:56:22.988287Z","shell.execute_reply":"2025-10-17T19:56:23.000292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device1 = torch.device(\"cuda:0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:23.001743Z","iopub.execute_input":"2025-10-17T19:56:23.001898Z","iopub.status.idle":"2025-10-17T19:56:23.018942Z","shell.execute_reply.started":"2025-10-17T19:56:23.001886Z","shell.execute_reply":"2025-10-17T19:56:23.018282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 128\nhidden_size = 512\nvocab_size = len(mapping)\nnum_epoch = 100\n\nnb_step_train = len(train_dl)\nnb_step_test = len(test_dl)\n\nmodel = CharLSTM(vocab_size, embedding_dim, hidden_size, num_layers=3).to(device1)\nmodel = torch.compile(model)\n\n#weights = torch.ones(vocab_size).to(device1)\n\n#Structure marker (really important)\n#for p in [\" \", \"\\n\"]:\n#    idx = mapping[p]\n#    weights[idx] = 1.25  \n\n#Ponctuation (important) + part indic\n#for p in [\",\", \".\", \"'\"]:\n#    idx = mapping[p]\n#    weights[idx] = 1.1  \n\nloss_fn = nn.CrossEntropyLoss(ignore_index=72)\nval_fn = nn.CrossEntropyLoss(ignore_index=72)\n\nopti = torch.optim.AdamW(model.parameters(), lr=0.003, weight_decay=1e-4)\nsched_warm = torch.optim.lr_scheduler.LinearLR(opti, start_factor=0.2, end_factor=1.0, total_iters=nb_step_train*5)\nsched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*10, T_mult=2, eta_min=0.001) #1 epoch => 2 => 4 => 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:23.019653Z","iopub.execute_input":"2025-10-17T19:56:23.019846Z","iopub.status.idle":"2025-10-17T19:56:27.685270Z","shell.execute_reply.started":"2025-10-17T19:56:23.019831Z","shell.execute_reply":"2025-10-17T19:56:27.684510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nclass SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        assert rho >= 0.0, \"Invalid rho\"\n        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n        super().__init__(params, defaults)\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        scale = [g['rho'] / (grad_norm + 1e-12) for g in self.param_groups]\n        for group, s in zip(self.param_groups, scale):\n            for p in group['params']:\n                if p.grad is None: continue\n                e_w = (torch.pow(p, 2) if group['adaptive'] else 1.0) * p.grad * s.to(p)\n                p.add_(e_w)\n                self.state[p]['e_w'] = e_w\n        if zero_grad:\n            self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None: continue\n                p.sub_(self.state[p]['e_w'])\n        self.base_optimizer.step()\n        if zero_grad:\n            self.zero_grad()\n\n    def _grad_norm(self):\n        device = self.param_groups[0]['params'][0].device\n        norms = []\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is not None:\n                    scale = torch.abs(p) if group['adaptive'] else 1.0\n                    norms.append((scale * p.grad).norm(p=2))\n        return torch.norm(torch.stack(norms), p=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:27.686060Z","iopub.execute_input":"2025-10-17T19:56:27.686428Z","iopub.status.idle":"2025-10-17T19:56:27.695610Z","shell.execute_reply.started":"2025-10-17T19:56:27.686405Z","shell.execute_reply":"2025-10-17T19:56:27.694879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\n@torch.no_grad()\ndef evaluate_tf1(model, dl, loss_fn, device, vocab_size):\n    \"\"\"Validation with teacher forcing = 1 (parallel, fast). Returns (ppl, acc).\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    correct = 0\n    total = 0\n\n    for X, Y in dl:\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        bs, sl = X.size(0), X.size(1)\n        hid = model.init_hidden(bs, device1)\n\n        with torch.amp.autocast(device_type=\"cuda\"):\n            pred, _ = model(X, hid)  # (bs, sl, vocab)\n            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n        total_loss += loss.item() * bs * sl\n        total_tokens += bs * sl\n\n        pred_ids = pred.argmax(dim=-1)\n        correct += (pred_ids == Y).sum().item()\n        total += bs * sl\n\n    ppl = math.exp(total_loss / max(1, total_tokens))\n    acc = correct / max(1, total)\n    return ppl, acc\n\n\n@torch.no_grad()\ndef evaluate_free(model, dl, loss_fn, device):\n    \"\"\"\n    Autoregressive validation (teacher forcing = 0).\n    Steps one token at a time and feeds predictions back in.\n    Returns ppl (computed on next-token NLL).\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for X, Y in dl:\n        X = X.to(device)\n        Y = Y.to(device, dtype=torch.long)\n        bs, sl = X.size(0), X.size(1)\n        hid = model.init_hidden(bs, device1)\n\n        # Start with the first input token\n        inp = X[:, :1]  # (bs, 1)\n        for t in range(sl):\n            with torch.amp.autocast(device_type=\"cuda\"):\n                pred, hid = model(inp, hid)          # (bs, 1, vocab)\n                logits = pred[:, -1, :]              # (bs, vocab)\n                loss = loss_fn(logits, Y[:, t])      # CE over current step\n\n            total_loss += loss.item() * bs\n            total_tokens += bs\n\n            # Greedy next-token to feed back in\n            next_token = logits.argmax(dim=-1).unsqueeze(1)  # (bs, 1)\n            inp = next_token\n\n    ppl = math.exp(total_loss / max(1, total_tokens))\n    return ppl\n\ndef sample_with_temp(logits, temp=1.0):\n    probs = (logits / temp).softmax(dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n    return next_token\n\ndef distinct_n_chars(text, n=3):\n    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n    return len(set(ngrams)) / max(1, len(ngrams))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:56:27.696616Z","iopub.execute_input":"2025-10-17T19:56:27.697086Z","iopub.status.idle":"2025-10-17T19:56:27.721023Z","shell.execute_reply.started":"2025-10-17T19:56:27.697058Z","shell.execute_reply":"2025-10-17T19:56:27.720309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#base_optimizer = torch.optim.AdamW  \n#opti = SAM(model.parameters(), base_optimizer, lr=0.0005, rho=0.05)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:30:35.158299Z","iopub.execute_input":"2025-10-17T20:30:35.158511Z","iopub.status.idle":"2025-10-17T20:30:35.161833Z","shell.execute_reply.started":"2025-10-17T20:30:35.158494Z","shell.execute_reply":"2025-10-17T20:30:35.161159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l_tot = []\nteacher_forcing_ratio = 1\nbest_val = float(\"inf\")\n\nscaler = torch.amp.GradScaler()\n\nfor epoch in range(num_epoch):\n\n    # --- Teacher Forcing ratio decay (linear) ---\n#    if epoch % 2 == 0 :\n#        teacher_forcing_ratio = max(0.0, min(1.0, teacher_forcing_ratio - 0.02))\n#        print(f\"\\nEpoch {epoch} | Teacher forcing ratio = {teacher_forcing_ratio:.2f}\")\n\n    model.train()\n\n    # -------------- TRAIN LOOP --------------\n    train_loss = 0.0\n\n    for X, Y in iter(train_dl):\n        hid = model.init_hidden(batch_size, device1)\n        X = X.to(device1)\n        Y = Y.to(device1, dtype=torch.long)\n        opti.zero_grad(set_to_none=True)\n\n        if opti.__class__.__name__ == \"SAM\" :\n            def closure():\n                opti.zero_grad(set_to_none=True)\n                with torch.amp.autocast(device_type=\"cuda\"):\n                    pred, hid_ = model(X, hid)\n                    loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n                scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n                return loss\n\n        # ----- SAM two-step -----\n        # 1️⃣ First (ascent) step – move slightly uphill\n            loss = closure()\n            scaler.unscale_(opti.base_optimizer)  # unscale before the manual step\n            opti.first_step(zero_grad=True)\n\n        # 2️⃣ Second (descent) step – compute new grad at perturbed point\n            closure()\n            opti.second_step(zero_grad=True)\n            scaler.update()\n\n        else : \n            with torch.amp.autocast(device_type=\"cuda\"):\n                pred, hid = model(X, hid)\n                loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n\n            scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            scaler.step(opti)\n            scaler.update()\n\n            if sched_post.T_cur == 0 and epoch > 9:  #After warm restart decrease the max learning rate\n                sched_post.base_lrs[0] = sched_post.base_lrs[0] * 0.85\n                print(f\"Decrease {sched_post.base_lrs[0]}, {sched_post.eta_min}\")\n\n            if epoch < 8:\n                sched_warm.step()\n            else:\n                sched_post.step()\n        train_loss += loss.detach().item() \n\n    train_ppl = math.exp(train_loss / nb_step_train)\n\n    # -------------- VALIDATION --------------\n    val_ppl_tf1, val_acc = evaluate_tf1(model, test_dl, val_fn, device1, vocab_size)\n    val_ppl_free = evaluate_free(model, test_dl, val_fn, device1)\n\n    print(\n        f\"Epoch {epoch} | \"\n        f\"Train PPL: {train_ppl:.3f} | \"\n        f\"Val PPL (TF=1): {val_ppl_tf1:.3f} | \"\n        f\"Val PPL (free): {val_ppl_free:.3f} | \"\n        f\"Val Acc: {val_acc:.3f}\"\n    )\n\n    # --------- Sample generation + diversity metrics ---------\n    if epoch % 5 == 0 :\n        model.eval()\n        with torch.no_grad():\n        # prends les 5 premiers chars du batch courant comme seed\n            start = X[0:1, :20]  \n            hid_gen = model.init_hidden(1,device1)\n            inp = start\n\n            gen_chars = []\n            for t in range(200):  # génère 200 caractères\n                pred, hid_gen = model(inp, hid_gen)\n                logits = pred[:, -1, :]  # dernier pas\n                next_char = sample_with_temp(logits, temp=0.6)\n                gen_chars.append(int2char[next_char.item()])\n                inp = next_char # feed back\n\n            gen_text = \"\".join(gen_chars)\n\n        d2 = distinct_n_chars(gen_text, n=2)\n        d3 = distinct_n_chars(gen_text, n=3)\n\n    \n        print(\"\\n=== Initial text ===\")\n        print(\"\".join([int2char[i] for i in X[0:1,:].squeeze(0).tolist()]))\n        print(\"\\n=== Sample Generation ===\")\n        print(gen_text[:200])  # affiche les 200 premiers chars\n        print(f\"Distinct-2: {d2:.3f} | Distinct-3: {d3:.3f}\", end=\"\\n\")\n\n    if val_ppl_tf1 < best_val :\n        best_val = val_ppl_tf1\n        l_tot.append(val_acc)\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": opti.state_dict(),\n                \"scheduler_state_dict\": sched_post.state_dict(),\n                \"val_ppl\": val_ppl_tf1,\n            },\n            \"model\",\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:30:35.194725Z","iopub.execute_input":"2025-10-17T20:30:35.195100Z","execution_failed":"2025-10-17T20:32:07.699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RNN : Val PPL (TF=1): 3.6423183370497765","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:30:34.803870Z","iopub.status.idle":"2025-10-17T20:30:34.804084Z","shell.execute_reply.started":"2025-10-17T20:30:34.803984Z","shell.execute_reply":"2025-10-17T20:30:34.803994Z"}},"outputs":[],"execution_count":null}]}