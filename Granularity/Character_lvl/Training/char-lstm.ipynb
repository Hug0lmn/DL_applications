{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:05.670592Z",
     "iopub.status.busy": "2025-10-05T19:54:05.670318Z",
     "iopub.status.idle": "2025-10-05T19:54:05.675114Z",
     "shell.execute_reply": "2025-10-05T19:54:05.674218Z",
     "shell.execute_reply.started": "2025-10-05T19:54:05.670572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:05.676833Z",
     "iopub.status.busy": "2025-10-05T19:54:05.676583Z",
     "iopub.status.idle": "2025-10-05T19:54:05.694945Z",
     "shell.execute_reply": "2025-10-05T19:54:05.694299Z",
     "shell.execute_reply.started": "2025-10-05T19:54:05.676809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"../../../Corpus/Encoding_RNN_LSTM/Char_level/encoding_map.pkl\", \"rb\") as f:\n",
    "    mapping = pickle.load(f)\n",
    "\n",
    "mapping[\"PAD\"] = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:05.695797Z",
     "iopub.status.busy": "2025-10-05T19:54:05.695643Z",
     "iopub.status.idle": "2025-10-05T19:54:05.708949Z",
     "shell.execute_reply": "2025-10-05T19:54:05.708259Z",
     "shell.execute_reply.started": "2025-10-05T19:54:05.695784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decode\n",
    "int2char = {i: ch for ch, i in mapping.items()}\n",
    "print(int2char)\n",
    "nb_char = len(int2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is a huge chunk of text but we need to split it in songs because we don't want that the model learn that there is a chance of having a text after the end of a song. Thiw will prevent learning non existing relationship in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:05.710602Z",
     "iopub.status.busy": "2025-10-05T19:54:05.710382Z",
     "iopub.status.idle": "2025-10-05T19:54:08.877238Z",
     "shell.execute_reply": "2025-10-05T19:54:08.876431Z",
     "shell.execute_reply.started": "2025-10-05T19:54:05.710586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_ = np.load(\"../../../Corpus/Encoding_RNN_LSTM/Char_level/corpora_encoded.npy\",\"r\")\n",
    "\n",
    "result = []\n",
    "for t in dataset_:\n",
    "    if t == 64 : # α or beginning of a song\n",
    "        current = []\n",
    "        current.append(t)\n",
    "    elif t == 70: # θ or end of a song\n",
    "        current.append(t)\n",
    "        result.append(torch.tensor(current))\n",
    "    else :\n",
    "        current.append(t)\n",
    "if current:  \n",
    "    result.append(torch.tensor(current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(Dataset):\n",
    "    def __init__(self, texts, length_seq, stride, pad_id=mapping[\"PAD\"], use_offset=True):\n",
    "        self.samples = []\n",
    "        self.length_seq = length_seq\n",
    "        self.stride = stride\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        for text in texts:\n",
    "\n",
    "            L = len(text)\n",
    "            offset = torch.randint(0, stride, (1,)).item() if use_offset else 0\n",
    "\n",
    "            # --- Boucle principale ---\n",
    "            for start in range(offset, max(1, L - self.length_seq - 1), self.stride):\n",
    "                x_start, x_end = start, start + self.length_seq\n",
    "                y_start, y_end = start + 1, start + 1 + self.length_seq\n",
    "\n",
    "                x = text[x_start:x_end]\n",
    "                y = text[y_start:y_end]\n",
    "\n",
    "                # --- Padding uniforme ---\n",
    "                def pad_to_len(seq, pad_id, target_len):\n",
    "                    pad_len = target_len - len(seq)\n",
    "                    if pad_len > 0:\n",
    "                        seq = torch.cat([seq, torch.full((pad_len,), pad_id, dtype=seq.dtype)])\n",
    "                    return seq\n",
    "\n",
    "                x = pad_to_len(x, self.pad_id, self.length_seq)\n",
    "                y = pad_to_len(y, self.pad_id, self.length_seq)\n",
    "\n",
    "                self.samples.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "seq_length = 256\n",
    "stride = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:11.238547Z",
     "iopub.status.busy": "2025-10-05T19:54:11.238374Z",
     "iopub.status.idle": "2025-10-05T19:54:11.255290Z",
     "shell.execute_reply": "2025-10-05T19:54:11.254762Z",
     "shell.execute_reply.started": "2025-10-05T19:54:11.238533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Then we can split between songs and not part of a song\n",
    "len_train = int(len(result) * 0.85)\n",
    "len_test = len(result) - len_train\n",
    "\n",
    "train, test = random_split(result, [len_train, len_test])\n",
    "\n",
    "#Use of stride 16 to limit the overlap between each sequence, that reduce training time and reduce overfit \n",
    "# and also because one character doesn't contain a lot of information in itself\n",
    "train_ds = SongDataset(train,length_seq=seq_length, stride = stride, use_offset = True)\n",
    "test_ds = SongDataset(test,length_seq=seq_length, stride = stride, use_offset = False)\n",
    "\n",
    "#Shuffle False because a song is like a time series and cannot be shuffle randomly inside it. The position of each part has a meaning.\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=\"cuda:0\", shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:11.256146Z",
     "iopub.status.busy": "2025-10-05T19:54:11.255940Z",
     "iopub.status.idle": "2025-10-05T19:54:11.279297Z",
     "shell.execute_reply": "2025-10-05T19:54:11.278808Z",
     "shell.execute_reply.started": "2025-10-05T19:54:11.256124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = num_layers, \n",
    "                            batch_first=True, dropout = 0.2)\n",
    "        self.drop = nn.Dropout(p=0.15)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.drop(self.embedding(x))     \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.drop(out)\n",
    "        logits = self.fc(out)                  \n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device)\n",
    "        return (h0,c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:11.280113Z",
     "iopub.status.busy": "2025-10-05T19:54:11.279942Z",
     "iopub.status.idle": "2025-10-05T19:54:11.296046Z",
     "shell.execute_reply": "2025-10-05T19:54:11.295567Z",
     "shell.execute_reply.started": "2025-10-05T19:54:11.280099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device1 = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:11.297172Z",
     "iopub.status.busy": "2025-10-05T19:54:11.296946Z",
     "iopub.status.idle": "2025-10-05T19:54:19.441707Z",
     "shell.execute_reply": "2025-10-05T19:54:19.440876Z",
     "shell.execute_reply.started": "2025-10-05T19:54:11.297149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_size = 512\n",
    "vocab_size = len(mapping)\n",
    "num_epoch = 150\n",
    "\n",
    "nb_step_train = len(train_dl)\n",
    "nb_step_test = len(test_dl)\n",
    "\n",
    "model = CharLSTM(vocab_size, embedding_dim, hidden_size, num_layers=2).to(device1)\n",
    "model = torch.compile(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=72)\n",
    "\n",
    "opti = torch.optim.AdamW(model.parameters(), lr=0.003, weight_decay=1e-4)\n",
    "sched_warm = torch.optim.lr_scheduler.LinearLR(opti, start_factor=0.2, end_factor=1.0, total_iters=nb_step_train*3)\n",
    "sched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*10, T_mult=2, eta_min=0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T19:54:19.442883Z",
     "iopub.status.busy": "2025-10-05T19:54:19.442585Z",
     "iopub.status.idle": "2025-10-05T19:54:19.453920Z",
     "shell.execute_reply": "2025-10-05T19:54:19.453062Z",
     "shell.execute_reply.started": "2025-10-05T19:54:19.442865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_tf1(model, dl, loss_fn, device, vocab_size, bs = batch_size, sl = seq_length):\n",
    "    \"\"\"Validation with teacher forcing = 1 (parallel, fast). Returns (ppl, acc).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, Y in dl:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device, dtype=torch.long)\n",
    "        hid = model.init_hidden(bs, device1)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            pred, _ = model(X, hid) \n",
    "            loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "\n",
    "        total_loss += loss.item() * bs * sl\n",
    "        total_tokens += bs * sl\n",
    "\n",
    "        pred_ids = pred.argmax(dim=-1)\n",
    "        correct += (pred_ids == Y).sum().item()\n",
    "        total += bs * sl\n",
    "\n",
    "    ppl = math.exp(total_loss / max(1, total_tokens))\n",
    "    acc = correct / max(1, total)\n",
    "    return ppl, acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_free(model, dl, loss_fn, device, bs=batch_size, sl=seq_length):\n",
    "    \"\"\"\n",
    "    Autoregressive validation (teacher forcing = 0).\n",
    "    Steps one token at a time and feeds predictions back in.\n",
    "    Returns ppl.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for X, Y in dl:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device, dtype=torch.long)\n",
    "        hid = model.init_hidden(bs, device1)\n",
    "\n",
    "        # Start with the first input token\n",
    "        inp = X[:, :1]  # (bs, 1)\n",
    "        for t in range(sl):\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred, hid = model(inp, hid)          # (bs, 1, vocab)\n",
    "                logits = pred[:, -1, :]              # (bs, vocab)\n",
    "                loss = loss_fn(logits, Y[:, t])      # CE over current step\n",
    "\n",
    "            total_loss += loss.item() * bs\n",
    "            total_tokens += bs\n",
    "\n",
    "            # Greedy next-token to feed back in\n",
    "            next_token = logits.argmax(dim=-1).unsqueeze(1)  # (bs, 1)\n",
    "            inp = next_token\n",
    "\n",
    "    ppl = math.exp(total_loss / max(1, total_tokens))\n",
    "    return ppl\n",
    "\n",
    "def sample_with_temp(logits, temp=1.0):\n",
    "    probs = (logits / temp).softmax(dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token\n",
    "\n",
    "def distinct_n_chars(text, n=3):\n",
    "    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    return len(set(ngrams)) / max(1, len(ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, first I put :\n",
    "- Every 2 epoch : teacher_forcing_ratio - 0.02\n",
    "\n",
    "Then when teacher_forcing_ratio < 0.5 :\n",
    "- Every epoch or 2 epoch : teacher_forcing_ratio - 0.01\n",
    "\n",
    "I stop the teacher_forcing_ratio around 0.25/0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T22:30:54.570047Z",
     "iopub.status.busy": "2025-10-05T22:30:54.569766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "l_tot = []\n",
    "bs = batch_size\n",
    "sl = seq_length\n",
    "teacher_forcing_ratio = 1\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    # --- Teacher Forcing ratio decay ---\n",
    "    if epoch % 2 == 0 :\n",
    "        teacher_forcing_ratio = max(0.0, min(1.0, teacher_forcing_ratio - 0.02))\n",
    "        print(f\"\\nEpoch {epoch} | Teacher forcing ratio = {teacher_forcing_ratio:.2f}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # -------------- TRAIN LOOP --------------\n",
    "    train_loss_sum = 0.0\n",
    "    train_tokens = 0\n",
    "    nb_step_train = 0\n",
    "\n",
    "    for X, Y in iter(train_dl):\n",
    "        hid = model.init_hidden(batch_size, device1)\n",
    "        X = X.to(device1)\n",
    "        Y = Y.to(device1, dtype=torch.long)\n",
    "        opti.zero_grad(set_to_none=True)\n",
    "\n",
    "        if teacher_forcing_ratio == 1.0:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred, hid = model(X, hid)\n",
    "                loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "        else:\n",
    "            # ---- Pass 1: forward with TF=1 ----\n",
    "            with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred_tf, _ = model(X, hid)     \n",
    "            pred_tokens = pred_tf.argmax(dim=-1)  \n",
    "\n",
    "            # ---- Random mask for TF < 1 ----\n",
    "            mask = (torch.rand_like(X.float()) < teacher_forcing_ratio)\n",
    "            X_mixed = torch.where(mask, X, pred_tokens)\n",
    "\n",
    "            # ---- Pass 2: forward with partial TF ----\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                pred, hid = model(X_mixed, hid)\n",
    "                loss = loss_fn(pred.view(-1, vocab_size), Y.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        scaler.step(opti)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss_sum += loss.detach().item() * bs * sl\n",
    "        train_tokens += bs * sl\n",
    "        nb_step_train += 1\n",
    "\n",
    "        if epoch < 3:\n",
    "            sched_warm.step()\n",
    "        else:\n",
    "            sched_post.step()\n",
    "\n",
    "    train_ppl = math.exp(train_loss_sum / max(1, train_tokens))\n",
    "\n",
    "    # -------------- VALIDATION --------------\n",
    "    val_ppl_tf1, val_acc = evaluate_tf1(model, test_dl, loss_fn, device1, vocab_size)\n",
    "    val_ppl_free = evaluate_free(model, test_dl, loss_fn, device1)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"Train PPL: {train_ppl:.3f} | \"\n",
    "        f\"Val PPL (TF=1): {val_ppl_tf1:.3f} | \"\n",
    "        f\"Val PPL (free): {val_ppl_free:.3f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # --------- Sample generation + diversity metrics ---------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Warm up for generation\n",
    "        start = X[0:1, :20]  \n",
    "        hid_gen = model.init_hidden(1,device1)\n",
    "        inp = start\n",
    "\n",
    "        gen_chars = []\n",
    "        for t in range(200): \n",
    "            pred, hid_gen = model(inp, hid_gen)\n",
    "            logits = pred[:, -1, :]\n",
    "            next_char = sample_with_temp(logits, temp=0.6)\n",
    "            gen_chars.append(int2char[next_char.item()])\n",
    "            inp = next_char # feed back\n",
    "\n",
    "        gen_text = \"\".join(gen_chars)\n",
    "\n",
    "    d2 = distinct_n_chars(gen_text, n=2)\n",
    "    d3 = distinct_n_chars(gen_text, n=3)\n",
    "\n",
    "    \n",
    "    if epoch % 4 == 0 :\n",
    "        print(\"\\n=== Initial text ===\")\n",
    "        print(\"\".join([int2char[i] for i in X[0:1,:].squeeze(0).tolist()]))\n",
    "        print(\"\\n=== Sample Generation ===\")\n",
    "        print(gen_text[:200]) \n",
    "        print(f\"Distinct-2: {d2:.3f} | Distinct-3: {d3:.3f}\", end=\"\\n\")\n",
    "\n",
    "    l_tot.append(val_acc)\n",
    "    torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": opti.state_dict(),\n",
    "                \"scheduler_state_dict\": sched_post.state_dict(),\n",
    "                \"val_acc\": val_acc,\n",
    "            },\n",
    "            \"model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.9"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8299981,
     "sourceId": 13220961,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
