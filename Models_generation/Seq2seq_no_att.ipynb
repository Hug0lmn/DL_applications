{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c76ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Functions_generation import Decodeur_no_att, Encodeur_no_att,generate_a_song_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aac604",
   "metadata": {},
   "source": [
    "# Preparation before Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4934523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"..\\Seq2seq\\Files\\Encoding_map.pkl\", \"rb\") as f:\n",
    "    mapping = pickle.load(f)\n",
    "\n",
    "int2char = {i: ch for ch, i in mapping.items()}\n",
    "nb_char = len(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee2919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_csv(r\"..\\Markov\\transition_matrix.csv\")\n",
    "\n",
    "states = np.array(matrix.iloc[6])\n",
    "prob_transi = np.array(matrix.iloc[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8ae7b",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6200a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(mapping)\n",
    "embedding_size = 96\n",
    "hidden_size = 512\n",
    "num_epoch = 200\n",
    "\n",
    "#Some of the mapping are only for the encodeur so the decodeur can't produce them, we need to mask them from the loss\n",
    "mapping_inverse = {i: ch for ch, i in mapping.items()}\n",
    "masked_mapping = list(mapping_inverse.keys())[116:-1]\n",
    "\n",
    "mask = torch.zeros(vocab_size, dtype=torch.bool)\n",
    "mask[masked_mapping] = True\n",
    "\n",
    "enco = Encodeur_no_att(vocab_size, embedding_size, hidden_size, num_layers=2)\n",
    "deco = Decodeur_no_att(vocab_size, embedding_size, hidden_size, mask,num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24ca519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decodeur_no_att(\n",
       "  (embed): Embedding(223, 96)\n",
       "  (lstm): LSTM(96, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (final): Linear(in_features=512, out_features=223, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"..\\Models\\Seq2seq_noatt_model.pt\", map_location=\"cpu\")\n",
    "\n",
    "enco.load_state_dict(ckpt[\"encoder_state_dict\"])\n",
    "deco.load_state_dict(ckpt[\"decoder_state_dict\"])\n",
    "\n",
    "enco.eval()\n",
    "deco.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29137658",
   "metadata": {},
   "source": [
    "## Preparation before generating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5154de",
   "metadata": {},
   "source": [
    "We need to create the first context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c77739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BEGINNING>', '<COUPLET>', '<REFRAIN>', '<COUPLET>', '<REFRAIN>', '<COUPLET>', '<END>']\n"
     ]
    }
   ],
   "source": [
    "struct = generate_a_song_structure(prob_transi.astype(float),states)\n",
    "\n",
    "encoded = torch.tensor([mapping[c] for c in struct[1]], dtype=torch.long).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802cfaf",
   "metadata": {},
   "source": [
    "At first, I will generate the song lyrics by indicating the length of the part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d413ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_context = [\"<PART=COUPLET>\", '<lines=1>', '<total=4>', 'Previous :', '<START>', 'Previous :', '<START>']\n",
    "\n",
    "context_encode = []\n",
    "\n",
    "for i in first_context :\n",
    "    if mapping[i] :\n",
    "        context_encode.append(mapping[i])\n",
    "    else : \n",
    "        for j in i :\n",
    "            context_encode.append(mapping[j])\n",
    "\n",
    "length_cont = len(context_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc535de",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, c = enco(torch.tensor([context_encode]), torch.tensor([length_cont]))\n",
    "\n",
    "input_t = torch.tensor([mapping[\"START\"]])\n",
    "outputs = [\"START\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd31e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OUPETITINMOUPAMPLDJPETNMUPETNMETAQTNONMJNMJEOTJCCSLEJJBMNONMNBJBMETLBMENNMEETNCLOTLNMYAJNONMNMYETNM'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while outputs[-1] != \"END\" :\n",
    "    with torch.no_grad():\n",
    "\n",
    "        emb = deco.embed(input_t).unsqueeze(1)\n",
    "        out, (h_dec, c_dec) = deco.lstm(emb, (h, c))\n",
    "\n",
    "        logit = deco.final(out.squeeze(1))\n",
    "        masked_logit = logit.masked_fill(deco.mask, float(\"-inf\"))\n",
    "\n",
    "        probs = torch.softmax(masked_logit/0.5, dim=-1)\n",
    "        topk_probs, topk_idx = torch.topk(probs, 20, dim=-1)\n",
    "\n",
    "        idx = torch.multinomial(topk_probs, 1)\n",
    "        next_token = topk_idx.gather(-1, idx)\n",
    "#        next_token = probs.argmax()\n",
    "        outputs.append(int2char[next_token.item()])\n",
    "\n",
    "        input_t = next_token.squeeze(1)\n",
    "\n",
    "        if len(outputs) > 100 :\n",
    "            break\n",
    "\n",
    "\"\".join(outputs[1:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
