{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QtPFNL1KiBC"
      },
      "source": [
        "## This code will implement the training of the Seq2seq light attention model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code is used on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V03ZNGsKiBE"
      },
      "source": [
        "## Pipeline comparaison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.266969Z",
          "iopub.status.busy": "2025-09-14T14:33:55.266626Z",
          "iopub.status.idle": "2025-09-14T14:33:55.274696Z",
          "shell.execute_reply": "2025-09-14T14:33:55.273631Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.266948Z"
        },
        "id": "b8-6hanaKiBE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.277073Z",
          "iopub.status.busy": "2025-09-14T14:33:55.276753Z",
          "iopub.status.idle": "2025-09-14T14:33:55.316805Z",
          "shell.execute_reply": "2025-09-14T14:33:55.315604Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.277042Z"
        },
        "id": "Tp3tb0-0KiBF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open(\"/content/Encoding_map.pkl\", \"rb\") as f:\n",
        "    mapping = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.318033Z",
          "iopub.status.busy": "2025-09-14T14:33:55.3177Z",
          "iopub.status.idle": "2025-09-14T14:33:55.326568Z",
          "shell.execute_reply": "2025-09-14T14:33:55.325559Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.31801Z"
        },
        "id": "krKcpZ7zKiBF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MMapSeq2Seq(Dataset):\n",
        "    def __init__(self, prefix=\"\"):\n",
        "        self.ctx = np.load(prefix+\"context_np.npy\",\"r\")\n",
        "        self.x   = np.load(prefix+\"X_np.npy\",\"r\")\n",
        "        self.y   = np.load(prefix+\"Y_np.npy\",\"r\")\n",
        "\n",
        "        self.ctx_off = np.load(prefix+\"context_offset.npy\")\n",
        "        self.ctx_len = np.load(prefix+\"context_length.npy\")\n",
        "        self.x_off   = np.load(prefix+\"X_offset.npy\")\n",
        "        self.x_len   = np.load(prefix+\"X_length.npy\")\n",
        "        self.y_off   = np.load(prefix+\"Y_offset.npy\")\n",
        "        self.y_len   = np.load(prefix+\"Y_length.npy\")\n",
        "\n",
        "        assert len(self.ctx_len) == len(self.x_len) == len(self.y_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ctx_len)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # vues memmap -> torch (zéro copie CPU)\n",
        "        s, L = int(self.ctx_off[i]), int(self.ctx_len[i])\n",
        "        ctx = torch.from_numpy(self.ctx[s:s+L].astype(np.int32))\n",
        "\n",
        "        s, L = int(self.x_off[i]), int(self.x_len[i])\n",
        "        x = torch.from_numpy(self.x[s:s+L].astype(np.int32))\n",
        "\n",
        "        s, L = int(self.y_off[i]), int(self.y_len[i])\n",
        "        y = torch.from_numpy(self.y[s:s+L].astype(np.int32))\n",
        "\n",
        "        return ctx, x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.328013Z",
          "iopub.status.busy": "2025-09-14T14:33:55.327691Z",
          "iopub.status.idle": "2025-09-14T14:33:55.341949Z",
          "shell.execute_reply": "2025-09-14T14:33:55.340953Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.327977Z"
        },
        "id": "xntaUTDNKiBG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    ctxs, xs, ys = zip(*batch)  # tuples de 1D tensors CPU\n",
        "    ctxs = [t.to(torch.long, copy=False) for t in ctxs]\n",
        "    xs   = [t.to(torch.long, copy=False) for t in xs]\n",
        "    ys   = [t.to(torch.long, copy=False) for t in ys]\n",
        "\n",
        "    ctx_pad = torch.nn.utils.rnn.pad_sequence(ctxs, batch_first=True, padding_value=PAD_ID)\n",
        "    x_pad   = torch.nn.utils.rnn.pad_sequence(xs,   batch_first=True, padding_value=PAD_ID)\n",
        "    y_pad   = torch.nn.utils.rnn.pad_sequence(ys,   batch_first=True, padding_value=PAD_ID)\n",
        "\n",
        "    ctx_len = torch.tensor([t.numel() for t in ctxs], dtype=torch.int64)  # CPU Long (pack_padded)\n",
        "    x_len   = torch.tensor([t.numel() for t in xs],   dtype=torch.int64)\n",
        "\n",
        "    return (ctx_pad, x_pad, y_pad), (ctx_len, x_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsn4SYICKiBG"
      },
      "source": [
        "### Now, pipeline prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.343416Z",
          "iopub.status.busy": "2025-09-14T14:33:55.342939Z",
          "iopub.status.idle": "2025-09-14T14:33:55.463839Z",
          "shell.execute_reply": "2025-09-14T14:33:55.462764Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.343393Z"
        },
        "id": "imr2hBe_KiBI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "PAD_ID = 0\n",
        "ds = MMapSeq2Seq(prefix=\"/content/\")  # mets des préfixes train/val si tu split offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.465034Z",
          "iopub.status.busy": "2025-09-14T14:33:55.464687Z",
          "iopub.status.idle": "2025-09-14T14:33:55.487296Z",
          "shell.execute_reply": "2025-09-14T14:33:55.486425Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.465006Z"
        },
        "id": "E21LZ5KsKiBJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_size = int(0.85 * len(ds))\n",
        "test_size   = len(ds) - train_size\n",
        "train_ds, test_ds = random_split(ds, [train_size, test_size], torch.Generator().manual_seed(42))\n",
        "\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.489665Z",
          "iopub.status.busy": "2025-09-14T14:33:55.489421Z",
          "iopub.status.idle": "2025-09-14T14:33:55.686217Z",
          "shell.execute_reply": "2025-09-14T14:33:55.68531Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.489645Z"
        },
        "id": "nPG7r0uxKiBJ",
        "outputId": "da90a23b-7f63-4d4d-fac3-3fad06970dbd",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "961"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len_max = 0\n",
        "for i in ds :\n",
        "    if len(i[0]) > len_max :\n",
        "        len_max = len(i[0])\n",
        "len_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.687399Z",
          "iopub.status.busy": "2025-09-14T14:33:55.687107Z",
          "iopub.status.idle": "2025-09-14T14:33:55.695664Z",
          "shell.execute_reply": "2025-09-14T14:33:55.694691Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.687374Z"
        },
        "id": "rjkLefbGKiBJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if device == \"cpu\" :\n",
        "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate) #Shuffle False because we need the RNN to use previous sequences data to predict next one\n",
        "    test_ld = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate)\n",
        "else :\n",
        "    train_ld = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=device,\n",
        "                        num_workers=4, shuffle=True, drop_last=True, persistent_workers=False, collate_fn=collate) #Shuffle False because we need the RNN to use previous sequences data to predict next one\n",
        "    test_ld = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=device,\n",
        "                        num_workers=2, shuffle=False, drop_last=True, persistent_workers=False, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed9NZlt4KiBJ"
      },
      "source": [
        "List of operations :\n",
        "\n",
        "- First the entire batch pass through the Encodeur, we extract the h,c from this\n",
        "\n",
        "- Then we iterate over all the length of the sequence, with the previous input and we use at first the h,c from the Encodeur\n",
        "\n",
        "- Then we calculate the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUSXEt3_KiBJ"
      },
      "source": [
        "## Create Seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.697263Z",
          "iopub.status.busy": "2025-09-14T14:33:55.697013Z",
          "iopub.status.idle": "2025-09-14T14:33:55.715187Z",
          "shell.execute_reply": "2025-09-14T14:33:55.714258Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.697245Z"
        },
        "id": "2xQxJAgvKiBJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Encodeur(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
        "        super(Encodeur, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers = num_layers,\n",
        "                            dropout = 0.2,\n",
        "                            batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, length_batch):\n",
        "\n",
        "        emb = self.embed(x)\n",
        "        emb_drop = self.dropout(emb)\n",
        "\n",
        "        packed_x = pack_padded_sequence(emb_drop, length_batch, batch_first=True, enforce_sorted=False)\n",
        "        packed_out, (h,c) = self.lstm(packed_x)\n",
        "        output, length_out = pad_packed_sequence(packed_out, batch_first=True, total_length=x.size(1))\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        return output, length_out, h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-13T17:44:09.275303Z",
          "iopub.status.busy": "2025-09-13T17:44:09.274972Z",
          "iopub.status.idle": "2025-09-13T17:44:09.281883Z",
          "shell.execute_reply": "2025-09-13T17:44:09.280985Z",
          "shell.execute_reply.started": "2025-09-13T17:44:09.275276Z"
        },
        "id": "ayXgHrZkKiBK"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "    def forward(self, query, keys, mask):\n",
        "        #query (B,H)\n",
        "        #keys/context_out (B,S,H) where S represent each unit time\n",
        "\n",
        "        q = self.Wa(query).unsqueeze(1)            # (B,1,H)\n",
        "        k = self.Ua(keys)                           # (B,S,H)\n",
        "        scores = self.Va(torch.tanh(q + k))  # (B,S,1)\n",
        "        scores = scores.squeeze(-1) # (B,S)\n",
        "\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)         # (B,S)\n",
        "        context = torch.bmm(weights.unsqueeze(1), keys)  # (B,1,H)\n",
        "\n",
        "        return context #, weights free some memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bV4RmqGv0zVS"
      },
      "outputs": [],
      "source": [
        "class DecoderAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, masked_mapping, num_layers=1):\n",
        "        super(DecoderAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.mask = masked_mapping\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim + hidden_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=0.2,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.final = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward_step(self, x, h, c, encod_out, mask_att):\n",
        "        \"\"\"\n",
        "        x : (B,)          token ids\n",
        "        h, c : (num_layers, B, H)\n",
        "        encod_out : (B, L_enc, H)\n",
        "        mask_att : (B, L_enc) attention mask\n",
        "        \"\"\"\n",
        "        B, L_enc, H = encod_out.shape\n",
        "        embedded = self.dropout(self.embed(x))  # (B,E)\n",
        "\n",
        "        # ---- Luong dot-product attention ----\n",
        "        # query = dernier état caché de la dernière couche\n",
        "        query = h[-1]  # (B,H)\n",
        "\n",
        "        # scores = produit scalaire (B,L_enc)\n",
        "        scores = torch.bmm(encod_out, query.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        # masque sur les PAD\n",
        "        if mask_att is not None:\n",
        "            scores = scores.masked_fill(mask_att == 0, float('-inf'))\n",
        "\n",
        "        # softmax pour obtenir les poids\n",
        "        weights = F.softmax(scores, dim=-1)  # (B,L_enc)\n",
        "\n",
        "        # contexte pondéré\n",
        "        h_prime = torch.bmm(weights.unsqueeze(1), encod_out).squeeze(1)  # (B,H)\n",
        "\n",
        "        # ---- LSTM step ----\n",
        "        lstm_in = torch.cat([embedded, h_prime], dim=-1).unsqueeze(1)  # (B,1,E+H)\n",
        "        out, (h, c) = self.lstm(lstm_in, (h, c))\n",
        "\n",
        "        logit = self.final(out.squeeze(1))  # (B, vocab_size)\n",
        "        masked_logit = logit.masked_fill(self.mask, float(\"-inf\"))\n",
        "\n",
        "        return masked_logit, (h, c)\n",
        "\n",
        "    def forward(self, x, encod_out, h, c, targets, teacher_forcing_ratio, mask_att, loss_fn=None):\n",
        "      batch_size, max_len = targets.size()\n",
        "      input_x = x[:, 0]  # <SOS>\n",
        "\n",
        "      all_logits = []\n",
        "\n",
        "      for t in range(max_len):\n",
        "        out, (h, c) = self.forward_step(input_x, h, c, encod_out, mask_att)\n",
        "        all_logits.append(out.unsqueeze(1))   # (B,1,V)\n",
        "\n",
        "        # Teacher forcing\n",
        "        if torch.rand(1).item() < teacher_forcing_ratio:\n",
        "            input_x = targets[:, t]\n",
        "        else:\n",
        "            input_x = out.argmax(dim=-1)\n",
        "\n",
        "      all_logits = torch.cat(all_logits, dim=1)  # (B, T, V)\n",
        "\n",
        "      if loss_fn is None:\n",
        "          return all_logits\n",
        "      else:\n",
        "        # reshape for CrossEntropyLoss\n",
        "          loss = loss_fn(\n",
        "              all_logits.reshape(-1, all_logits.size(-1)),\n",
        "              targets.reshape(-1)\n",
        "          )\n",
        "          return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUvKjl_B03Bt"
      },
      "source": [
        "class DecoderAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, masked_mapping, len_max, num_layers=1):\n",
        "        super(DecoderAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.mask = masked_mapping\n",
        "        \n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "#        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.lstm = nn.LSTM(input_size = embedding_dim + hidden_size,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers = num_layers,\n",
        "                            dropout = 0.2,\n",
        "                            batch_first=True)\n",
        "        \n",
        "        self.final = nn.Linear(hidden_size,vocab_size)\n",
        "        self.att_lin = nn.Linear(embedding_dim, len_max)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward_step(self,x, h, c, encod_out,mask_att) :\n",
        "        \n",
        "        global L_enc\n",
        "        _,L_enc,_ = encod_out.shape\n",
        "        embedded = self.dropout(self.embed(x)) #(B,E)\n",
        "\n",
        "        #Synthesizer part\n",
        "        #Now Dense synthesizer from https://proceedings.mlr.press/v139/tay21a/tay21a.pdf\n",
        "        #and https://iopscience.iop.org/article/10.1088/1742-6596/2580/1/012006/pdf\n",
        "        \n",
        "        emb_lin = self.att_lin(embedded)[:,:L_enc]\n",
        "        emb_lin = emb_lin.masked_fill(mask_att == 0, float('-inf'))\n",
        "\n",
        "        weights = F.softmax(emb_lin, dim=-1)\n",
        "        h_prime = torch.bmm(weights.unsqueeze(1),encod_out).squeeze(1)\n",
        "\n",
        "        #Use of attention by using the latest hidden\n",
        "#        query = h[-1]\n",
        "#        attn_context = self.attention(query,encod_out,mask_att)\n",
        "\n",
        "        lstm_in = torch.cat([embedded, h_prime], dim=-1).unsqueeze(1) #(B,1,E+H)\n",
        "        out, (h,c) = self.lstm(lstm_in, (h, c))\n",
        "        \n",
        "        logit = self.final(out.squeeze(1))      # (B, vocab_size)\n",
        "        masked_logit = logit.masked_fill(self.mask, float(\"-inf\"))\n",
        "        \n",
        "        return masked_logit, h\n",
        "\n",
        "    def forward(self, x, encod_out, h, c, targets, teacher_forcing_ratio, mask_att, loss_fn=None): #On each batch\n",
        "\n",
        "        batch_size, max_len = targets.size()\n",
        "        input_x = x[:, 0]\n",
        "        \n",
        "        if loss_fn is None :\n",
        "            all_logits = []\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for t in range(0, max_len):\n",
        "            out, h = self.forward_step(input_x, h, c, encod_out,mask_att)\n",
        "            if loss_fn is None :\n",
        "                all_logits.append(out.unsqueeze(1))\n",
        "            else :\n",
        "                total_loss += loss_fn(out, targets[:, t])\n",
        "\n",
        "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
        "                input_x = targets[:, t]   # vérité\n",
        "            else:\n",
        "                input_x = out.argmax(dim=-1)  # prédiction\n",
        "\n",
        "            del out\n",
        "\n",
        "        if loss_fn is None :\n",
        "            all_logits = torch.cat(all_logits, dim=1)\n",
        "            return all_logits\n",
        "        else :\n",
        "            total_loss = total_loss / max_len  \n",
        "            return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:55.739031Z",
          "iopub.status.busy": "2025-09-14T14:33:55.738653Z",
          "iopub.status.idle": "2025-09-14T14:33:58.935991Z",
          "shell.execute_reply": "2025-09-14T14:33:58.935077Z",
          "shell.execute_reply.started": "2025-09-14T14:33:55.739002Z"
        },
        "id": "kN05Wln9KiBK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "vocab_size = len(mapping)\n",
        "embedding_size = 96\n",
        "hidden_size = 512\n",
        "num_epoch = 100\n",
        "#accum_steps = 16\n",
        "nb_step_test = len(test_ld)\n",
        "nb_step_train = len(train_ld)\n",
        "#nb_update_per_epoch = nb_step_train // accum_steps\n",
        "\n",
        "#Some of the mapping are only for the encodeur so the decodeur can't produce them, we need to mask them from the loss\n",
        "mapping_inverse = {i: ch for ch, i in mapping.items()}\n",
        "masked_mapping = list(mapping_inverse.keys())[116:-1]\n",
        "\n",
        "mask = torch.zeros(vocab_size, dtype=torch.bool, device=device)\n",
        "mask[masked_mapping] = True\n",
        "\n",
        "enco = Encodeur(vocab_size, embedding_size, hidden_size, num_layers=2).to(device)\n",
        "deco = DecoderAttention(vocab_size, embedding_size, hidden_size, mask,num_layers=2).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0) #Just ignore the padding\n",
        "\n",
        "params = list(enco.parameters()) + list(deco.parameters())\n",
        "\n",
        "opti = torch.optim.AdamW(params, lr=0.002, betas=(0.9,0.999), weight_decay=1e-4)\n",
        "\n",
        "sched_warm = torch.optim.lr_scheduler.LinearLR(opti, start_factor=0.2, end_factor=1.0, total_iters=nb_step_train*3)\n",
        "sched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*40, T_mult=2, eta_min=0.0005) #1 epoch => 2 => 4 => 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVeWw7IE7wF7"
      },
      "source": [
        "Recharge to previous level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jph9YyIwAFf0"
      },
      "source": [
        "checkpoint = torch.load(\"/content/model1\", map_location=device,weights_only=False)\n",
        "\n",
        "enco.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
        "deco.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
        "opti.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "sched_post.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "start_epoch = checkpoint[\"epoch\"] + 1\n",
        "best_val = checkpoint[\"val_loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHuFal1-KiBL"
      },
      "source": [
        "## Creation boucle entraînement Decodeur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:58.938019Z",
          "iopub.status.busy": "2025-09-14T14:33:58.937046Z",
          "iopub.status.idle": "2025-09-14T14:33:58.943717Z",
          "shell.execute_reply": "2025-09-14T14:33:58.942903Z",
          "shell.execute_reply.started": "2025-09-14T14:33:58.937975Z"
        },
        "id": "cGOmDqApKiBL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def token_accuracy(logits, Y, pad_id=0):\n",
        "    # logits: [B, T, V], Y: [B, T]\n",
        "    preds = logits.argmax(dim=-1)                 # [B, T]\n",
        "    mask  = (Y != pad_id)                         # [B, T]\n",
        "    correct = (preds == Y) & mask\n",
        "    return correct.sum().float() / mask.sum().float()\n",
        "\n",
        "def topk_token_accuracy(logits, Y, k=5, pad_id=0):\n",
        "    \"\"\"\n",
        "    logits : (B, T, V)\n",
        "    Y      : (B, T)\n",
        "    \"\"\"\n",
        "    # tronquer Y si jamais logits et Y diffèrent un peu\n",
        "    Y = Y[:, :logits.size(1)]\n",
        "\n",
        "    # top-k indices\n",
        "    topk = logits.topk(k, dim=-1).indices  # (B, T, k)\n",
        "\n",
        "    # mask pour ignorer les PAD\n",
        "    mask = (Y != pad_id)\n",
        "\n",
        "    # comparaison : Y est-il dans top-k ?\n",
        "    correct = (topk == Y.unsqueeze(-1)).any(dim=-1) & mask  # (B, T)\n",
        "\n",
        "    return correct.sum().float() / mask.sum().float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-09-14T14:33:58.945465Z",
          "iopub.status.busy": "2025-09-14T14:33:58.944636Z",
          "iopub.status.idle": "2025-09-14T14:34:00.029888Z",
          "shell.execute_reply": "2025-09-14T14:34:00.028692Z",
          "shell.execute_reply.started": "2025-09-14T14:33:58.945432Z"
        },
        "id": "5zP1vY0ZKiBL",
        "outputId": "b85fb058-526f-457b-a74d-fb817c6b8e3d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 0 6.114624229603374 0.457869417886966 0.8001526954733296 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Joiseeoesdo l  doest le puaoa poruedore Jo  pranne  dour lu'jn pour lnle  prnse t das lesenne  \n",
            "\n",
            " 1 5.076262533564633 0.5094245431441365 0.8302845233752404 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicaeqe du l  qoest pe puauoivêmuedore Ji  caannss daur lu'in pour dsle  mrnrest das les rneENDEND\n",
            "\n",
            " 2 4.843221971714835 0.5229486869175844 0.8376495404007994 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiioéoe du g  doest la puauo vêmu dure Ji  caonsss dour lu'on tour dtle  danrestsdas les nne  \n",
            "\n",
            " 3 4.253583034049412 0.5575154302311509 0.8549634777469399 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicaaqe du g  doest pa puaào pêru duce Jis paansss dour lu'on pour dnle  drnrent das lesanneENDEND\n",
            "\n",
            " 4 4.123942149385835 0.56903515672274 0.859158777160409 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicaeqe du l  doest la puaua pêru duce Jis caannss dour lu'on pour dtle  panrent das lesENDrleENDEND\n",
            "\n",
            " 5 4.026483280925265 0.5756610795110735 0.86248665753706 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiiaeoe du n  doest la mueuo mêru duce Li  daonnss dour lu'in pour dsle  manrest lor lesENDrneENDEND\n",
            "\n",
            " 6 3.953406574586331 0.5814171608374682 0.8649114362987471 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicaine di z  doest la pueia pêru duce Jis caannss dour lu'on cour dsle  manrent das lexENDnneENDEND\n",
            "\n",
            " 7 3.897988161234054 0.5861975069587317 0.8659803065252892 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiiaina di n  doest la fuauo vêrm duce Ji  faonnss dour lu'on pour dsle  eanrent las lesENDrneENDEND\n",
            "\n",
            " 8 3.841227545808526 0.5902744160716447 0.8686257917204021 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiieine eu n  doest la mueua pêrm duce Lis peonsss dour lu'on pour dtle  manrent las lesarneENDEND\n",
            "\n",
            " 9 3.7934155249881645 0.5939277502117875 0.8698345768598863 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiceine di n  doest pa puaua pêrm duce Li  daonnss dour lu'on pour dnle  panrest das lexENDrneENDEND\n",
            "\n",
            " 10 3.7627967174158554 0.5975735205131308 0.8708410851749373 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicaiée ei n  doest pa pueua vêrm duce Jis daonlss dour mu'on cour dtle  danrest lar lexENDrneENDEND\n",
            "\n",
            " 11 3.723350661729309 0.6000468958005567 0.8728571946238294 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiieime eirz  doest pa cueua pêrm duce Lis paonnss dour lu'on cour onle  sanrest las lexarneENDEND\n",
            "\n",
            " 12 3.694868308608515 0.6032463996127314 0.8730944946960166 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicaime ei n  doest pa mueua pêsm duce Jis paannss dour lu'on pour dtle  srnrest las lesarneENDEND\n",
            "\n",
            " 13 3.6641053989661954 0.6060677114849329 0.8744653088075144 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiiaime ei n  doest le mueua têsm duce Lis pronnss dour lu'on pour dnle  sanrest las lex rneENDEND\n",
            "\n",
            " 14 3.6455116571576966 0.6095697688490863 0.8751898820017591 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiieime ei n  doest l' mueua pêsm duce Li  paonnss dour lu'on pour dtle  danrast las lexENDrneENDEND\n",
            "\n",
            " 15 3.6249293193922925 0.6112943240953649 0.8758201084019225 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicei e ai n  doest la mueua pêsm duce Lis deonnss dour lu'on cour dtle  danrest las lex rne END\n",
            "\n",
            " 16 3.6003630122588772 0.6136844971559966 0.8769445456104514 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Laiceime ai n  doest le mueua rêsm duce Li  daannss dour lu'on cour dnle  sanrest las lexENDrnesEND\n",
            "\n",
            " 17 3.579377135324637 0.6163167130582113 0.8771242403689726 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiceime pi n  aoest le mueua pêsm duce Li  daonnss dour lu'on pour otle  sanrist las lexaineENDEND\n",
            "\n",
            " 18 3.5600219442173473 0.6193119932228004 0.8784576544055233 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiieime di n  àoest l' mueua pêsm duce Jiu paonnss dour lu'on cour dtle  manrest las lexarneENDEND\n",
            "\n",
            " 19 3.549324716277946 0.6192893017064021 0.8787272807992534 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiieime,ai n  doest le mueua pêsm duce Liu daonnss dour lu'on pour dtle  sanrest las lexaine END\n",
            "\n",
            " 20 3.5359838016142158 0.6221938158053976 0.879410030665221 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiiaiqe ei n  doest le pueua pêsm duce Li  peonnss dour lu'on pour onle  sanrest las lexarne END\n",
            "\n",
            " 21 3.523891250394596 0.6240393924724676 0.8800156535925688 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaicei e ai n  doest le fueua mêam duce Lil peonnss dour lu'on pour otle  man-est las lexENDrne END\n",
            "\n",
            " 22 3.513002158888564 0.6258244584291419 0.8806664384441611 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiceime airn  àoest la mueua rêsm duce Lil deonnes dour mueon pour otle  manrest las lexENDine END\n",
            "\n",
            " 23 3.506276666059635 0.6273674815442333 0.8805924970426677 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiieime ei n  doest le cueua rêsm duce Li  daonnss dour lu'on pour dtle  sanrest las lexaine END\n",
            "\n",
            " 24 3.4899260586212875 0.6288953769817258 0.8816653372328959 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiiaime,airn  doest le fueua pêsm puce Li  deonnes dour lu'on cour otle  manrest las lexarne END\n",
            "\n",
            " 25 3.487547333228315 0.6291222921457098 0.8816370110452911 \n",
            "\n",
            "Maritime seine c'est la qu'ma team sème<EOL>Ses graines pour qu'un jour elles germent par dizainesEND\n",
            "Jaiieime,airn  doest la muaua pêim duce Liu paannss dour lu'on pour dtle  manrist las lex ine END\n"
          ]
        }
      ],
      "source": [
        "l_tot = []\n",
        "teacher_forcing_ratio = 1\n",
        "#Early stopping\n",
        "#early_stopping_count = 0\n",
        "#patience = 5\n",
        "best_val = float(\"inf\")\n",
        "begin_epoch = 0\n",
        "scaler = torch.amp.GradScaler()\n",
        "#accum_steps = 16   # number of mini-batches to accumulate\n",
        "#global_step = 0\n",
        "\n",
        "for epoch in range(num_epoch) :\n",
        "    enco.train(); deco.train()\n",
        "\n",
        "    l_train = 0.0\n",
        "    l_test = 0.0\n",
        "\n",
        "    for (context, X, Y), (length_cont,length_text) in iter(train_ld):\n",
        "        X = X.to(device, non_blocking=True)\n",
        "        Y = Y.to(device, non_blocking=True)\n",
        "        context = context.to(device, non_blocking=True)\n",
        "\n",
        "        # pas de zero_grad ici → on le fait seulement après accum_steps\n",
        "        encod_out, length_out, h_enco, c_enco = enco(context,length_cont)\n",
        "\n",
        "        arange = torch.arange(context.shape[1], device=length_out.device).unsqueeze(0)\n",
        "        mask_attn = (arange < length_out.unsqueeze(1)).to(device)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\"):\n",
        "            loss = deco(X, encod_out, h_enco, c_enco, Y, teacher_forcing_ratio , mask_attn, loss_fn=loss_fn)\n",
        "            #loss = loss_fn(logits.reshape(-1, vocab_size), Y.reshape(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(opti)\n",
        "        torch.nn.utils.clip_grad_norm_(list(enco.parameters()) + list(deco.parameters()), 1)\n",
        "\n",
        "        scaler.step(opti)\n",
        "        scaler.update()\n",
        "\n",
        "        step_scheduler = sched_warm if epoch < 3 else sched_post\n",
        "        step_scheduler.step()\n",
        "\n",
        "        del loss, X, Y, context\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # scheduler\n",
        "   # if epoch in [20, 40, 80]:\n",
        "   #     sched_post.base_lrs[0] *= 0.9\n",
        "   #     print(f\"Decrease {sched_post.base_lrs[0]}, {sched_post.eta_min}\")\n",
        "\n",
        "#        step_scheduler = sched_warm if epoch < 2 else sched_post\n",
        "#        step_scheduler.step()\n",
        "\n",
        "    #Test data part\n",
        "\n",
        "    enco.eval()\n",
        "    deco.eval()\n",
        "\n",
        "    acc_sum, tok_sum = 0.0, 0\n",
        "    topk_acc_sum = 0.0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "            for (context, X, Y), (length_cont,length_text) in iter(test_ld) :\n",
        "                X = X.to(device, non_blocking = True)\n",
        "                Y = Y.to(device, non_blocking = True)\n",
        "                context = context.to(device, non_blocking = True)\n",
        "\n",
        "                encod_out, length_out, h_enco, c_enco = enco(context,length_cont)\n",
        "                arange = torch.arange(context.shape[1], device=length_out.device).unsqueeze(0)  # (1,S)\n",
        "                mask_attn = arange < length_out.unsqueeze(1)\n",
        "                mask_attn = mask_attn.to(device)\n",
        "\n",
        "                logits = deco(X, encod_out, h_enco, c_enco, Y, 1, mask_attn)\n",
        "                loss = loss_fn(logits.reshape(-1, vocab_size), Y.reshape(-1))\n",
        "\n",
        "                #logits, (h_dec, c_dec) = deco(X,h_dec,c_dec,length_text)\n",
        "                #loss = loss_fn(logits.reshape(-1,vocab_size),Y.reshape(-1))\n",
        "                l_test += loss.item()\n",
        "                acc = token_accuracy(logits, Y).item()\n",
        "\n",
        "                # top-1 accuracy\n",
        "                acc_sum += ((logits.argmax(-1) == Y) & (Y != 0)).sum().item()\n",
        "                tok_sum += (Y != 0).sum().item()\n",
        "\n",
        "                # top-k accuracy (par ex. k=5)\n",
        "                topk_acc_sum += topk_token_accuracy(logits, Y, k=5).item()\n",
        "\n",
        "        epoch_token_acc = acc_sum / tok_sum\n",
        "        epoch_topk_acc  = topk_acc_sum / nb_step_test   # moyenne sur les batches\n",
        "        print(\"\\n\", epoch, np.exp(l_test/nb_step_test), epoch_token_acc, epoch_topk_acc, \"\\n\")\n",
        "        print(\"\".join([mapping_inverse[j.item()] for j in Y[0][:length_text[0].item()]]))\n",
        "        print(\"\".join([mapping_inverse[j.argmax().item()] for j in logits[0][:length_text[0].item()]]))\n",
        "\n",
        "        #process = psutil.Process(os.getpid())\n",
        "        #print(\"RAM used:\", process.memory_info().rss / 1024**3, \"GB\")\n",
        "\n",
        "        del logits, loss, X, Y, context\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        #Record the loss of the epoch\n",
        "        l_tot.append(l_test);\n",
        "\n",
        "        if l_test < best_val :\n",
        "            best_val = l_test\n",
        "#            early_stopping_count = 0\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"encoder_state_dict\": enco.state_dict(),\n",
        "                \"decoder_state_dict\": deco.state_dict(),\n",
        "                \"optimizer_state_dict\": opti.state_dict(),\n",
        "                \"scheduler_state_dict\": sched_post.state_dict(),\n",
        "                \"val_loss\": l_test,\n",
        "            }, \"model1.pt\")\n",
        "\n",
        "print(f\"Liste of offset used : {list_offset}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 8228362,
          "sourceId": 13008689,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
