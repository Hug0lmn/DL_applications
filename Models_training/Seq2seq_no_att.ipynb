{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code will implement the Seq2seq no attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:27.040022Z",
     "iopub.status.busy": "2025-09-23T09:21:27.039737Z",
     "iopub.status.idle": "2025-09-23T09:21:33.480754Z",
     "shell.execute_reply": "2025-09-23T09:21:33.480151Z",
     "shell.execute_reply.started": "2025-09-23T09:21:27.039995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.482795Z",
     "iopub.status.busy": "2025-09-23T09:21:33.482467Z",
     "iopub.status.idle": "2025-09-23T09:21:33.576444Z",
     "shell.execute_reply": "2025-09-23T09:21:33.575649Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.482777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.577343Z",
     "iopub.status.busy": "2025-09-23T09:21:33.577147Z",
     "iopub.status.idle": "2025-09-23T09:21:33.598081Z",
     "shell.execute_reply": "2025-09-23T09:21:33.597352Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.577326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/new-seq2seq/Encoding_map.pkl\", \"rb\") as f:\n",
    "    mapping = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.599005Z",
     "iopub.status.busy": "2025-09-23T09:21:33.598814Z",
     "iopub.status.idle": "2025-09-23T09:21:33.606997Z",
     "shell.execute_reply": "2025-09-23T09:21:33.606473Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.598989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MMapSeq2Seq(Dataset):\n",
    "    def __init__(self, prefix=\"\"):\n",
    "        self.ctx = np.load(prefix+\"context_np.npy\",\"r\")\n",
    "        self.x   = np.load(prefix+\"X_np.npy\",\"r\")\n",
    "        self.y   = np.load(prefix+\"Y_np.npy\",\"r\")\n",
    "\n",
    "        self.ctx_off = np.load(prefix+\"context_offset.npy\")\n",
    "        self.ctx_len = np.load(prefix+\"context_length.npy\")\n",
    "        self.x_off   = np.load(prefix+\"X_offset.npy\")\n",
    "        self.x_len   = np.load(prefix+\"X_length.npy\")\n",
    "        self.y_off   = np.load(prefix+\"Y_offset.npy\")\n",
    "        self.y_len   = np.load(prefix+\"Y_length.npy\")\n",
    "\n",
    "        assert len(self.ctx_len) == len(self.x_len) == len(self.y_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ctx_len)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # vues memmap -> torch (zéro copie CPU)\n",
    "        s, L = int(self.ctx_off[i]), int(self.ctx_len[i])\n",
    "        ctx = torch.from_numpy(self.ctx[s:s+L].astype(np.int32))\n",
    "\n",
    "        s, L = int(self.x_off[i]), int(self.x_len[i])\n",
    "        x = torch.from_numpy(self.x[s:s+L].astype(np.int32))\n",
    "\n",
    "        s, L = int(self.y_off[i]), int(self.y_len[i])\n",
    "        y = torch.from_numpy(self.y[s:s+L].astype(np.int32))\n",
    "\n",
    "        return ctx, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.607748Z",
     "iopub.status.busy": "2025-09-23T09:21:33.607520Z",
     "iopub.status.idle": "2025-09-23T09:21:33.624254Z",
     "shell.execute_reply": "2025-09-23T09:21:33.623538Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.607722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    ctxs, xs, ys = zip(*batch)  # tuples de 1D tensors CPU\n",
    "    ctxs = [t.to(torch.long, copy=False) for t in ctxs]\n",
    "    xs   = [t.to(torch.long, copy=False) for t in xs]\n",
    "    ys   = [t.to(torch.long, copy=False) for t in ys]\n",
    "\n",
    "    ctx_pad = torch.nn.utils.rnn.pad_sequence(ctxs, batch_first=True, padding_value=PAD_ID)\n",
    "    x_pad   = torch.nn.utils.rnn.pad_sequence(xs,   batch_first=True, padding_value=PAD_ID)\n",
    "    y_pad   = torch.nn.utils.rnn.pad_sequence(ys,   batch_first=True, padding_value=PAD_ID)\n",
    "\n",
    "    ctx_len = torch.tensor([t.numel() for t in ctxs], dtype=torch.int64)  # CPU Long (pack_padded)\n",
    "    x_len   = torch.tensor([t.numel() for t in xs],   dtype=torch.int64)\n",
    "\n",
    "    return (ctx_pad, x_pad, y_pad), (ctx_len, x_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, pipeline prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.625099Z",
     "iopub.status.busy": "2025-09-23T09:21:33.624906Z",
     "iopub.status.idle": "2025-09-23T09:21:33.712863Z",
     "shell.execute_reply": "2025-09-23T09:21:33.712375Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.625085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "ds = MMapSeq2Seq(prefix=\"/kaggle/input/new-seq2seq/\")  # mets des préfixes train/val si tu split offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.715290Z",
     "iopub.status.busy": "2025-09-23T09:21:33.714868Z",
     "iopub.status.idle": "2025-09-23T09:21:33.751023Z",
     "shell.execute_reply": "2025-09-23T09:21:33.750534Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.715272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(ds))\n",
    "test_size   = len(ds) - train_size\n",
    "train_ds, test_ds = random_split(ds, [train_size, test_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.751828Z",
     "iopub.status.busy": "2025-09-23T09:21:33.751592Z",
     "iopub.status.idle": "2025-09-23T09:21:33.756516Z",
     "shell.execute_reply": "2025-09-23T09:21:33.755951Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.751811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if device == \"cpu\" :\n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate) #Shuffle False because we need the RNN to use previous sequences data to predict next one\n",
    "    test_ld = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate)\n",
    "else : \n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=device, \n",
    "                        num_workers=4, shuffle=True, drop_last=True, persistent_workers=False, collate_fn=collate) #Shuffle False because we need the RNN to use previous sequences data to predict next one\n",
    "    test_ld = DataLoader(test_ds, batch_size=batch_size, pin_memory=True, pin_memory_device=device, \n",
    "                        num_workers=2, shuffle=False, drop_last=True, persistent_workers=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of operations :\n",
    "\n",
    "- First the entire batch pass through the Encodeur, we extract the h,c from this\n",
    "\n",
    "- Then we iterate over all the length of the sequence, with the previous input and we use at first the h,c from the Encodeur\n",
    "\n",
    "- Then we calculate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.757397Z",
     "iopub.status.busy": "2025-09-23T09:21:33.757212Z",
     "iopub.status.idle": "2025-09-23T09:21:33.775288Z",
     "shell.execute_reply": "2025-09-23T09:21:33.774812Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.757382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encodeur(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(Encodeur, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = num_layers, \n",
    "                            dropout = 0.2,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, length_batch):\n",
    "        #Encodeur\n",
    "        emb = self.embed(x)\n",
    "        emb_drop = self.dropout(emb)\n",
    "        #Pack before LSTM to avoid unecessary compute\n",
    "        packed_x = pack_padded_sequence(emb_drop, length_batch, batch_first=True, enforce_sorted=False)                      \n",
    "        packed_out, (h,c) = self.lstm(packed_x)             \n",
    "        #Unpack after\n",
    "        _, _ = pad_packed_sequence(packed_out, batch_first=True) #First right now only h,c\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.776356Z",
     "iopub.status.busy": "2025-09-23T09:21:33.776194Z",
     "iopub.status.idle": "2025-09-23T09:21:33.798413Z",
     "shell.execute_reply": "2025-09-23T09:21:33.797763Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.776343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decodeur(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, masked_mapping, num_layers=1):\n",
    "        super(Decodeur, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = num_layers, \n",
    "                            dropout = 0.2,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.final = nn.Linear(hidden_size,vocab_size)\n",
    "        self.mask = masked_mapping\n",
    "\n",
    "#    def forward(self, x, h, c, length_batch):\n",
    "    def forward(self, x, h, c, length_batch, targets=None, teacher_forcing_ratio=1.0):\n",
    "        batch_size, max_len = targets.size()\n",
    "        #batch_size, max_len = Y.size() \n",
    "        device = x.device\n",
    "\n",
    "        # On stockera les logits ici\n",
    "        all_logits = []\n",
    "\n",
    "        # Premier input : <SOS> pour tout le batch\n",
    "        input_t = x[:, 0]\n",
    "\n",
    "        for t in range(0, max_len):\n",
    "            emb = self.embed(input_t).unsqueeze(1)  # (batch, 1, emb_dim)\n",
    "            out, (h, c) = self.lstm(emb, (h, c))    # (batch, 1, hidden)\n",
    "            logit = self.final(out.squeeze(1))      # (batch, vocab_size)\n",
    "            masked_logit = logit.masked_fill(self.mask, float(\"-inf\"))\n",
    "            all_logits.append(masked_logit.unsqueeze(1))\n",
    "\n",
    "            # Decide teacher forcing ou pas\n",
    "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                input_t = targets[:, t]   # vérité\n",
    "            else:\n",
    "                input_t = masked_logit.argmax(dim=-1)  # prédiction\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=1)  # (batch, max_len-1, vocab_size)\n",
    "        return all_logits, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:33.799302Z",
     "iopub.status.busy": "2025-09-23T09:21:33.799116Z",
     "iopub.status.idle": "2025-09-23T09:21:39.667305Z",
     "shell.execute_reply": "2025-09-23T09:21:39.666741Z",
     "shell.execute_reply.started": "2025-09-23T09:21:33.799289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(mapping)\n",
    "embedding_size = 96\n",
    "hidden_size = 512\n",
    "num_epoch = 200\n",
    "\n",
    "#Some of the mapping are only for the encodeur so the decodeur can't produce them, we need to mask them from the loss\n",
    "mapping_inverse = {i: ch for ch, i in mapping.items()}\n",
    "masked_mapping = list(mapping_inverse.keys())[117:-1]\n",
    "\n",
    "mask = torch.zeros(vocab_size, dtype=torch.bool, device=device)\n",
    "mask[masked_mapping] = True\n",
    "\n",
    "nb_step_test = len(test_ld)\n",
    "nb_step_train = len(train_ld)\n",
    "\n",
    "enco = Encodeur(vocab_size, embedding_size, hidden_size, num_layers=2).to(device)\n",
    "deco = Decodeur(vocab_size, embedding_size, hidden_size, mask,num_layers=2).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0) #Just ignore the padding\n",
    "\n",
    "params = list(enco.parameters()) + list(deco.parameters())\n",
    "opti = torch.optim.AdamW(params, lr=0.01, betas=(0.9,0.999), weight_decay=1e-3)\n",
    "\n",
    "sched_warm = torch.optim.lr_scheduler.LinearLR(opti, start_factor=0.25, end_factor=1.0, total_iters=nb_step_train*5)\n",
    "sched_post = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opti, T_0=nb_step_train*200, T_mult=2, eta_min=0.001) #1 epoch => 2 => 4 => 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation boucle entraînement Decodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:39.668372Z",
     "iopub.status.busy": "2025-09-23T09:21:39.668030Z",
     "iopub.status.idle": "2025-09-23T09:21:39.673549Z",
     "shell.execute_reply": "2025-09-23T09:21:39.672918Z",
     "shell.execute_reply.started": "2025-09-23T09:21:39.668346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def token_accuracy(logits, Y, pad_id=0):\n",
    "    # logits: [B, T, V], Y: [B, T]\n",
    "    preds = logits.argmax(dim=-1)                 # [B, T]\n",
    "    mask  = (Y != pad_id)                         # [B, T]\n",
    "    correct = (preds == Y) & mask\n",
    "    return correct.sum().float() / mask.sum().float()\n",
    "\n",
    "def topk_token_accuracy(logits, Y, k=5, pad_id=0):\n",
    "    \"\"\"\n",
    "    logits : (B, T, V)\n",
    "    Y      : (B, T)\n",
    "    \"\"\"\n",
    "    # tronquer Y si jamais logits et Y diffèrent un peu\n",
    "    Y = Y[:, :logits.size(1)]\n",
    "\n",
    "    # top-k indices\n",
    "    topk = logits.topk(k, dim=-1).indices  # (B, T, k)\n",
    "\n",
    "    # mask pour ignorer les PAD\n",
    "    mask = (Y != pad_id)\n",
    "\n",
    "    # comparaison : Y est-il dans top-k ?\n",
    "    correct = (topk == Y.unsqueeze(-1)).any(dim=-1) & mask  # (B, T)\n",
    "\n",
    "    return correct.sum().float() / mask.sum().float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:21:39.674389Z",
     "iopub.status.busy": "2025-09-23T09:21:39.674222Z",
     "iopub.status.idle": "2025-09-23T10:47:05.223072Z",
     "shell.execute_reply": "2025-09-23T10:47:05.222066Z",
     "shell.execute_reply.started": "2025-09-23T09:21:39.674375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "l_tot = []\n",
    "teacher_forcing_ratio = 1\n",
    "#Early stopping\n",
    "#early_stopping_count = 0\n",
    "#patience = 5\n",
    "best_val = float(\"inf\")\n",
    "begin_epoch = 0\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epoch) :\n",
    "    \n",
    "    enco.train()\n",
    "    deco.train()\n",
    "\n",
    "    #Create loss per epoch\n",
    "    l_train = 0.0\n",
    "    l_test = 0.0\n",
    "\n",
    "#    teacher_forcing_ratio = max(0.5, 1.0 - (max(0,epoch-40)) * 0.002)\n",
    "    if epoch%10==0:\n",
    "        print(teacher_forcing_ratio)\n",
    "    \n",
    "    for (context,X, Y), (length_cont,length_text) in iter(train_ld) :\n",
    "        X = X.to(device, non_blocking = True)\n",
    "        Y = Y.to(device, non_blocking = True)\n",
    "        context = context.to(device, non_blocking = True)\n",
    "#        length_text = (Y != 0).sum(dim=1).cpu()\n",
    "        opti.zero_grad(set_to_none=True)\n",
    "\n",
    "        #Encodeur part, sees the whole context of the batch and output the hidden and cell state\n",
    "        h_enco,c_enco = enco(context,length_cont)\n",
    "        \n",
    "        h_dec = h_enco\n",
    "        c_dec = c_enco\n",
    "        \n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            logits, (h_dec, c_dec) = deco(\n",
    "            X, h_dec, c_dec, length_text, \n",
    "            targets=Y, teacher_forcing_ratio=teacher_forcing_ratio   # <--- par ex.\n",
    "            )\n",
    "            loss = loss_fn(logits.reshape(-1, vocab_size), Y.reshape(-1))\n",
    "            #logits, (h_dec, c_dec) = deco(X,h_dec,c_dec,length_text)\n",
    "            #loss = loss_fn(logits.reshape(-1,vocab_size),Y.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        #Gradient clipping to avoid exploding gradient\n",
    "        scaler.unscale_(opti)\n",
    "        torch.nn.utils.clip_grad_norm_(list(enco.parameters()) + list(deco.parameters()), 0.25)\n",
    "\n",
    "        #Scaler\n",
    "        scaler.step(opti); scaler.update()\n",
    "        l_train += loss.item()\n",
    "\n",
    "        #Scheduler part\n",
    "        #Warm start\n",
    "        \n",
    "        if sched_post.T_cur == 0 and epoch > 5:  #After warm restart decrease the max learning rate\n",
    "            sched_post.base_lrs[0] = sched_post.base_lrs[0] * 0.5\n",
    "#            sched_post.eta_min = sched_post.eta_min * 1.25\n",
    "            print(f\"Decrease {sched_post.base_lrs[0]}, {sched_post.eta_min}\")\n",
    "\n",
    "        step_scheduler = sched_warm if epoch < 5 else sched_post\n",
    "        step_scheduler.step()\n",
    "\n",
    "    #Test data part\n",
    "    \n",
    "    enco.eval()\n",
    "    deco.eval()\n",
    "\n",
    "    acc_sum, tok_sum = 0.0, 0\n",
    "    topk_acc_sum = 0.0    \n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            for (context, X, Y), (length_cont,length_text) in iter(test_ld) :\n",
    "                X = X.to(device, non_blocking = True)\n",
    "                Y = Y.to(device, non_blocking = True)\n",
    "                context = context.to(device, non_blocking = True)\n",
    "\n",
    "                #Encodeur part, sees the whole context of the batch and output the hidden and cell state\n",
    "                h_enco,c_enco = enco(context,length_cont)\n",
    "        \n",
    "                h_dec = h_enco\n",
    "                c_dec = c_enco\n",
    "                \n",
    "                logits, (h_dec, c_dec) = deco(X, h_dec, c_dec, length_text, targets=Y, teacher_forcing_ratio=1)\n",
    "                loss = loss_fn(logits.reshape(-1, vocab_size), Y.reshape(-1))\n",
    "\n",
    "                #logits, (h_dec, c_dec) = deco(X,h_dec,c_dec,length_text)\n",
    "                #loss = loss_fn(logits.reshape(-1,vocab_size),Y.reshape(-1))\n",
    "                l_test += loss.item()\n",
    "                acc = token_accuracy(logits, Y).item()\n",
    "                \n",
    "                # top-1 accuracy\n",
    "                acc_sum += ((logits.argmax(-1) == Y) & (Y != 0)).sum().item()\n",
    "                tok_sum += (Y != 0).sum().item()\n",
    "\n",
    "                # top-k accuracy (par ex. k=5)\n",
    "                topk_acc_sum += topk_token_accuracy(logits, Y, k=5).item()\n",
    "\n",
    "        epoch_token_acc = acc_sum / tok_sum\n",
    "        epoch_topk_acc  = topk_acc_sum / nb_step_test   # moyenne sur les batches\n",
    "        if np.exp(best_val/nb_step_test) < 3.5 :\n",
    "            teacher_forcing_ratio = 0.9\n",
    "        elif np.exp(best_val/nb_step_test) < 3.75 :\n",
    "            teacher_forcing_ratio = 0.9\n",
    "        elif np.exp(best_val/nb_step_test) < 4 :\n",
    "            teacher_forcing_ratio = 0.95\n",
    "\n",
    "        #Record the loss of the epoch\n",
    "        l_tot.append(l_test); \n",
    "\n",
    "        if l_test < best_val :\n",
    "            print(epoch, np.exp(l_test/nb_step_test), epoch_token_acc, epoch_topk_acc, \"\\n\")\n",
    "            best_val = l_test\n",
    "#            early_stopping_count = 0\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"encoder_state_dict\": enco.state_dict(),\n",
    "                \"decoder_state_dict\": deco.state_dict(),\n",
    "                \"optimizer_state_dict\": opti.state_dict(),\n",
    "                \"scheduler_state_dict\": sched_post.state_dict(),\n",
    "                \"val_loss\": l_test,\n",
    "            }, \"model\")\n",
    "        \n",
    "#        elif l_test >= best_val :\n",
    "#            early_stopping_count += 1\n",
    "\n",
    "#        if early_stopping_count == patience :\n",
    "#            print(\"Early Stopping\")\n",
    "#            break \n",
    "\n",
    "print(f\"Liste of offset used : {list_offset}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8228362,
     "sourceId": 13150697,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
